* Summary
We ran the HPL benchmark on a cluster named dahu, which is part of Grid'5000, an
open access testbed.

We made successful predictions of HPL performance with SimGrid, an open-source
simulator. This required to run different calibration codes on the cluster
nodes, to build a model for the network and HPL computation kernels.

All our software contributions are open-source. The new features implemented in
SimGrid have been integrated. All the HPL modifications as well as our
experiment scripts and notebooks are available in public repositories with
open-source licenses.
* (a) Performed verification and validation studies:
Our article is precisely about the comparison between simulation of HPC
applications and real experiments. Therefore, we extensively checked that our
simulations were close to reality:
1) We instrumented the code to get traces from both the simulation and the
   reality. This allowed us to visually compare the Gantt charts and check that
   the number of calls to each function and their durations were similar.
2) We performed complete executions of HPL, both in reality and in simulation,
   to compare their reported performance. This confirmed that our prediction of
   the performance was very close to reality.

We checked extensively that our simulations were close to reality:
1) We instrumented the code to get traces from both the simulation and the
   reality. This allowed us to visually compare the Gantt charts and check that
   the number of calls to each function and their durations was similar.
2) We performed complete executions of HPL, both in reality and in simulation,
   to compare their reported performance. This confirmed that our prediction of
   the performance was very close to the reality.
* (b) Validated the accuracy and precision of timings
All timings have been measured with the function clock_gettime. The MPI traces of
HPL have been collected with the Akypuera library that makes sure to correct any
clock drift of the machines. All the real executions of HPL have been repeated
several times, we report in our plot (figure 9) both the average and all the
original values.
* (c) Used manufactured solutions or spectral properties
Not applicable.
* (d) Quantified the sensitivity of your results to initial conditions and/or parameters of the computational environment
For a given model, the simulation results are deterministic.

One goal of the paper was to show that a different model could lead to important
changes in the prediction. For instance, we identified four nodes of our cluster
that were significantly slower than the others (~10%), which led to a
significant performance drop of the whole application. We have shown how to take
this heterogeneity into account in our models, which allow us to successfully
predict the performance of HPL on the cluster despite the heterogeneity.
* (e) Describe controls, statistics, or other steps taken to make the measurements and analyses robust to variability and unknowns in the system
All the scripts, data and notebooks to run and analyze the required experiments
are available in the artifact : one can easily try to reproduce our results.

For all the experiments we did, our custom experiment engine has automatically
collected relevant information (e.g., software versions, environment variables,
nodes on which the experiment was made, executed commands). It is therefore
possible to investigate retrospectively on any experiment.

Finally, to reduce the variability as much as possible, we used the same
software versions throughout all our work (in particular, MPI, BLAS, HPL, Linux
kernel, GCC). For each experiment, we also disabled the hyperthreading, fixed
the core frequencies and pinned the threads to the cores.
