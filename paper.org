# -*- coding: utf-8 -*-
# -*- org-confirm-babel-evaluate: nil -*-
# -*- mode: org -*-
#+TITLE:
#+LANGUAGE:  en
#+OPTIONS: H:5 author:nil email:nil creator:nil timestamp:nil skip:nil toc:nil ^:nil
#+TAGS: ARNAUD(a) CHRISTIAN(c) ANNE-CECILE(A)
#+TAGS: noexport(n) DEPRECATED(d) ignore(i)
#+TAGS: EXPERIMENT(e) LU(l) EP(e)
#+STARTUP: overview indent inlineimages logdrawer hidestars
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w@) | DONE(d!) CANCELLED(c@) DEFERRED(@) FLAWED(f@)
#+LATEX_CLASS: IEEEtran
#+PROPERTY: header-args :eval never-export
#+LATEX_HEADER: \usepackage{DejaVuSansMono}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: %\usepackage{fixltx2e}
#+LATEX_HEADER: \usepackage{ifthen,figlatex}
#+LATEX_HEADER: \usepackage{longtable}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{wrapfig}
#+LATEX_HEADER: \usepackage{subfigure}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{color,soul}
#+LATEX_HEADER: \usepackage[export]{adjustbox}
#+LATEX_HEADER: \usepackage{xspace}
#+LATEX_HEADER: \usepackage{amsmath,amssymb}
#+LATEX_HEADER: \usepackage[american]{babel}
#+LATEX_HEADER: \usepackage{relsize}
#+LATEX_HEADER: \AtBeginDocument{
#+LATEX_HEADER:   \definecolor{pdfurlcolor}{rgb}{0,0,0.6}
#+LATEX_HEADER:   \definecolor{pdfcitecolor}{rgb}{0,0.6,0}
#+LATEX_HEADER:   \definecolor{pdflinkcolor}{rgb}{0.6,0,0}
#+LATEX_HEADER:   \definecolor{light}{gray}{.85}
#+LATEX_HEADER:   \definecolor{vlight}{gray}{.95}
#+LATEX_HEADER: }
#+LATEX_HEADER: %\usepackage[paper=letterpaper,margin=1.61in]{geometry}
#+LATEX_HEADER: \usepackage{url} \urlstyle{sf}
#+LATEX_HEADER: \usepackage[normalem]{ulem}
#+LATEX_HEADER: \usepackage{todonotes}
#+LATEX_HEADER: \usepackage{fancyvrb}
#+LATEX_HEADER: \usepackage[colorlinks=true,citecolor=pdfcitecolor,urlcolor=pdfurlcolor,linkcolor=pdflinkcolor,pdfborder={0 0 0}]{hyperref}
# #+LATEX_HEADER: \usepackage[round-precision=3,round-mode=figures,scientific-notation=true]{siunitx}
#+LATEX_HEADER: \usepackage{color,colortbl}
#+LATEX_HEADER: \definecolor{gray98}{rgb}{0.98,0.98,0.98}
#+LATEX_HEADER: \definecolor{gray20}{rgb}{0.20,0.20,0.20}
#+LATEX_HEADER: \definecolor{gray25}{rgb}{0.25,0.25,0.25}
#+LATEX_HEADER: \definecolor{gray16}{rgb}{0.161,0.161,0.161}
#+LATEX_HEADER: \definecolor{gray60}{rgb}{0.6,0.6,0.6}
#+LATEX_HEADER: \definecolor{gray30}{rgb}{0.3,0.3,0.3}
#+LATEX_HEADER: \definecolor{bgray}{RGB}{248, 248, 248}
#+LATEX_HEADER: \definecolor{amgreen}{RGB}{77, 175, 74}
#+LATEX_HEADER: \definecolor{amblu}{RGB}{55, 126, 184}
#+LATEX_HEADER: \definecolor{amred}{RGB}{228,26,28}
#+LATEX_HEADER: \definecolor{amdove}{RGB}{102,102,122}
#+LATEX_HEADER: \usepackage{xcolor}
#+LATEX_HEADER: \usepackage[procnames]{listings}
#+LATEX_HEADER: \lstset{ %
#+LATEX_HEADER:  backgroundcolor=\color{gray98},    % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
#+LATEX_HEADER:  basicstyle=\tt\prettysmall,      % the size of the fonts that are used for the code
#+LATEX_HEADER:  breakatwhitespace=false,          % sets if automatic breaks should only happen at whitespace
#+LATEX_HEADER:  breaklines=true,                  % sets automatic line breaking
#+LATEX_HEADER:  showlines=true,                  % sets automatic line breaking
#+LATEX_HEADER:  captionpos=b,                     % sets the caption-position to bottom
#+LATEX_HEADER:  commentstyle=\color{gray30},      % comment style
#+LATEX_HEADER:  extendedchars=true,               % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
#+LATEX_HEADER:  frame=single,                     % adds a frame around the code
#+LATEX_HEADER:  keepspaces=true,                  % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
#+LATEX_HEADER:  keywordstyle=\color{amblu},       % keyword style
#+LATEX_HEADER:  procnamestyle=\color{amred},       % procedures style
#+LATEX_HEADER:  language=[95]fortran,             % the language of the code
#+LATEX_HEADER:  numbers=none,                     % where to put the line-numbers; possible values are (none, left, right)
#+LATEX_HEADER:  numbersep=5pt,                    % how far the line-numbers are from the code
#+LATEX_HEADER:  numberstyle=\tiny\color{gray20}, % the style that is used for the line-numbers
#+LATEX_HEADER:  rulecolor=\color{gray20},          % if not set, the frame-color may be changed on line-breaks within not-black text (\eg comments (green here))
#+LATEX_HEADER:  showspaces=false,                 % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
#+LATEX_HEADER:  showstringspaces=false,           % underline spaces within strings only
#+LATEX_HEADER:  showtabs=false,                   % show tabs within strings adding particular underscores
#+LATEX_HEADER:  stepnumber=2,                     % the step between two line-numbers. If it's 1, each line will be numbered
#+LATEX_HEADER:  stringstyle=\color{amdove},       % string literal style
#+LATEX_HEADER:  tabsize=2,                        % sets default tabsize to 2 spaces
#+LATEX_HEADER:  % title=\lstname,                    % show the filename of files included with \lstinputlisting; also try caption instead of title
#+LATEX_HEADER:  procnamekeys={call}
#+LATEX_HEADER: }
#+LATEX_HEADER: \newcommand{\prettysmall}{\fontsize{6}{8}\selectfont}
#+LATEX_HEADER: \let\oldtexttt=\texttt
#+LATEX_HEADER: \renewcommand\texttt[1]{\oldtexttt{\smaller[1]{#1}}}
#+LATEX_HEADER: \usepackage[binary-units]{siunitx}
#+LATEX_HEADER: \usepackage{enumitem}
#+LATEX_HEADER: \usepackage[mode=buildnew]{standalone}

#+LaTeX: \newcommand\labspace[1][-0.25cm]{\vspace{#1}}

* LaTeX Preamble                                                     :ignore:
#+BEGIN_EXPORT latex
\let\oldcite=\cite
\renewcommand\cite[2][]{~\ifthenelse{\equal{#1}{}}{\oldcite{#2}}{\oldcite[#1]{#2}}\xspace}
\let\oldref=\ref
\def\ref#1{~\oldref{#1}\xspace}
\def\eqref#1{~(\oldref{#1})\xspace}
\def\ie{i.e.,\xspace}
\def\eg{e.g.,\xspace}
\def\etal{~\textit{et al.\xspace}}
\newcommand{\AL}[2][inline]{\todo[caption={},color=green!50,#1]{\small\sf\textbf{AL:} #2}}
\newcommand{\TOM}[2][inline]{\todo[caption={},color=blue!50,#1]{\small\sf\textbf{TOM:} #2}}
\newcommand{\CH}[2][inline]{\todo[color=red!30,#1]{\small\sf \textbf{CH:} #2}}

%% Omit the copyright space.
%\makeatletter
%\def\@copyrightspace{}
%\makeatother

%\def\IEEEauthorblockN#1{\gdef\IEEEauthorrefmark##1{\ensuremath{{}^{\textsf{##1}}}}#1}
%\newlength{\blockA}
%\setlength{\blockA}{.35\linewidth}
%\def\IEEEauthorblockA#1{
%  \scalebox{.9}{\begin{minipage}{\blockA}\normalsize\sf
%    \def\IEEEauthorrefmark##1{##1: }
%    #1
%  \end{minipage}}
%}
% \def\IEEEauthorrefmark#1{#1: }

\title{Emulating High Performance Linpack on a Commodity Computer at the Scale of a Supercomputer}
%\title{Simulating the Energy Consumption of MPI~Applications}
% Predicting the Performance and the Power Consumption of MPI Applications With SimGrid
  %\titlerunning{Power-aware simulation for large-scale systems with SimGrid}
  %

  \author{
  \IEEEauthorblockN{
  Tom Cornebize, Franz C. Heinrich, Arnaud Legrand}
  \IEEEauthorblockA{Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, 38000 Grenoble, France\\
    tom.cornebize@univ-grenoble-alpes.fr, franz-christian.heinrich@inria.fr, arnaud.legrand@imag.fr}
  }


  \maketitle              % typeset the title of the contribution
#+END_EXPORT
* Abstract                                                           :ignore:
#+LaTeX: \begin{abstract}
The Linpack benchmark, in particular the High-Performance Linpack
(HPL) implementation, has emerged as the de-facto standard benchmark
to rank supercomputers in the top500. With a power consumption of
several MW per hour on a TOP500 machine, test-running HPL on the whole
machine for hours is extremely expensive. With core-counts beyond the
100,000 cores threshold being common and sometimes even ranging into
the millions, an optimization of HPL parameters (problem size, grid
arrangement, granularity, collective operation algorithms, etc.)
specifically suited to the network topology and performance is
essential. Such optimization can be particularly time consuming and
can hardly be done through simple mathematical performance models. In
this article, we explain how we both extended the SimGrid's SMPI
simulator and slightly modified HPL to allow a fast emulation of HPL
on a single commodity computer at the scale of a supercomputer. More
precisely, we take as a motivating use case the large-scale run
performed on the Stampede cluster at TACC in 2013, when it got ranked
6th in the top500. While this qualification run required the
dedication of 6006 computing nodes of the supercomputer and more than
\SI{120}{\tera\byte} of RAM for more than 2 hours, we manage to simulate a similar
configuration on a commodity computer with \SI{16}{\giga\byte} of RAM in about 37
hours. Allied to a careful modeling of stampede, this simulation
allows us to evaluate the performance that would have been obtained
using the freely available version of HPL. Such performance reveals much
lower than what was reported and which was obtained using a
closed-source version specifically designed by the Intel
engineers. Our simulation allows us to hint where the main algorithmic
improvements must have been done in HPL. We conclude with a brief
illustration of typical capacity planning studies enabled by our
proposal. 

# With supercomputers growing both in size and popularity, it has become
# important to reduce their usage for the optimization of application
# codes rather than serious research. Simulation is well-known to be
# able to aid researchers to study the behavior of massively parallel
# applications. Alas, running these simulations at the scale of the
# largest supercomputers has been practically infeasible, rendering it
# almost impossible to faithfully predict resource-hungry applications. 
# In this work, we show how we adapted HPL and the SMPI simulator of the SimGrid
# simulation framework to predict HPL's behavior on the 6,006 nodes of
# the Stampede1 cluster. We first outline how we reduced the time spent executing
# code from HPL to only \SI{0.3}{\percent} without loosing accuracy by
# using empirically obtained
# performance models for the computation kernels before we discuss the
# network and communication models used
# by SMPI and how we modeled an accelerator-based cluster such as Stampede.
# We finally demonstrate the practical applicability of our
# approach through the simulation of HPL at scale on a commonly
# available single server node by accurately predicting its
# runtime on a \SI{120}{\tera\byte} large problem instance that was used
# to evaluate the Stampede1 system for the Top500 list.
#+LaTeX: \end{abstract}


#+BEGIN_EXPORT latex
% this is need to trim the number of authors and et al. for more than 3 authors
\bstctlcite{IEEEexample:BSTcontrol}
#+END_EXPORT
* Introduction

The world's largest and fastest machines are often evaluated using
benchmarks and ranked twice a year in the so-called
TOP500 list. Today, the Linpack benchmark, in particular the High-Performance Linpack (HPL)
implementation, has emerged as the de-facto standard benchmark, although
other benchmarks such as HPCG and HPGMG have been proposed. With
core-counts beyond the 100,000 cores threshold being common and sometimes
even ranging into the millions, diligent optimization of application
parameters, such as problem size or process organization, 
become a necessity. To yield the best benchmark results,
runtimes (such as OpenMPI) and supporting libraries such as LAPACK
(for HPL in particular the DGEMM routine) need to be fine-tuned and adapted to the
underlying platform (network) as well. Lastly, the platform itself and
its network need to be setup in a way that HPL can efficiently use the
resources it runs on. Only with these optimizations in place can peak
performance be attained. 

Alas, the time it takes to run HPL on the number one system has
sharply increased and took around 17 hours on the 33rd TOP500 list in
2012 
\CH{TODO update this!; see http://www.icl.utk.edu/~luszczek/pubs/post165s1.pdf}. With a power
consumption of several MW per hour on a TOP500 machine, test-running HPL on the whole
machine for hours becomes financially infeasible. On the other hand,
the performance of a current-generation machine typically also
plays a role in funding for future machines and is hence critical for
HPC centers. It is hence beneficial to be able to estimate
HPL performance outcomes without actually running the benchmark. This
can be done either via (mathematical) performance models (e.g., by
estimating performance of specific functions) or by a simulation based approach.
While estimating runtime through performance models neglects the
impact of the network (congestion, bandwidth, ...), simulation is
potentially able to consider these important factors. Furthermore,
simulations can be used to validate/check that the execution went well
(operated near the peak performance) but can also help to find the
right parameters for the application, runtime and network.

In this article, we explain how to perform such simulations using
SimGrid's SMPI simulator. We particularly detail how we obtained
faithful models for several functions (DGEMM, ...) and how we managed
to reduce a memory consumption of hundreds of terrabyte to several
Gigabytes, allowing us to simulate HPL on a commonly available laptop.
We furthermore demonstrate the effectiveness of our solution by
simulating a large-scale scenario run on the STAMPEDE cluster at TACC
and submitted to the TOP500 in *YEAR*.

The remainder of this article is structured as follows: Section\ref{sec:con}
discusses ...
Section\ref{sec:relwork} gives references to already existing
related work. In Section\ref{sec:em}, we discuss extensively the
optimizations on all levels (\ie simulator, application, system) that were necessary in order to make a large-scale run
feasible. The scalability of our approach is evaluated in
Section\ref{sec:scalabilityevol}. The modeling of the platform, the
network and the communication is detailed in
Section\ref{sec:science}. Lastly, Section\ref{sec:cl} concludes this
paper by summarizing our work and results.
  
* Context
#+LaTeX: \label{sec:con}

# The HPLinpack benchmark consists of a set of rules: A set of linear
# equations, $Ax = b$, needs to be solved and it requires furthermore that the input matrix can be of
# arbitrary dimension =n= and that O(n³) + O(n²) operations be used
# (hence, Strassen's matrix multiplication is prohibited).

** HPL
For this work, we use the reference-implementation of the HPLinpack
benchmark, HPL, that is freely available \CH{cite} and widely used to benchmark systems.
HPL solves a linear system of equations, more precisely the problem
$Ax = b$ with $A$ being a square, real matrix with dimensions of size
$n$ and consisting of double precision floats. 

To solve this system, HPL uses a lower-upper (LU) matrix decomposition
with row partial pivoting, i.e., HPL computes a factorization of
matrix $A$ into two triangle matrices $L$ ("lower") and $U$ ("upper")
such that $A=LU$ holds true.

The nodes used to run HPL are organized in a virtual $P \times Q$ grid and
the data of the $n \times n+1$ extended coefficient matrix is distributed
onto this grid by cutting the matrix first into $n/N_{B}$ square blocks of size $N_{B}$
and then cyclically assigning a block to a process in the process grid.

\CH{See my journal entry on 2017-10-04. Describe the broadcast here; introduce the 6 algorithms; explain that panels are being broadcast to other nodes}

Older versions of MPI only supported non-blocking point-to-point
communications but did not support non-blocking collective
communications. However, HPL ships with in total 6 self-implemented,
point-to-point based broadcast algorithms to efficiently overlap the
time spent waiting for an incoming panel with updates to e.g. the trailing matrix. 

Every host that is waiting for a panel to arrive enters a loop and
tests in each iteration whether or not the panel has been received by
calling =MPI_Iprobe=. If the panel has not been received yet, updates to
parts of the trailing matrix are made and row-interchanges are applied.
\CH{Check exactly what is being done here, and using which panel; see =HPL_pdupdateNT.c= and the comment of the function ("Purpose"). There are several panels involved.}
As soon as =MPI_Iprobe= returns that the panel has been fully received,
no more updates are performed and the received panel is forwarded to
the next host and only after this has been done are the remaining updates finished.

Unfortunately, in version 2.2 of HPL, this overlapping is only enabled
in four out of six algorithms as the =bandwidth= and the =bandwidth
modified= algorithms seem to have had issues on some machines with getting stuck due to
too many messages.
\CH{See HPL_blonM.c, ll. 264 ff.}

\AL{See www/algorithm.html and describe the main params: 0) geometry, size and granularity, 1) Panel Broadcast, 2) Look-Ahead, 3) Update.}
** A typical run on a supercomputer
\AL{Describe stampede and the stampede fullrun}

** Difficulties
#+LaTeX: \label{sec:con:diff}

\AL{Too detailed on simulation. Difficulties = difficulties in term of performance prediction, i.e., application, runtime and platform and no formula may account for such complexity. These difficulties are rather conclusions of the related work section.}
   Several difficulties were well-known and had to be resolved in
   order to simulate HPL:

   1. The time-complexity of the algorithm is $\mathcal{O}(N^3)$ and
      $\mathcal{O}(N^2)$ communications are performed, with $N$ being
      very large. This causes executions of large problem sizes to
      become rather slow. For instance, the run on the Stampede cluster took almost
      two hours with $N=3,875,000$.
      
   2. Each node of a large cluster only allocates memory for a part of
      the whole matrix. With 4422\nbsp{}nodes, the Stampede run required
      \SI{120}{\tera\byte} of memory. A simulation running and executing HPL on only one
      single node will hence require this amount of data to be available on that particular
      node. It is hence vital to reduce the amount of memory for a
      simulation to become feasible.
      \CH{Tom's slides say the Stampede run was 6,006 MPI processes. I thought it was 1 process per node - where are the other processes coming from?}

      \CH{Should I already mention the pagetable size here - "not only the amount of memory itself but also the size of the pagetable becomes problematic"}
      
   3. Since HPL implements its own broadcast strategies, simulation is
      no longer sufficient as these strategies are vital for HPL's
      performance. Hence, emulation is required.


* Related Work
#+LaTeX: \label{sec:relwork}
  
Two approaches are commonly used in order to study a parallel
application with the help of a simulator: Offline and online simulation.

Offline simulation denotes a rather static approach: First, the
application is executed on a real machine and a tracefile with all the
important events (calls to MPI functions, computations) is
generated, with the events being time-independent (i.e., only the
order of their appearance is relevant). Offline simulation is static
as the traces contain only information about a single run and give no
hints about how, for instance, changes to the topology may impact the
communication patterns. To study these effects with offline simulation
is tedious as it requires the researcher to obtain new traces. 

Most simulators available today, among them BigSim\ref{bigsim_04},
Dimemas\ref{dimemas} and CODES\ref{CODES}, allow users to replay a
trace, \ie they support offline simulation. 
Alas, this approach is unusable in the case of HPL due to the size of the obtained traces and the complexity of
the application, as HPL implements for instance several broadcast
strategies that influence the performance significantly.

It is for these reasons necessary to not simulate, but emulate HPL.

A broad selection of tools enabling researchers to study MPI
applications on complex platforms exists. The extreme-scale simulator
xSim\cite{xsim}, although it is not publicly available, 
SST\cite{sstmacro} just as SimGrid/SMPI\cite{simgrid} all support online
emulation.
\CH{This needs to be expanded}
    

* Emulation mechanisms
#+LaTeX: \label{sec:em}
** TODO MPI process representation (mmap vs. dlopen)
SimGrid folds parallel applications into a single process and hence,
local static and global variables become an issue as it must be guaranteed that
each rank has its own set of global variables. SMPI supports two
mechanisms to achieve this: The usage of either =mmap= or =dlopen=.
*** mmap

When =mmap= is used, SMPI copies the =data= segment on startup for each
rank into the heap. When control is transferred from one rank to
another, the =data= segment is =mmap='ed to the location of this rank's
copy on the heap. All ranks have hence the same addresses in the
virtual address space at their disposition although they point to
different physical addresses based on the rank. This also means
inevitably that caches must be flushed to ensure that no data of one
rank leaks into the other rank. This overhead makes the usage of =mmap=
a rather expensive operation.
\TOM{Can you tell me how often these operations were executed, as you've already done in your journal on 2017-04-11 ("Looking at the syscalls")?}

*** TODO dlopen

With =dlopen=, copies of the global variables are still made but they
are stored inside the =data= segment as opposed to the heap. When
switching from one rank to another, the starting virtual address for
the storage is readjusted rather than the addresses point
to. This means that each rank has its own unique pool of addresses for
global variables. The advantage of this is that caches do not need to
be flushed as is the case for the =mmap= approach, because data
consistency can always be guaranteed.
\CH{This needs to be reviewed.}

*** Impact of choice of mmap/dlopen

The choice of mmap or dlopen influences the simulation time indirectly
through its direct impact on system/user time and page faults.

\CH{See also the note in the org-file at this place for expansion.}

# See Tom's journal; there are some graphs that we might be able to use,
# such as in https://github.com/Ezibenroc/m2_internship_journal/blob/master/simgrid_privatization/

** STARTED Kernel modeling: Affine, easy replacement, almost no code modification.
       HPL heavily relies on commonly available BLAS functions such as
       =dgemm= (for matrix-matrix multiplication) or =dtrsm= (for solving
       an equation of the form $Ax=b$). Our analysis of an
       HPL simulation with a relatively small matrix with dimensions 30,000 and
       64 processes has shown that around \SI{96}{\percent} of the time is spent in these
       two functions. 
       
       As explained in Section\ref{sec:con:diff}, faithful prediction requires 
       emulating HPL, i.e., to execute the code. Therefore, immediate and significant time savings can be
       realised by obtaining and using a parameter-aware performance model of =dgemm= and
       =dtrsm=. By making SMPI aware of this model, calls to the =dgemm= and
       =dtrsm= functions can be replaced by calls to and evaluation of
       the model. This is possible as HPL's code does not depend on the computed values of these
       functions. 
       Note that the parameters to the original functions
       are always passed to the model, as they are crucial for the
       computed runtime of these functions, and that the outcome of
       the HPL run is no longer correct.

       The execution time determined in this manner is then
       used as an argument to =smpi_usleep=, which makes the
       process enter a sleep-state for the entire duration,
       effectively advancing the clock for that process by the same
       amount as the execution would have. 

       This function is not normally found in HPL and had to be
       inserted manually. By defining the following preprocessing macro, the code
       modifications were kept to an absolute minimum:

#+BEGIN_SRC C
#define HPL_dtrsm(layout, Side, Uplo, TransA, Diag, M, N, alpha, A, lda, B, ldb) ({\
    double expected_time = (9.246e-08)*(double)M*(double)N - 1.024e-05;\
    if(expected_time > 0)\
        smpi_usleep((useconds_t)(expected_time*1e6));\
})
#+END_SRC

\CH{Found this in Tom's logbook. Check if this is the final version. Also, we can apparently just call ~make SMPI_OPTS=-DSMPI_OPTIMIZATION~ (what about ~arch=SMPI~?). See his logbook}

** TODO Other HPL adaptations:
#+LaTeX: \label{sec:hplchanges}

HPL uses huge pseudo-randomly generated matrices that need to be setup
every time HPL is executed. In order to minimize the impact of this
setup procedure on potential results, HPL does not account for the
time spent setting up the matrices. Likewise, the validation of the
computed results are also not accounted for by default. As they do not
impact the performance of the platform, we can safely skip both steps:
\CH{How do we initialize the matrix? See Tom's report on page 20, it doesn't explain that}
The verification, on the other hand, is meaningless as our
computations are wrong due to our reduction of the matrix to one
single panel.
\CH{This is explained in the following section so we need to move this}

Although the lion's share of computation time was consumed by calls to
=dgemm= and =dtrsm=, several other functions were identified through
profiling as computationally expensive enough to justify handling them
differently: In total seven BLAS functions such as =dgemv= or =dswap= and
five HPL functions. All of these functions are called during the LU
factorization and hence accounted for by HPL; however, they all
operate on bogus data and hence produce bogus data. We also determined
that the they are not slow enough to be modeled seperately and they were hence just removed.

\CH{See Tom's labbook; he added an option ~-DSMPI_DO_INITIALIZATION_VERIFICATION~ because there were some performance issues without the initialization} - Handling "sensitive" parts (the max pivot computation)
** TODO Memory folding
   
   We've already explained how the execution of several kernels was
   replaced with a performance model. It is clear that, as we do no
   longer operate on the data for real, storing the whole matrix $A$ (and
   hence the "real" data) is no longer a requirement. On the other
   hand, processes still read or write in "their" parts of the matrix. A consequence from
   removing most of the data is that the 
   aforementioned, dire memory situation (caused by the fact that all
   the data needs to be stored on one single node instead of
   potentially thousands) is alleviated.
   \CH{Reference memory statistics that should've appeared before}
   
   We will now explain how this reduction was achieved.
   
   HPL's pre-dominant datastructure, the =panel=, consists of both
   shared and private memory. This is illustrated in *Figure REF*.
   In this context, =shared= memory means memory that can be written to
   and read from by all processes; the actual value of this memory
   section is of little importance.
   =private= memory, on the other hand, is sensitive, process-dependent memory that must be
   protected from read-/write accesses by other processes. Failing to
   do so may result in classical invalid memory accesses or even
   deadlocks, as processes may not send/receive to/from the right process.
   An HPL =panel= contains not only matrix data (which we can share,
   as it doesn't need to be protected) but also
   indices that need to be always coherent and that are therefore private.
   Thankfully, a datastructure with some private and some shared
   elements, called a partially shared datastructure, does not need to be
   completely private. In SMPI, it is supported through a call to
   =SMPI_PARTIAL_SHARED_MALLOC=, which works as follows: (*From the SimGrid Doc*)
   
   #+BEGIN_CENTER
   mem = SMPI_PARTIAL_SHARED_MALLOC(500, {27,42 , 100,200}, 2);
   #+END_CENTER
   
   In this example, 500 bytes are allocated to mem with the elements
   mem[27], ..., mem[41] and mem[100], ..., mem[199] being shared
   while all other remain private. See Figure *REFERENCE* for an
   exampler representation.
   \CH{Maybe the Figure from Slide 11 of Tom's presentation?}
   \CH{Should we explain how SHARED_MALLOC works in SimGrid? This is also in options.doc, search for SMPI_PARTIAL_SHARED_MALLOC}


   Designating memory explicitly as private, shared or partially
   shared is not only important in cases where memory is scarce, but
   also to improve performance. As SMPI is internally aware of the
   memory's visibility, it can avoid calling =memcopy= when large
   messages containing shared segments are sent from one MPI rank to
   another. In the cases of private data segments or partially shared
   segments, SMPI identifies and only copies those parts that are designated as
   private (as they are process-dependent) into the corresponding
   private buffers on the receiver side.

   In the case of HPL, this speeds up simulation times considerably,
   as the main datastructure that is being communicated between ranks,
   the =panel=, is a partially shared datastructure with the largest
   part being shared.

** STARTED Panel reuse

The original HPL code malloc's/free's panels *in each iteration (correct?)*.
SimGrid requires some extra effort to make the panels partially
shared, as described above, introducing an overhead. Alas, repeated
allocations / frees become a bottleneck due to this overhead. We hence
had to modify HPL to only allocate and use the very first panel, as it
has the property to be the largest (size-wise) panel used during
the whole execution. 

\CH{Do we want a small figure?}

** STARTED Huge pages    
    As described above, we fold the memory and realise significant
    physical memory savings. However, the allocations are still performed for
    and the /virtual/ memory is still allocated for every process. This
    implies that there is no reduction in the overall amount of
    virtual addresses, causing the page table to become too large to
    be efficiently maintained.

    In general, the size of the page table with page size of 4,096 bytes can be computed as:

    #+LATEX: \[ PT_{size}(N) = \frac{N^2 \cdot 8}{4,096} \cdot 8 \]
    
    \CH{Explain better what the two 8's mean: Size of doubles and entry size for a virtual address}

    This means that for a matrix of size $N=4,000,000$, the page table
    grows to 

    #+LATEX: \[ PT_{size}(4,000,000) = 2.5e11 \]

    bytes, i.e., to \SI{250}{\gibi\byte}. Resolving this problem requires
    administrator (root) privileges as the Linux kernel support for
    /hugepages/ needs to be activated. With hugepages enabled, page size is
    increased by the system from \SI{4}{\kibi\byte} to 
    \SI{2-256}{\mibi\byte}, depending on the
    configuration.\footnote{The current page size for hugetables is reported in /proc/meminfo} 
    
    In our case, setting the page size to \SI{2}{\mibi\byte} resulted in the page
    table to shrink from \SI{250}{\gibi\byte} to \SI{0.488}{\gibi\byte}.
    
    It is also noteworthy that using hugetables decreases the amount
    of page faults.\CH{Do we have performance data here; how much faster are we? See https://github.com/Ezibenroc/m2_internship_journal/tree/master/page_faults}

* Scalability Evaluation
#+LaTeX: \label{sec:scalabilityevol}

In Section\ref{sec:em} we described the work we did in order to run a
large-scale simulation on a single node. We will now present the
results of our evaluation.\footnote{For more information, see the labbook in file =intern_report.org=, available at https://github.com/Ezibenroc/m2_internship_journal/}
\TOM{We need a reference to your Msc thesis; we need to add that each modification has been investigated}

Although our goal is to model and simulate HPL on the Stampede
platform eventually, we decided to produce some first results on a
similar, albeit non-existing platform with the following, particular features:

#+LATEX: \begin{enumerate}
#+LATEX: \item
  In total, *XYZ nodes* make up the platform. Each node consists only of
  a single CPU with *XYZ cores*; there are no accelerators / GPU's.
#+LATEX: \item
  A fat-tree network topology with an interconnect of *XYZ bandwidth/latency*
#+LATEX: \item

#+LATEX: \end{enumerate}

\CH{I need to figure out what the configuration is in order to work on this: "Just showing that when using the default SMPI, it works but it's obviously slow."}

#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth,page=2]{./figures/scalability_plot_size.pdf}                                                                                                                               
  \includegraphics[width=\linewidth,page=2]{./figures/scalability_plot_nbproc.pdf}
  \caption{Time complexity is linear in the number of processes with a fixed matrix size but becomes slightly quadratic when matrix size is varied. }
  \label{fig:hpl_scalability}
  \labspace
\end{figure}
#+END_EXPORT
\TOM{In this Figure, we do not have data for 3e6. We should remove the tick here.}

Figure\ref{fig:hpl_scalability} compares the impact of changes to
process number or matrix size on total makespan and memory. A total of
four different process numbers were used, namely 512, 1024, 2048
and\nbsp{}4096 processes. The matrix size was selected out of four available
sizes as well, in this case $0.5\cdot10^{6}, 10^{6}, 2\cdot10^{6}$ and $4\cdot10^{6}$.

In the first and second row, the matrix size and number of processes, respectively,
are varied. When the matrix size ($N$) is changed, as depicted in the
two panels of the first row, memory consumption and
simulation time grow slightly quadratic as the amount of matrix
elements grows quadratically ($N^{2}$) and more iterations of the
algorithm are required to solve the equation system. 
\CH{This needs to be verified, it just seems logical to me. Why is the memory consumption not growing quadratically and why does it consume around 6 GB?}

As becomes apparent when studying the results shown in the lower two
panels, a linear connection of simulation time and number of processes
exists when the matrix size is fixed. However, the slope of the linear
functions are clearly different; the larger the matrix, the steeper
the slope. An explanation for this is that the algorithm requires more
iterations for larger matrices and hence requires more panels to be
sent over the network, causing the simulator to re-compute the somewhat
(computation-wise) expensive network congestion information.
\CH{Were the nodes sharing some links?}

It is furthermore noteworthy that the memory consumption in this case
is very uniform; the matrix size determines the "lower barrier" for
the memory consumption and a constant amount of memory is then added
for the private memory that every process requires. This also explains why
the distance between any pair of linear functions is constant.
\CH{My god, I interpreted this just from the plots. It makes sense and looks like it, but this really needs to be verified.}
\CH{What is the size of the largest (= first) panel in each case?}

* Scientific part
#+LaTeX: \label{sec:science}

** Modeling Stampede

*** Computations

The Stampede cluster contains *XYZ* compute nodes, each with two 8-core Intel Xeon
E5-2680 8C \SI{2.7}{\GHz} CPU and one, for very few nodes even two 61-core Intel Xeon Phi SE10P (KNC) with
\SI{1.1}{\GHz} accelerator. The accelerators are essential to the performance
of the cluster, delivering 7 PFlop/s of sustainable performance
whereas the CPUs are only capable of delivering 2 PFlop/s. On
matrices of the size used for this work, however, CPUs are barely used.

# See CH's journal from [2017-10-03 Tue] to see how the version was determined
The Xeon Phis can be used in two ways: First, as a classical
accelerator, i.e., to offload expensive computations from the CPU onto
the accelerator. We used Intel's Math Kernel Library (MKL) version
11.1.1 that comes with support for automatic offloading for several BLAS
functions. In the case of DGEMM, the matrix dimensions determine
whether or not the computation is transferred to the KNC: If both
dimensions of the matrix are larger than $1280$, the computation is
offloaded.
\CH{And K > 256? See also here: https://software.intel.com/en-us/articles/intel-mkl-automatic-offload-enabled-functions-for-intel-xeon-phi-coprocessors}

The second way the Xeon Phi's can be used is by compiling binaries for
and executing them on the Xeon Phi. While the accelerator's memory of \SI{8}{\gibi\byte} is rather
small, its main advantage is that data does not need to be
transferred from the node's CPU to the accelerator via the x16 PCIe bus.
\CH{Not done here}


*** Communications

SMPI's communication model is a hybrid model between the LogP family
and a fluid model, supporting different modes for the send operation
such as fully asynchronous, detached or eager. For each message, the
mode used is determined solely on the message size. It is hence
possible to model the resulting performance through a piece-wise,
linear model, as depicted in Figure\ref{fig:stampede_calibration}.
For a thorough discussion of the calibration techniques, see\ref{smpi}.

SMPI uses this model by default, however, it is not leveraged in the
particular case of HPL as its communication patterns use primarily large, bulk messages,
making small messages scarce. Almost all messages are hence only sent
via the /detached/ mode (depicted in green).
\CH{I'm not sure why we chose the breakpoint the way it is for detached; this is not obvious?}

#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth,page=1]{./figures/stampede_calibration_send.png}
  \caption{Visualization of the analysis results as obtained on the Stampede system.}
  \label{fig:stampede_calibration}
  \labspace
\end{figure}
#+END_EXPORT

*** Network topology

#+BEGIN_EXPORT latex
%% 
%% This draws a fat tree. If you want to change its appearance, see the \size macro.
%%
\tikzstyle{switch}=[draw, circle, minimum width=1cm, minimum height = 1cm]
\tikzstyle{compute}=[draw, rectangle, minimum width=0.5cm, minimum height = 0.5cm, node distance=0.5cm]
\tikzstyle{base}=[ellipse, minimum width=2cm, minimum height = 0.5cm, node distance = 0.5cm]
\tikzstyle{bigswitch}=[base, draw]
\begin{figure}[t]
  \centering
	\begin{tikzpicture}[scale=0.4,transform shape]
	    \pgfmathtruncatemacro{\size}{3}    % Number of ports in a switch, THE PARAMETER TO CHANGE
	    \pgfmathtruncatemacro{\sizesquare}{\size*\size}
	    \pgfmathtruncatemacro{\boundSwitch}{\size-1}
	    \pgfmathtruncatemacro{\boundCompute}{\size*\size-1}
	    % Compute nodes
	    \foreach \i in {0,...,\boundSwitch}{
		\pgfmathtruncatemacro{\incr}{\sizesquare*\i}
		\foreach \x in {0,...,\boundCompute} {
              -- CH: I added the +\boundSwitch here to move the nodes to the right
		    \pgfmathtruncatemacro{\z}{\x+\incr+\boundSwitch}
		    \pgfmathsetmacro{\pos}{\z/2}
		    \node[compute] (c_\z) at (\pos, 0) {} ;
		}
	    }
	    % Switches L1 and L2
	    \foreach \i in {0,...,\boundSwitch}{
		\pgfmathtruncatemacro{\incr}{\size*\i}
		\foreach \x in {0,...,\boundSwitch} {
		    \pgfmathtruncatemacro{\z}{\x+\incr}
              -- CH: I added the +\boundSwitch/1 here (was: /4) to move the nodes to the right
              -- (this moves them below the L3 layer)
		    \pgfmathsetmacro{\pos}{\incr*\size/2+\x*\size/2+(\boundSwitch/1)}
		    \node[switch] (l1_\z) at (\pos, 4) {} ;
		    \node[switch] (l2_\z) at (\pos, 8) {} ;
		}
	    }
	    % Edges of the islets
	    \foreach \i in {0,...,\boundSwitch}{
		\pgfmathtruncatemacro{\incr}{\size*\i}
		\foreach \switch in {0,...,\boundSwitch} {
		    \pgfmathtruncatemacro{\sw}{\switch+\incr}
		    \foreach \y in {0,...,\boundSwitch} {
                  -- CH: I added the +\boundSwitch here. This corrects the edges for the leafs. 
			\pgfmathtruncatemacro{\comp}{\switch*\size+\y+\incr*\size+\boundSwitch}
			\draw (l1_\sw.south) -- (c_\comp.north);
		    }
		    \foreach \root in {0,...,\boundSwitch} {
			\pgfmathtruncatemacro{\ro}{\root+\incr}
			\draw (l1_\sw.north) -- (l2_\ro.south);
		    }
		}
	    }
	    \node (l1) at (-1, 4) {\Huge $L_1$} ;
	    \node (l2) at (-1, 8) {\Huge $L_2$} ;
	    \node (l3) at (-1, 12) {\Huge $L_3$} ;

	    \pgfmathtruncatemacro{\boundDoubleSwitch}{\size*2-1}
	    % Switches L3
	    \foreach \x in {0,...,\boundDoubleSwitch} {
		\pgfmathsetmacro{\pos}{\x*\size+(\boundSwitch/2)}
		\node[switch] (l3_\x) at (\pos, 12) {} ;
	    }
	    % Upper edges
	    \foreach \root in {0,...,\boundDoubleSwitch} {
		\foreach \switch in {0,...,\boundCompute} {
		    \pgfmathtruncatemacro{\switchmod}{mod(\switch,\size)}
		    \pgfmathtruncatemacro{\rootmod}{mod(\root,\size)}
		    \ifthenelse{\equal{\switchmod}{\rootmod}}{
			\draw (l2_\switch.north) -- (l3_\root.south);
		    }{}
		}
	    }
	\end{tikzpicture}
      \caption{\label{fig:fat_tree}A fat tree}
    \end{figure}
#+END_EXPORT

# Interestingly, the Stampede website says it's "56 GB/s" but I
# checked and can only find 56 Gbit/s. I hence go with that, as it
# seems more reasonable as well.

Stampede leverages Mellanox FDR InfiniBand technology with
\SI{56}{\giga\bit}/s, setup in a
fat-tree topology on two levels (called cores and
leafs) with 8\nbsp{}SX6536 core-switches, each with 648\nbsp{}ports and
\SI{73}{\tera\bit}/s capacity, and 320\nbsp{}36-port
leaf-switches with a capacity of \SI{4}{\tera\bit}/s. 
20\nbsp{}ports of each leaf-switch are connected to compute nodes, whereas the remaining 16\nbsp{}ports are connected to
the core-switches. In this topology, any message reaches the
destination within no more than 5\nbsp{}hops.

Figure\ref{fig:fat_tree} depicts a fat-tree with *4 (?) levels* (*I'm tired now.*)

** Running at scale

Our simulations have been executed on a single node of the Nova
cluster, provided by the Grid'5000 platform and featuring 
two 8-core Xeon E5-2620\nbsp{}v4\nbsp{}CPUs with \SI{2.1}{\GHz} and
\SI{32}{\giga\byte} RAM. Although these CPUs are
different from the ones used by Stampede, we did not account for
these differences since, as discussed in \ref{sec:em}, the majority of
computations was either removed or replaced with performance models.
\AL{Do we need to cite G5K?}

On this platform, a simulation run took us around 30\nbsp{}hours.
\CH{Not done here.}

* Conclusions
#+LaTeX: \label{sec:cl}

Prediction of makespan of applications running on large-scale clusters
is an intricate problem. In this article, we explained the problems
that we encountered and how we adjusted parts of HPL to make
emulation feasible. Although we had to change or remove some of the source code of the
program, changesets remained small and were applied to less than $1\%$ of
the code base. These modifications allowed us to run HPL on top of a
simulation framework, SimGrid / SMPI, using just a commodity laptop
instead of a cluster with several thousand nodes.

We also pointed out that not only the application or the runtime may
render an out-of-the-box approach at large-scale infeasible but that
the kernel configuration may be the cause as well. More specifically,
we showed that performance can become unsupportable due to page table
sizes, when support for huge pages is not activated.

Although being capable of predicting an application's performance on a
platform is by itself interesting, we believe that this will become
invaluable in the future to aid compute centers with the decision of
whether a new machine will work best for a given application or if an
upgrade of the current machine should be considered. This goal will be
subject to a more thorough investigation in the very near future.

As we saw in Section\ref{sec:hplchanges}, two BLAS functions (=dgemm=
and =dtrsm=) were the dominating factor with regards to the runtime although other BLAS
functions were called as well. For this study, we neglected the other
functions but with a fully automatic calibration procedure for any
BLAS function results could effortlessly become more precise as the
application could just be linked against a BLAS-replacement
library. 
\CH{Problem here: HPL uses HPL_dtrsm() wrappers.}

* Acknowledgements

Experiments presented in this paper were carried out using the Grid'5000 testbed, supported by a scientific interest group hosted by Inria and including CNRS, RENATER and several Universities as well as other organizations (see https://www.grid5000.fr).


* Emacs Setup 							   :noexport:
# Local Variables:
# eval:    (require 'org-install)
# eval:    (org-babel-do-load-languages 'org-babel-load-languages '( (shell . t) (R . t) (perl . t) (ditaa . t) ))
# eval:    (setq org-confirm-babel-evaluate nil)
# eval:    (unless (boundp 'org-latex-classes) (setq org-latex-classes nil))
# eval:    (add-to-list 'org-latex-classes '("IEEEtran"
# "\\documentclass[conference, 10pt]{IEEEtran}\n \[NO-DEFAULT-PACKAGES]\n \[EXTRA]\n  \\usepackage{graphicx}\n  \\usepackage{hyperref}"  ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")                       ("\\subsubsection{%s}" . "\\subsubsection*{%s}")                       ("\\paragraph{%s}" . "\\paragraph*{%s}")                       ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# eval:    (add-to-list 'org-latex-classes '("llncs" "\\documentclass{llncs2e/llncs}\n \[NO-DEFAULT-PACKAGES]\n \[EXTRA]\n"  ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")                       ("\\subsubsection{%s}" . "\\subsubsection*{%s}")                       ("\\paragraph{%s}" . "\\paragraph*{%s}")                       ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# eval:    (add-to-list 'org-latex-classes '("acm-proc-article-sp" "\\documentclass{acm_proc_article-sp}\n \[NO-DEFAULT-PACKAGES]\n \[EXTRA]\n"  ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")                       ("\\subsubsection{%s}" . "\\subsubsection*{%s}")                       ("\\paragraph{%s}" . "\\paragraph*{%s}")                       ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# eval:    (add-to-list 'org-latex-classes '("sig-alternate" "\\documentclass{sig-alternate}\n \[NO-DEFAULT-PACKAGES]\n \[EXTRA]\n"  ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")                       ("\\subsubsection{%s}" . "\\subsubsection*{%s}")                       ("\\paragraph{%s}" . "\\paragraph*{%s}")                       ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# eval:    (setq org-alphabetical-lists t)
# eval:    (setq org-src-fontify-natively t)
# eval:   (setq ispell-local-dictionary "american")
# eval:   (eval (flyspell-mode t))
# eval:   (setq org-todo-keyword-faces '(("FLAWED" . (:foreground "RED" :weight bold))))
# eval:   (custom-set-variables '(org-babel-shell-names (quote ("sh" "bash" "csh" "ash" "dash" "ksh" "mksh" "posh" "zsh"))))
# eval:   (add-to-list 'load-path ".")
# eval:   (require 'ox-extra)
# eval:   (setq org-latex-tables-centered nil)
# eval:   (ox-extras-activate '(ignore-headlines))
# End:
