# -*- coding: utf-8 -*-
# -*- org-confirm-babel-evaluate: nil -*-
# -*- mode: org -*-
#+TITLE:
#+LANGUAGE:  en
#+OPTIONS: H:5 author:nil email:nil creator:nil timestamp:nil skip:nil toc:nil ^:nil
#+TAGS: ARNAUD(a) CHRISTIAN(c) ANNE-CECILE(A)
#+TAGS: noexport(n) DEPRECATED(d) ignore(i)
#+TAGS: EXPERIMENT(e) LU(l) EP(e)
#+STARTUP: overview indent inlineimages logdrawer hidestars
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w@) | DONE(d!) CANCELLED(c@) DEFERRED(@) FLAWED(f@)
#+LATEX_CLASS: IEEEtran
#+PROPERTY: header-args :eval never-export
#+LATEX_HEADER: \usepackage{DejaVuSansMono}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: %\usepackage{fixltx2e}
#+LATEX_HEADER: \usepackage{ifthen,figlatex}
#+LATEX_HEADER: \usepackage{longtable}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{wrapfig}
#+LATEX_HEADER: \usepackage{subfigure}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{color,soul}
#+LATEX_HEADER: \usepackage[export]{adjustbox}
#+LATEX_HEADER: \usepackage{xspace}
#+LATEX_HEADER: \usepackage{amsmath,amssymb}
#+LATEX_HEADER: \usepackage[american]{babel}
#+LATEX_HEADER: \usepackage{relsize}
#+LATEX_HEADER: \AtBeginDocument{
#+LATEX_HEADER:   \definecolor{pdfurlcolor}{rgb}{0,0,0.6}
#+LATEX_HEADER:   \definecolor{pdfcitecolor}{rgb}{0,0.6,0}
#+LATEX_HEADER:   \definecolor{pdflinkcolor}{rgb}{0.6,0,0}
#+LATEX_HEADER:   \definecolor{light}{gray}{.85}
#+LATEX_HEADER:   \definecolor{vlight}{gray}{.95}
#+LATEX_HEADER: }
#+LATEX_HEADER: %\usepackage[paper=letterpaper,margin=1.61in]{geometry}
#+LATEX_HEADER: \usepackage{url} \urlstyle{sf}
#+LATEX_HEADER: \usepackage[normalem]{ulem}
#+LATEX_HEADER: \usepackage{todonotes}
#+LATEX_HEADER: \usepackage{fancyvrb}
#+LATEX_HEADER: \usepackage[colorlinks=true,citecolor=pdfcitecolor,urlcolor=pdfurlcolor,linkcolor=pdflinkcolor,pdfborder={0 0 0}]{hyperref}
#+LATEX_HEADER: \usepackage{color,colortbl}
#+LATEX_HEADER: \definecolor{gray98}{rgb}{0.98,0.98,0.98}
#+LATEX_HEADER: \definecolor{gray20}{rgb}{0.20,0.20,0.20}
#+LATEX_HEADER: \definecolor{gray25}{rgb}{0.25,0.25,0.25}
#+LATEX_HEADER: \definecolor{gray16}{rgb}{0.161,0.161,0.161}
#+LATEX_HEADER: \definecolor{gray60}{rgb}{0.6,0.6,0.6}
#+LATEX_HEADER: \definecolor{gray30}{rgb}{0.3,0.3,0.3}
#+LATEX_HEADER: \definecolor{bgray}{RGB}{248, 248, 248}
#+LATEX_HEADER: \definecolor{amgreen}{RGB}{77, 175, 74}
#+LATEX_HEADER: \definecolor{amblu}{RGB}{55, 126, 184}
#+LATEX_HEADER: \definecolor{amred}{RGB}{228,26,28}
#+LATEX_HEADER: \definecolor{amdove}{RGB}{102,102,122}
#+LATEX_HEADER: \usepackage{xcolor}
#+LATEX_HEADER: \usepackage[procnames]{listings}
#+LATEX_HEADER: \lstset{ %
#+LATEX_HEADER:  backgroundcolor=\color{gray98},    % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
#+LATEX_HEADER:  basicstyle=\tt\scriptsize,        % the size of the fonts that are used for the code
#+LATEX_HEADER:  breakatwhitespace=false,          % sets if automatic breaks should only happen at whitespace
#+LATEX_HEADER:  breaklines=true,                  % sets automatic line breaking
#+LATEX_HEADER:  showlines=true,                   % sets automatic line breaking
#+LATEX_HEADER:  captionpos=b,                     % sets the caption-position to bottom
#+LATEX_HEADER:  commentstyle=\color{gray30},      % comment style
#+LATEX_HEADER:  extendedchars=true,               % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
#+LATEX_HEADER:  frame=single,                     % adds a frame around the code
#+LATEX_HEADER:  keepspaces=true,                  % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
#+LATEX_HEADER:  keywordstyle=\color{amblu},       % keyword style
#+LATEX_HEADER:  procnamestyle=\color{amred},      % procedures style
#+LATEX_HEADER:  language=[95]fortran,             % the language of the code
#+LATEX_HEADER:  numbers=left,                     % where to put the line-numbers; possible values are (none, left, right)
#+LATEX_HEADER:  numbersep=5pt,                    % how far the line-numbers are from the code
#+LATEX_HEADER:  numberstyle=\tiny\color{gray20},  % the style that is used for the line-numbers
#+LATEX_HEADER:  rulecolor=\color{gray20},         % if not set, the frame-color may be changed on line-breaks within not-black text (\eg comments (green here))
#+LATEX_HEADER:  showspaces=false,                 % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
#+LATEX_HEADER:  showstringspaces=false,           % underline spaces within strings only
#+LATEX_HEADER:  showtabs=false,                   % show tabs within strings adding particular underscores
#+LATEX_HEADER:  stepnumber=2,                     % the step between two line-numbers. If it's 1, each line will be numbered
#+LATEX_HEADER:  stringstyle=\color{amdove},       % string literal style
#+LATEX_HEADER:  tabsize=2,                        % sets default tabsize to 2 spaces
#+LATEX_HEADER:  % title=\lstname,                    % show the filename of files included with \lstinputlisting; also try caption instead of title
#+LATEX_HEADER:  procnamekeys={call}
#+LATEX_HEADER: }
#+LATEX_HEADER: \definecolor{colorfuncall}{rgb}{0.6,0,0}
#+LATEX_HEADER: \newcommand{\prettysmall}{\fontsize{6}{8}\selectfont}
#+LATEX_HEADER: \let\oldtexttt=\texttt
#+LATEX_HEADER: \renewcommand\texttt[1]{\oldtexttt{\smaller[1]{#1}}}
# #+LATEX_HEADER: \usepackage[round-precision=3,round-mode=figures,scientific-notation=true]{siunitx}
#+LATEX_HEADER: \usepackage[binary-units]{siunitx}
#+LATEX_HEADER: \DeclareSIUnit\flop{Flop}
#+LATEX_HEADER: \DeclareSIUnit\flops{\flop\per\second}
#+LATEX_HEADER:\usepackage{tikz}
#+LATEX_HEADER:\usetikzlibrary{arrows,shapes,positioning,shadows,trees,calc}
#+LATEX_HEADER:\usepackage{pgfplots}
#+LATEX_HEADER:\pgfplotsset{compat=1.13}

#+LATEX_HEADER: \usepackage{enumitem}
#+LATEX_HEADER: \usepackage[mode=buildnew]{standalone}


#+LaTeX: \newcommand\labspace[1][-0.25cm]{\vspace{#1}}
#+LaTeX: \renewcommand\O{\ensuremath{\mathcal{O}}\xspace}%

* LaTeX Preamble                                                     :ignore:
#+BEGIN_EXPORT latex
\let\oldcite=\cite
\renewcommand\cite[2][]{~\ifthenelse{\equal{#1}{}}{\oldcite{#2}}{\oldcite[#1]{#2}}\xspace}
\let\oldref=\ref
\def\ref#1{~\oldref{#1}\xspace}
\def\eqref#1{~(\oldref{#1})\xspace}
\def\ie{i.e.,\xspace}
\def\eg{e.g.,\xspace}
\def\etal{~\textit{et al.\xspace}}
\newcommand{\AL}[2][inline]{\todo[caption={},color=green!50,#1]{\small\sf\textbf{AL:} #2}}
\newcommand{\TC}[2][inline]{\todo[caption={},color=blue!50,#1]{\small\sf\textbf{TOM:} #2}}
\newcommand{\CH}[2][inline]{\todo[color=red!30,#1]{\small\sf \textbf{CH:} #2}}

%% Omit the copyright space.
%\makeatletter
%\def\@copyrightspace{}
%\makeatother

%\def\IEEEauthorblockN#1{\gdef\IEEEauthorrefmark##1{\ensuremath{{}^{\textsf{##1}}}}#1}
%\newlength{\blockA}
%\setlength{\blockA}{.35\linewidth}
%\def\IEEEauthorblockA#1{
%  \scalebox{.9}{\begin{minipage}{\blockA}\normalsize\sf
%    \def\IEEEauthorrefmark##1{##1: }
%    #1
%  \end{minipage}}
%}
% \def\IEEEauthorrefmark#1{#1: }

\title{Emulating High Performance Linpack on a Commodity Computer at the Scale of a Supercomputer}
%\title{Simulating the Energy Consumption of MPI~Applications}
% Predicting the Performance and the Power Consumption of MPI Applications With SimGrid
  %\titlerunning{Power-aware simulation for large-scale systems with SimGrid}
  %

  \author{
  \IEEEauthorblockN{
  Tom Cornebize, Franz C. Heinrich, Arnaud Legrand}
  \IEEEauthorblockA{Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, 38000 Grenoble, France\\
    tom.cornebize@univ-grenoble-alpes.fr, franz-christian.heinrich@inria.fr, arnaud.legrand@imag.fr}
  }


  \maketitle              % typeset the title of the contribution
#+END_EXPORT
* Abstract                                                           :ignore:
#+LaTeX: \begin{abstract}
The Linpack benchmark, in particular the High-Performance Linpack
(HPL) implementation, has emerged as the de-facto standard benchmark
to rank supercomputers in the top500. With a power consumption of
several MW per hour on a TOP500 machine, test-running HPL on the whole
machine for hours is extremely expensive. With core-counts beyond the
100,000 cores threshold being common and sometimes even ranging into
the millions, an optimization of HPL parameters (problem size, grid
arrangement, granularity, collective operation algorithms, etc.)
specifically suited to the network topology and performance is
essential. Such optimization can be particularly time consuming and
can hardly be done through simple mathematical performance models. In
this article, we explain how we both extended the SimGrid's SMPI
simulator and slightly modified HPL to allow a fast emulation of HPL
on a single commodity computer at the scale of a supercomputer. More
precisely, we take as a motivating use case the large-scale run
performed on the Stampede cluster at TACC in 2013, when it got ranked
6th in the top500. While this qualification run required the
dedication of 6006 computing nodes of the supercomputer and more than
\SI{120}{\tera\byte} of RAM for more than 2 hours, we manage to simulate a similar
configuration on a commodity computer with \SI{16}{\giga\byte} of RAM in about 37
hours. Allied to a careful modeling of stampede, this simulation
allows us to evaluate the performance that would have been obtained
using the freely available version of HPL. Such performance reveals much
lower than what was reported and which was obtained using a
closed-source version specifically designed by the Intel
engineers. Our simulation allows us to hint where the main algorithmic
improvements must have been done in HPL. We conclude with a brief
illustration of typical capacity planning studies enabled by our
proposal. 

# With supercomputers growing both in size and popularity, it has become
# important to reduce their usage for the optimization of application
# codes rather than serious research. Simulation is well-known to be
# able to aid researchers to study the behavior of massively parallel
# applications. Alas, running these simulations at the scale of the
# largest supercomputers has been practically infeasible, rendering it
# almost impossible to faithfully predict resource-hungry applications. 
# In this work, we show how we adapted HPL and the SMPI simulator of the SimGrid
# simulation framework to predict HPL's behavior on the 6,006 nodes of
# the Stampede1 cluster. We first outline how we reduced the time spent executing
# code from HPL to only \SI{0.3}{\percent} without loosing accuracy by
# using empirically obtained
# performance models for the computation kernels before we discuss the
# network and communication models used
# by SMPI and how we modeled an accelerator-based cluster such as Stampede.
# We finally demonstrate the practical applicability of our
# approach through the simulation of HPL at scale on a commonly
# available single server node by accurately predicting its
# runtime on a \SI{120}{\tera\byte} large problem instance that was used
# to evaluate the Stampede1 system for the Top500 list.
#+LaTeX: \end{abstract}


#+BEGIN_EXPORT latex
% this is need to trim the number of authors and et al. for more than 3 authors
\bstctlcite{IEEEexample:BSTcontrol}
#+END_EXPORT
* Introduction

The world's largest and fastest machines are often evaluated using
benchmarks and ranked twice a year in the so-called
TOP500 list. Today, the Linpack benchmark, in particular the High-Performance Linpack (HPL)
implementation, has emerged as the de-facto standard benchmark, although
other benchmarks such as HPCG and HPGMG have been proposed. With
core-counts beyond the 100,000 cores threshold being common and sometimes
even ranging into the millions, diligent optimization of application
parameters, such as problem size or process organization, 
become a necessity. To yield the best benchmark results,
runtimes (such as OpenMPI) and supporting libraries such as LAPACK
(for HPL in particular the DGEMM routine) need to be fine-tuned and adapted to the
underlying platform (network) as well. Lastly, the platform itself and
its network need to be setup in a way that HPL can efficiently use the
resources it runs on. Only with these optimizations in place can peak
performance be attained. 

Alas, the time it takes to run HPL on the number one system has
sharply increased and took around 17 hours on the 33rd TOP500 list in
2012 
\CH{TODO update this!; see http://www.icl.utk.edu/~luszczek/pubs/post165s1.pdf}. With a power
consumption of several MW per hour on a TOP500 machine, test-running HPL on the whole
machine for hours becomes financially infeasible. On the other hand,
the performance of a current-generation machine typically also
plays a role in funding for future machines and is hence critical for
HPC centers. It is hence beneficial to be able to estimate
HPL performance outcomes without actually running the benchmark. This
can be done either via (mathematical) performance models (e.g., by
estimating performance of specific functions) or by a simulation based approach.
While estimating runtime through performance models neglects the
impact of the network (congestion, bandwidth, ...), simulation is
potentially able to consider these important factors. Furthermore,
simulations can be used to validate/check that the execution went well
(operated near the peak performance) but can also help to find the
right parameters for the application, runtime and network.

In this article, we explain how to perform such simulations using
SimGrid's SMPI simulator. We particularly detail how we obtained
faithful models for several functions (DGEMM, ...) and how we managed
to reduce a memory consumption of hundreds of terrabyte to several
Gigabytes, allowing us to simulate HPL on a commonly available laptop.
We furthermore demonstrate the effectiveness of our solution by
simulating a large-scale scenario run on the STAMPEDE cluster at TACC
and submitted to the TOP500 in *YEAR*.

The remainder of this article is structured as follows: Section\ref{sec:con}
discusses ...
Section\ref{sec:relwork} gives references to already existing
related work. In Section\ref{sec:em}, we discuss extensively the
optimizations on all levels (\ie simulator, application, system) that were necessary in order to make a large-scale run
feasible. The scalability of our approach is evaluated in
Section\ref{sec:scalabilityevol}. The modeling of the platform, the
network and the communication is detailed in
Section\ref{sec:science}. Lastly, Section\ref{sec:cl} concludes this
paper by summarizing our work and results.
  
* Context
#+LaTeX: \label{sec:con}

# The HPLinpack benchmark consists of a set of rules: A set of linear
# equations, $Ax = b$, needs to be solved and it requires furthermore that the input matrix can be of
# arbitrary dimension =n= and that O(n³) + O(n²) operations be used
# (hence, Strassen's matrix multiplication is prohibited).

** HPL
For this work, we use the reference-implementation of the HPLinpack
benchmark, HPL, that is freely available \CH{cite} and widely used to benchmark systems.
HPL solves a linear system of equations, more precisely the problem
$Ax = b$ with $A$ being a square, real matrix with dimensions of size
$n$ and consisting of double precision floats. 

To solve this system, HPL uses a lower-upper (LU) matrix decomposition
with row partial pivoting, i.e., HPL computes a factorization of
matrix $A$ into two triangle matrices $L$ ("lower") and $U$ ("upper")
such that $A=LU$ holds true.

The nodes used to run HPL are organized in a virtual $P \times Q$ grid and
the data of the $n \times n+1$ extended coefficient matrix is distributed
onto this grid by cutting the matrix first into $n/N_{B}$ square blocks of size $N_{B}$
and then cyclically assigning a block to a process in the process grid.

\CH{See my journal entry on 2017-10-04. Describe the broadcast here; introduce the 6 algorithms; explain that panels are being broadcast to other nodes}

Older versions of MPI only supported non-blocking point-to-point
communications but did not support non-blocking collective
communications. However, HPL ships with in total 6 self-implemented,
point-to-point based broadcast algorithms to efficiently overlap the
time spent waiting for an incoming panel with updates to e.g. the trailing matrix. 

Every host that is waiting for a panel to arrive enters a loop and
tests in each iteration whether or not the panel has been received by
calling =MPI_Iprobe=. If the panel has not been received yet, updates to
parts of the trailing matrix are made and row-interchanges are applied.
#+LaTeX: \CH{Check exactly what is being done here, and using which panel; see  \texttt{HPL\_pdupdateNT.c} and the comment of the function ("Purpose"). There are several panels involved.}
As soon as =MPI_Iprobe= returns that the panel has been fully received,
no more updates are performed and the received panel is forwarded to
the next host and only after this has been done are the remaining updates finished.

Unfortunately, in version 2.2 of HPL, this overlapping is only enabled
in four out of six algorithms as the =bandwidth= and the =bandwidth
modified= algorithms seem to have had issues on some machines with getting stuck due to
too many messages.
#+LaTeX: \CH{See \texttt{HPL\_blonM.c}, ll. 264 ff.}

\AL{See www/algorithm.html and describe the main params: 0) geometry, size and granularity, 1) Panel Broadcast, 2) Look-Ahead, 3) Update.}
** A typical run on a supercomputer
\AL{Describe stampede and the stampede fullrun}

** Difficulties
#+LaTeX: \label{sec:con:diff}

\AL{Too detailed on simulation. Difficulties = difficulties in term of performance prediction, i.e., application, runtime and platform and no formula may account for such complexity. These difficulties are rather conclusions of the related work section.}
   Several difficulties were well-known and had to be resolved in
   order to simulate HPL:

   1. The time-complexity of the algorithm is $\O(N^3)$ and
      $O(N^2)$ communications are performed, with $N$ being
      very large. This causes executions of large problem sizes to
      become rather slow. For instance, the run on the Stampede cluster took almost
      two hours with $N=3,875,000$.
      
   2. Each node of a large cluster only allocates memory for a part of
      the whole matrix. With 4422\nbsp{}nodes, the Stampede run required
      \SI{120}{\tera\byte} of memory. A simulation running and executing HPL on only one
      single node will hence require this amount of data to be available on that particular
      node. It is hence vital to reduce the amount of memory for a
      simulation to become feasible.
      \CH{Tom's slides say the Stampede run was 6,006 MPI processes. I thought it was 1 process per node - where are the other processes coming from?}

      \CH{Should I already mention the pagetable size here - "not only the amount of memory itself but also the size of the pagetable becomes problematic"}
      
   3. Since HPL implements its own broadcast strategies, simulation is
      no longer sufficient as these strategies are vital for HPL's
      performance. Hence, emulation is required.

Real execution:
- Matrix of size 3,875,000
- Using 6,006 MPI processes
- About 2 hours
Requirement for the emulation of Stampede's execution:
- $\ge 3, 875, 000 2 \times 8$ bytes \approx 120 terabytes of memory
- $\ge 6, 006 \times 2$ hours \approx 500 days (very optimistic)

* Related Work
#+LaTeX: \label{sec:relwork}
  
Two approaches are commonly used in order to study a parallel
application with the help of a simulator: Offline and online simulation.

Offline simulation denotes a rather static approach: First, the
application is executed on a real machine and a tracefile with all the
important events (calls to MPI functions, computations) is
generated, with the events being time-independent (i.e., only the
order of their appearance is relevant). Offline simulation is static
as the traces contain only information about a single run and give no
hints about how, for instance, changes to the topology may impact the
communication patterns. To study these effects with offline simulation
is tedious as it requires the researcher to obtain new traces. 

Most simulators available today, among them BigSim\cite{bigsim_04},
Dimemas\cite{dimemas} and CODES\cite{CODES}, allow users to replay a
trace, \ie they support offline simulation. 
Alas, this approach is unusable in the case of HPL due to the size of the obtained traces and the complexity of
the application, as HPL implements for instance several broadcast
strategies that influence the performance significantly.

It is for these reasons necessary to not simulate, but emulate HPL.

A broad selection of tools enabling researchers to study MPI
applications on complex platforms exists. The extreme-scale simulator
xSim\cite{xsim}, although it is not publicly available, 
SST\cite{sstmacro} just as SimGrid/SMPI\cite{simgrid} all support online
emulation.
\CH{This needs to be expanded}
    

* SimGrid/SMPI in a nutshell
\label{sec:smpi}
\AL{We need such a section to separate our contribution from what was already there}

\AL{Emulation. And fast emulation if sampling. Requires shared malloc to scale.}
- explain the =SMPI_SHARED_MALLOC= mechanism with figure 4.7 from Tom's report


    Saving the memory of the matrix allocation is as simple as replacing the call to =malloc= (resp. =free=) by
    =SMPI_SHARED_MALLOC= (resp. =SMPI_SHARED_FREE=).

    Two different mechanisms exist in Simgrid, called /local/ and /global/. The local algorithm allocates one block per call
    location, shared by all MPI processes. The real memory footprint of this block is exactly the size of the allocation,
    hence the memory consumption of all the MPI processes is divided by the number of processes. This mechanism is based
    on POSIX shared memory objects, using =shm_*= functions.

    The global algorithm is much more efficient in terms of memory consumption. First, it allocates a single block for
    the whole execution, shared by all MPI processes. Moreover, the real memory footprint of this block is constant,
    regardless of the size of the allocation, hence providing a very small memory consumption. This mechanism is
    detailed below as we had to extend it for the panels.

    The main idea is to reserve a range of virtual addresses of the desired size and map it cyclically on a small range of
    physical addresses, as illustrated by figure\nbsp{}\ref{fig:global_shared_malloc}. The granularity is the size of this
    range of physical addresses (1MB by default).

    At the first call to =SMPI_SHARED_MALLOC=, a temporary file is created. The file descriptor is a global variable,
    accessible by all the MPI processes, since they are implemented by POSIX threads.

    At every call to =SMPI_SHARED_MALLOC=, a first call to =mmap= is done with the required size and the flag =MAP_ANONYMOUS=
    (thus without any file descriptor). The effect of this call is to reserve the whole interval of virtual
    addresses. Then, for each sub-interval, a new call to =mmap= is done with the temporary file. The address of the
    sub-interval itself is passed with the flag =MAP_FIXED=, which forces the mapping to keep the same virtual address.
    As a result, each of these sub-intervals of virtual addresses are mapped onto a same interval of physical
    addresses. We therefore have a block of virtual addresses of arbitrary size backed by a constant amount of physical
    memory. Since there are almost no computations left, this is harmless with respect to the simulation. Note that such
    allocations cannot be fully removed as many parts of the code still access it from time to time.
    #+BEGIN_EXPORT latex
    \tikzset{draw half paths/.style 2 args={%
      % From https://tex.stackexchange.com/a/292108/71579
      decoration={show path construction,
        lineto code={
          \draw [#1] (\tikzinputsegmentfirst) --
             ($(\tikzinputsegmentfirst)!0.5!(\tikzinputsegmentlast)$);
          \draw [#2] ($(\tikzinputsegmentfirst)!0.5!(\tikzinputsegmentlast)$)
            -- (\tikzinputsegmentlast);
        }
      }, decorate
    }}
    \begin{figure}[htbp]
      \centering
      \begin{tikzpicture}
        \pgfmathtruncatemacro{\size}{4}
        \pgfmathtruncatemacro{\width}{2}
        \pgfmathtruncatemacro{\sizem}{\size-1}
        \pgfmathtruncatemacro{\smallbasex}{4}
        \pgfmathtruncatemacro{\smallbasey}{\size/2}
        \pgfmathtruncatemacro{\smallstopx}{\smallbasex+\width}
        \pgfmathtruncatemacro{\smallstopy}{\smallbasey+1}
        \foreach \i in {0,\sizem}{
	    \pgfmathtruncatemacro{\j}{\i+1}
	    \draw (0, \i) -- (0, \j);
	    \draw (\width, \i) -- (\width, \j);
	    \draw[dotted] (0, \i) -- (\width, \i);
	    \draw[dotted] (0, \j) -- (\width, \j);
	}
	\draw[dashed] (0, 1) -- (0, \sizem);
	\draw[dashed] (\width, 1) -- (\width, \sizem);
	\draw (0, 0)     -- (\width, 0);
	\draw (0, \size) -- (\width, \size);
        \draw (\smallbasex,\smallbasey) -- (\smallstopx,\smallbasey) -- (\smallstopx,\smallstopy) -- (\smallbasex,\smallstopy) -- cycle;
        \foreach \i in {0,\sizem}{
	    \pgfmathtruncatemacro{\j}{\i+1}
	    \draw[dotted] (\width, \i) -- (\smallbasex, \smallbasey);
	    \draw[dotted] (\width, \j) -- (\smallbasex, \smallstopy);
	    \pgfmathsetmacro{\xleft}{\width}
	    \pgfmathsetmacro{\xright}{\smallbasex}%{\width/2.0+\smallbasex/2.0}
	    \pgfmathsetmacro{\yleft}{\i + 0.5}
	    \pgfmathsetmacro{\yright}{\smallbasey + 0.5}
	    \path [draw half paths={solid, -latex}{draw=none}]  (\xleft, \yleft) -- (\xright, \yright);
	}
	\draw[decorate,line width=1pt,decoration={brace,raise=0.2cm}] (0, 0) -- (0, \size) node [pos=0.5, xshift=-1cm] {virtual};
	\draw[decorate,line width=1pt,decoration={brace,mirror,raise=0.2cm}] (\smallstopx, \smallbasey) -- (\smallstopx, \smallstopy) node [pos=0.5, xshift=1.2cm] {physical};
      \end{tikzpicture}
      \caption{\label{fig:global_shared_malloc}Global shared malloc}
    \end{figure}
    #+END_EXPORT

* Improving Emulation Mechanisms and Preparing HPL
#+LaTeX: \label{sec:em}

In this section, we described the adjustments we made in SimGrid and
the few modifications we had to do in HPL to allow a scalable and
faithful simulation of HPL. A quick performance evaluation of each
modification is provided when space allows but we refer the interested
reader to\cite{cornebize:hal-01544827} and to a laboratory 
#+LaTeX: notebook\footnote{See \texttt{journal.org} at \url{https://github.com/Ezibenroc/m2_internship_journal/}},
where each modification has been carefully investigated.
The experiments of this section have been performed on nodes of the
Nova cluster from the Grid'5000 testbed\cite{grid5000}. These nodes have
\SI{32}{\giga\byte} RAM of memory and two 8-core Intel Xeon E5-2620 v4
CPUs processors running at \SI{2.1}{\GHz}. They are deployed with a
Debian Stretch image (kernel 4.9) and a single core was used for each
experiment.

** Kernel modeling
#+BEGIN_EXPORT latex
\begin{figure*}%[!htb]
  \centering
  \subfigure[Non-intrusive macro replacement.\label{fig:macro_simple}]{
    \begin{minipage}[b]{.5\linewidth}
      \lstset{frame=bt,language=C,escapechar=|}\lstinputlisting{HPL_dtrsm_macro_simple.c}
    \end{minipage}}%
  \subfigure[Gain in term of simulation time.\label{fig:kernel_sampling}]{
    \begin{minipage}[b]{.5\linewidth}
      \includegraphics[width=\linewidth,page=2]{figures/validation/L1/report_plot_gflops.pdf}\\
      \includegraphics[width=\linewidth,page=2]{figures/validation/L1/report_plot_time.pdf}
      \TC{Figures need to be regenerated (adjust font-size, remove gray box) and merged (simulation time and gflops  side by side) as only the dependency on matrix size is interesting (remove number of process). The term "kernel sampling" should also be replaced by "kernel modeling".}
    \end{minipage}}
  \caption{Replacing the calls to computationnaly expensive functions by a model.}
\end{figure*}
#+END_EXPORT

       As explained in Section\ref{sec:con:diff}, faithful prediction
       requires emulating HPL, i.e., to execute the code. However, HPL
       heavily relies on commonly available BLAS functions such as
       =dgemm= (for matrix-matrix multiplication) or =dtrsm= (for solving
       an equation of the form $Ax=b$). A quick analysis of an HPL
       simulation with a relatively small matrix with dimensions
       30,000 and 64 processes shows that around \SI{96}{\percent} of
       the time is spent in these two very regular functions. The values computed
       by these functions are barely used in the control flow of HPL
       and are thus of no interest when simulating.

       Therefore, immediate and significant time savings can be
       realized by replacing such expensive calls to =dgemm= and =dtrsm=
       by a performance model. Figure\ref{fig:macro_simple} shows how
       every call to the =HPL_dtrsm= function can be skipped and
       replaced by a simple evaluation of a model. The macro mechanism
       allows to keep HPL code modifications to an absolute
       minimum. The =(9.882e-12)= value represents the inverse of the
       flop rate for such computation kernel and can be obtained
       through a simple calibration. The predicted execution time is
       then used as an argument to =smpi_execute_benched=, which makes
       the simulated process enter a sleep-state for the entire
       duration, effectively advancing the clock for that process by
       the same amount as the execution would have. The gain in term
       of simulation time for a small scenario is depicted on
       Figure\ref{fig:kernel_sampling}. As expected this modification
       speeds up the simulation by orders of magnitude when matrix
       size increases. The consequence in term performance (i.e., the
       output of HPL) prediction are a slight overestimation due to
       the absence of performance variability when kernel models are
       used. Such variability could however easily be accounted for in
       the model.
*** Hidden section with macro code                               :noexport:
#+BEGIN_SRC C :exports none :tangle HPL_dtrsm_macro_real.c
#define |\color{colorfuncall}HPL\_dtrsm|(layout, Side, Uplo, TransA, Diag, M, N, alpha, A, lda, B, ldb) ({                        \
    double expected_time;                                                                                  \
    double coefficient, intercept;                                                                         \
    if((M) > 512 && (N) > 512) {                                                                           \
        coefficient = (double)SMPI_DTRSM_PHI_COEFFICIENT; intercept = (double)SMPI_DTRSM_PHI_INTERCEPT;    \
    } else {                                                                                               \
        coefficient = (double)SMPI_DTRSM_COEFFICIENT;     intercept = (double)SMPI_DTRSM_INTERCEPT;        \
    }                                                                                                      \
    if((Side) == HplLeft) {                                                                                \
        expected_time = coefficient*((double)(M))*((double)(M))*((double)(N)) + intercept;                 \
    } else {                                                                                               \
        expected_time = coefficient*((double)(M))*((double)(N))*((double)(N)) + intercept;                 \
    }                                                                                                      \
    if(expected_time > 0)                                                                                  \
        |\color{colorfuncall}smpi\_execute\_benched|(expected_time);                                                               \
})
#+END_SRC

#+BEGIN_SRC C :exports none :tangle HPL_dtrsm_macro_simple_old.c
#define |\color{colorfuncall}HPL\_dtrsm|(layout, Side, Uplo, TransA, Diag, M, N, alpha, A, lda, B, ldb) ({      \
    double expected_time = (9.882e-12)*((double)M)*((double)M)*((double)N) + 4.329e-02;   \
    if(expected_time > 0)                                                                 \
        |\color{colorfuncall}smpi\_execute\_benched|((useconds_t)(expected_time));                                \
})
#+END_SRC

#+BEGIN_SRC C :exports none :tangle HPL_dtrsm_macro_simple.c
#define |\color{colorfuncall}HPL\_dtrsm|(layout, Side, Uplo, TransA, Diag,        \ 
        M, N, alpha, A, lda, B, ldb) ({                    \
    double expected_time = (9.882e-12)*((double)M)*        \
                   ((double)M)*((double)N) + 4.329e-02;    \
    if(expected_time > 0)                                  \
        |\color{colorfuncall}smpi\_execute\_benched|((useconds_t)(expected_time)); \
})
#+END_SRC

#+BEGIN_EXPORT latex
\CH{Found this in Tom's logbook. Check if this is the final version. Also, we can apparently just call \texttt{make SMPI\_OPTS=-DSMPI\_OPTIMIZATION} (what about \texttt{arch=SMPI}?). See his logbook}
#+END_EXPORT
** Adjusting the behavior of HPL
#+LaTeX: \label{sec:hplchanges}

#+BEGIN_EXPORT latex
    \tikzstyle{switch}=[draw, circle, minimum width=1cm, minimum height = 1cm]
    \tikzstyle{compute}=[draw, rectangle, minimum width=0.5cm, minimum height = 0.5cm, node distance=0.5cm]
    \tikzstyle{base}=[ellipse, minimum width=2cm, minimum height = 0.5cm, node distance = 0.5cm]
    \tikzstyle{bigswitch}=[base, draw]
    \begin{figure*}%[htbp]
      \centering
      \subfigure[Structure of the panel in HPL.\label{fig:panel_structure}]{\small
        \begin{minipage}[b]{.5\linewidth}
          \begin{tikzpicture}
            \draw [fill=gray] (3, 2) -- (6, 2) -- (6, 3) -- (3, 3) -- cycle;
            \draw (0, 2) -- (9, 2) -- (9, 3) -- (0, 3) -- cycle;
            \draw[dashed] (3, 2) -- (3, 3);
            \draw[dashed] (6, 2) -- (6, 3);
            \node(1) at (1.5, 2.5) {matrix parts};
            \node(2) at (4.5, 2.5) {indices};
            \node(3) at (7.5, 2.5) {matrix parts};
            \draw[decorate,line width=1pt,decoration={brace,raise=0.2cm}] (0, 3) -- (3, 3) node [pos=0.5, yshift=0.5cm] {can be shared};
            \draw[decorate,line width=1pt,decoration={brace,raise=0.2cm}] (6, 3) -- (9, 3) node [pos=0.5, yshift=0.5cm] {can be shared};
            \draw[decorate,line width=1pt,decoration={brace,raise=0.2cm, mirror}] (3, 2) -- (6, 2) node [pos=0.5, yshift=-0.5cm] {must not be shared};
          \end{tikzpicture}
        \end{minipage}}%
      \subfigure[Reusing panel allocation from an iteration to another.\label{fig:panel_reuse}]{\small
        \begin{minipage}[b]{.5\linewidth}
          \begin{center}
          \begin{tikzpicture}
            \draw [fill=gray] (2, 1) -- (4, 1) -- (4, 1.5) -- (2, 1.5) --cycle;
            \draw (0, 1) -- (6, 1) -- (6, 1.5) -- (0, 1.5) -- cycle;
            \draw[dashed] (2, 1) -- (2, 1.5);
            \draw[dashed] (4, 1) -- (4, 1.5);

            \draw [fill=gray] (2, 0) -- (3, 0) -- (3, .5) -- (2, .5) --cycle;
            \draw (1, 0) -- (4, 0) -- (4, .5) -- (1, .5) -- cycle;
            \draw[dashed] (2, 0) -- (2, .5);
            \draw[dashed] (3, 0) -- (3, .5);

            \draw[dotted, -latex] (2, 1) -- (2, .5);
            \draw[decorate,line width=1pt,decoration={brace,raise=0.2cm}] (0, 1.5) -- (6, 1.5) node [pos=0.5, yshift=0.5cm] {initial buffer};
            \draw[decorate,line width=1pt,decoration={brace,raise=0.2cm, mirror}] (1, 0) -- (4, 0) node [pos=0.5, yshift=-0.5cm] {current buffer};
          \end{tikzpicture}
          \end{center}
        \end{minipage}
      }
      \caption{Panel structure and allocation strategy when simulating.\label{fig:panel_reuse}}
    \end{figure*}
#+END_EXPORT

HPL uses huge pseudo-randomly generated matrices that need to be setup
every time HPL is executed. HPL does not account for the time spent
setting up the matrices nor for the validation of the computed result
in the reported \si{\giga\flops} performance. Furthermore, the
verification would now be meaningless as we skipped all the
computations and replaced them by a model evaluation. Since such
phases do not impact the performance of the platform, we can safely
skip both steps.

Although the lion's share of computation time was consumed by calls to
=dgemm= and =dtrsm=, several other functions were identified through
profiling as computationally expensive enough to justify a specific
handling: In total seven additional BLAS functions (=dgemv=, =dswap=, =daxpy=,
=dscal=, =dtrsv=, =dger=, and =idamax=). All of these functions are called during the
LU factorization and hence accounted for by HPL; however, because of
the removal of the =dgemm= and =dtrsm= computations they all operate on
bogus data and hence produce bogus data. We also determined that their
impact on the performance prediction was minimal and that precisely
modeling them was simply not worth the effort: We simply modeled them
as being instantaneous.

Note that working on bogus data has a few consequences as HPL
implements an LU factorization with partial pivoting and a special
treatment of the =idamax= function that returns the index of the first
element having maximum absolute value. The cost of this function was
ignored as well but its return value was arbitrarily set to make the
simulation fully deterministic. In all our evaluations, this
modification was harmless in term of performance prediction while it
allows to speed up the simulation by an additional factor $\approx3$ to $4$
on small scenarios ($N=20,000$) and much more on larger setups.
** Memory folding
As explained in Section\ref{sec:smpi}, when emulating an application
with SMPI, all MPI process are run within the same process of a single
node. The memory consumption of the simulation can therefore be
ridiculously large (several \si{\tera\byte} of RAM).

Yet, as we do no longer operate on the data for real, storing the whole
matrix $A$ (and hence the "real" data) is no longer a requirement. On
the other hand, since only a minimal modification of the code was
done, some functions may still read or write some parts of the matrix.
It is thus not possible to simply remove the memory allocations of
heavy data structures but SMPI's =SHARED_MALLOC= mechanism can be used
so that such data structures are shared between all MPI processes and
that they only occupy a minimal amount of physical pages.

The largest two allocated data structures in HPL are the matrix =A=
which is to be factorized (and whose size is typically of several
\si{\giga\byte} per process) and the =panel= which contains many
information about the sub-matrix currently factorized (and whose size
is typically of a few hundreds of \si{\mega\byte} per process).

Although using the default =SHARED_MALLOC= mechanism works like a charm
with =A=, a more careful strategy needs to be used for the
=panel=. Indeed, =panel= is an intricate data structure with both \texttt{int}s,
accounting for matrix indices, error codes, MPI tags, and pivoting information,
and \texttt{double}s corresponding to a copy of submatrices of =A=. To
optimize data transfers, HPL flattens this structure into a single
allocation of \texttt{double}s (see Figure\ref{fig:panel_structure}). Using a fully shared memory allocation
for the =panel= therefore leads to indices corruption that result in
both classical invalid memory accesses and even communication
deadlocks, as processes may not send/receive to/from the right
process. Since {int}s and \texttt{double}s are stored in
non-contiguous parts of this flat allocation, it is therefore
essential to have a mechanism allowing to preserve the content of
specific parts for each process. We have thus introduced a new
=SMPI_PARTIAL_SHARED_MALLOC= which works as follows: 
~mem = SMPI_PARTIAL_SHARED_MALLOC(500, {27,42 , 100,200}, 2)~.
In this example, 500 bytes are allocated in =mem= with the elements
=mem[27]=, ..., =mem[41]= and =mem[100]=, ..., =mem[199]= being shared between
processes (and hence generally completely corrupted) while all other
remain private. This mechanism allows us with a few lines of
modification in HPL to allocate large panels that are mostly shared
between processes and mapped into the same physical page while a small
part in the middle (containing indices) is actually private to each
MPI process.

Designating memory explicitly as private, shared or partially shared
is not only important in cases where memory is scarce, but also to
improve performance. As SMPI is internally aware of the memory's
visibility, it can avoid calling =memcopy= when large messages
containing shared segments are sent from one MPI rank to another. In
the cases of private data segments or partially shared segments, SMPI
identifies and only copies those parts that are designated as private
(as they are process-dependent) into the corresponding private buffers
on the receiver side.

In the case of HPL, this speeds up simulation times considerably, as
the main datastructure that is being communicated between ranks, the
=panel=, is a partially shared datastructure with the largest part being
shared. The error made with these new allocations, in comparison with
the version from section 4.3, is negligible (below 1%) while the
improvement of the memory consumption is drastic. For instance, with a
matrix of size 40,000 and 64 MPI processes, the memory consumption
decreases from about \SI{13.5}{\giga\byte} to less than
\SI{40}{\mega\byte}.
** Panel reuse
The original HPL code \texttt{malloc}s/\texttt{free}s panels in each
iteration whereas the size of the panel only decreases along
iterations. As described above, the partial sharing of panels requires
some extra effort, introducing an overhead. At scale, repeated
allocations / frees become a bottleneck due to this overhead. Since
the very first allocation can fit all subsequent panels, we modified
HPL to only allocate the very first panel and reuse it from an
iteration to another (see Figure\ref{fig:panel_reuse}).

As usual, this optimization is harmless in term of simulation
accuracy: The maximum observed error, in comparison with the previous
version, is always lower than 1%. The gain in terms of simulation
time, albeit less impressive than for previous optimizations, is
significant: For a matrix of size 40,000 and 64 MPI processes, the
simulation time decreases by four seconds, from 20.5 seconds to 16.5
seconds, thanks to a reduction of the system time from 5.9 seconds to
1.7 seconds. The number of page faults decreased from 2 millions to
0.2 million, thus confirming the dramatic effect such
series of allocation/deallocation would have at scale.
** MPI process representation (mmap vs. dlopen)
SimGrid folds parallel applications into a single process and hence,
local static and global variables become an issue as it must be guaranteed that
each rank has its own set of global variables. SMPI supports two
mechanisms to achieve this: The usage of either =mmap= or =dlopen=.
*** mmap
When =mmap= is used, SMPI copies the =data= segment on startup for each
rank into the heap. When control is transferred from one rank to
another, the =data= segment is =mmap='ed to the location of this rank's
copy on the heap. All ranks have hence the same addresses in the
virtual address space at their disposition although they point to
different physical addresses based on the rank. This also means
inevitably that caches must be flushed to ensure that no data of one
rank leaks into the other rank. This overhead makes the usage of =mmap=
a rather expensive operation.

# \TOM{Can you tell me how often these operations were executed, as you've already done in your journal on 2017-04-11 ("Looking at the syscalls")?}
*** dlopen
With =dlopen=, copies of the global variables are still made but they
are stored inside the =data= segment as opposed to the heap. When
switching from one rank to another, the starting virtual address for
the storage is readjusted rather than the addresses point
to. This means that each rank has its own unique pool of addresses for
global variables. The main advantage of this approach is that caches do not need to
be flushed as in the case for the =mmap= approach, because data
consistency can always be guaranteed.

*** Impact of choice of mmap/dlopen
The choice of mmap or dlopen influences the simulation time indirectly
through its direct impact on system/user time and page faults.  As an
example, for a matrix of size 80,000 and 32 MPI processes, the number
of minor page faults drops from \num{4412047} (with =mmap=) to
\num{6880} (with =dlopen=). This results in a drop in system time from 
\SI{10.64}{\sec} (out of \SI{51.47}{\sec} in total) to
\SI{2.12}{\sec}. Obviously, the larger the matrix and the number of
process, the larger the number of context switch during the
simulation, and the higher the gain.

# See Tom's journal (Performance evaluation of the privatization
# mechanism: =dlopen= vs =mmap= ) ; there are some graphs that we might be
# able to use, such as in
# https://github.com/Ezibenroc/m2_internship_journal/blob/master/simgrid_privatization/

** Huge pages    
For larger matrix sizes (e.g., when $N$ is above a few hundreds of
thousands), the performance of the simulation quickly
deteriorates. The memory consumption gets surprisingly high and the CPU
utilization drops. Running the simulation while monitoring the system
shows that the program is regularly stalled while the kernel loads the
CPU at 100%, which explains the low CPU utilization for the program
itself.

As described above, we fold the memory and realise significant
physical memory savings. However, the allocations are still performed
for and the /virtual/ memory is still allocated for every process. This
implies that there is no reduction in the overall amount of virtual
addresses, causing the page table to become too large to be
efficiently maintained. In general, the size of the page table with
page size of 4,096 bytes can be computed as:

    #+LATEX: \[ PT_{size}(N) = \frac{N^2 \cdot \texttt{sizeof(double)}}{4,096} \cdot \texttt{sizeof(pointer)} \]

This means that for a matrix of size $N=4,000,000$, the page table
grows to $PT_{size}(4,000,000) = \num{2.5e11}$ bytes, \ie to
\SI{250}{\gibi\byte}. The x86-64 architecture supports several page
sizes. On Linux, these larger pages are known as huge page. A typical
size for these pages is 2 MiB, although there exists other sizes
(\SIrange{2}{256}{\mebi\byte}).

Resolving this problem requires administrator (root) privileges as the
Linux kernel support for /hugepages/ needs to be activated. One should
then simply mount a =hugetlbfs= file system, allocate at least one huge
page and then pass the path of the allocated file system to
Simgrid. The implementation consists in passing the flag MAP_HUGETLB
to =mmap= in =SMPI_SHARED_MALLOC= and replacing the file given to mmap by
a file opened in the hugetlbfs file system.

In our case, setting the page size to \SI{2}{\mebi\byte} resulted in
the page table to shrink from \SI{250}{\gibi\byte} to
\SI{0.488}{\gibi\byte}.  It is also noteworthy that using hugetables
decreases the amount of page faults thereby improving even further
simulation time. For instance, with a size of 300,000 and 64 MPI
process, the CPU utilization rises from 66% to 99%, hence reducing
simulation time from about \SI{580}{\sec} to about \SI{175}{\sec}.
# Values from Figure 4.9 in Tom's report
* Scalability Evaluation
#+LaTeX: \label{sec:scalabilityevol}

#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth,page=2]{./figures/scalability_plot_size.pdf}                                                                                                                               
  \includegraphics[width=\linewidth,page=2]{./figures/scalability_plot_nbproc.pdf}
  \caption{Time complexity and memory consumption are linear in the number of processes with a fixed matrix size but remain slightly quadratic when matrix size is varied. }
  \TC{It would be nice to regenerate these figures with larger fonts and possibly a different aspect ratio (height may be increased a bit).}
  \label{fig:hpl_scalability}
  \labspace
\end{figure}
#+END_EXPORT

In Section\ref{sec:em} we described the work we did in order to run a
large-scale simulation on a single node. Most of this work mostly
consists in identifying and eliminating bottlenecks one after the
other while making sure the consequences on performance prediction are
harmless. Doing so, the goal is to reduce as much as possible the
complexity of simulating HPL from $\O(N^3) + O(N^2.P.Q)$ to a more
reasonable complexity. The removal of most of the computations allow
to get rid of the $\O(N^3)$. Since there are $N/NB$ steps, the ideal
complexity reduction would be to decrease the cost of an iteration to
something independent of $N$. Thanks SimGrid fluid models, the time to
simulate a communication does not depend on does not $N$. The time to
simulate a step of HPL should therefore mostly depend on $P$ and
$Q$. Yet, some of memory operations on the panel related to pivoting
are intertwined in HPL with collective communications, which does not
allow to completely get rid of the $\O(N)$ complexity without
modifying a bit more deeply HPL.

Although our goal is to model and simulate HPL on the Stampede
platform eventually, we decided to produce some first evaluation on a
similar, albeit non-existing, platform comprising 4,096 8-core nodes
interconnected through a $\langle2;16,32;1,16;1,1;8\rangle$ fat-tree topology
built on ideal network links with a bandwidth of
\SI{50}{\giga\byte\per\sec} and a latency of \SI{5}{\micro\sec}.  We run
simulations with $512$; $1,024$; $2,048$ or $4,096$ MPI processes and
with matrices of size \num{5e5}, \num{1e6}, \num{2e6} or \num{4e6}. To
illustrates the scalability of HPL simulation with all the previously
described optimizations enabled, the largest simulation took
approximately 47 hours and \SI{16}{\giga\byte} of memory. The smallest
one took 20 minutes and \SI{282}{\mega\byte} of memory.

Figure\ref{fig:hpl_scalability} compares in detail the impact of changes to
process number or matrix size on total makespan and memory. 
In the first and second row, the matrix size and number of processes, respectively,
are varied. When the matrix size ($N$) is changed, as depicted in the
two panels of the first row, memory consumption and
simulation time both grow slightly quadratic as the amount of matrix
elements grows quadratically ($N^{2}$) and the number of steps of the
algorithm also grows linearly.

As becomes apparent when studying the results shown in the lower two
panels, simulation time is perfectly linear in the number of processes
when the matrix size is fixed. It is noteworthy that the memory
consumption only mildly depends on the number of processes: the matrix
size determines the "lower bound" for the memory consumption and a
constant amount of memory is then added for the private memory of
panels and global variables that every process requires.

For all these simulations, the CPU utilization is above 98%. This
means that the kernel is still able to manage the page table without
stalling too much the simulation process. Moreover, all the
simulations spend less than 10% of their execution time in kernel
mode, which means the number of system calls is reasonably low.
Therefore, the simulation of the largest supercomputers is now within
reach.
* Modeling Stampede and Simulating HPL
#+LaTeX: \label{sec:science}

** Modeling Stampede
*** Computations

The Stampede cluster contains *XYZ* compute nodes, each with two 8-core Intel Xeon
E5-2680 8C \SI{2.7}{\GHz} CPU and one, for very few nodes even two 61-core Intel Xeon Phi SE10P (KNC) with
\SI{1.1}{\GHz} accelerator. The accelerators are essential to the performance
of the cluster, delivering \SI{7}{\peta\flops} of sustainable performance
whereas the CPUs are only capable of delivering  \SI{2}{\peta\flops}. On
matrices of the size used for this work, however, CPUs are barely used.

# See CH's journal from [2017-10-03 Tue] to see how the version was determined
The Xeon Phis can be used in two ways: First, as a classical
accelerator, i.e., to offload expensive computations from the CPU onto
the accelerator. We used Intel's Math Kernel Library (MKL) version
11.1.1 that comes with support for automatic offloading for several BLAS
functions. In the case of DGEMM, the matrix dimensions determine
whether or not the computation is transferred to the KNC: If both
dimensions of the matrix are larger than $1280$, the computation is
offloaded.
\CH{And K > 256? See also here: https://software.intel.com/en-us/articles/intel-mkl-automatic-offload-enabled-functions-for-intel-xeon-phi-coprocessors}

The second way the Xeon Phi's can be used is by compiling binaries for
and executing them on the Xeon Phi. While the accelerator's memory of \SI{8}{\gibi\byte} is rather
small, its main advantage is that data does not need to be
transferred from the node's CPU to the accelerator via the x16 PCIe bus.
\CH{Not done here}

*** Communications

SMPI's communication model is a hybrid model between the LogP family
and a fluid model, supporting different modes for the send operation
such as fully asynchronous, detached or eager. For each message, the
mode used is determined solely on the message size. It is hence
possible to model the resulting performance through a piece-wise,
linear model, as depicted in Figure\ref{fig:stampede_calibration}.
For a thorough discussion of the calibration techniques, see\cite{degomme:hal-01415484}.

SMPI uses this model by default, however, it is not leveraged in the
particular case of HPL as its communication patterns use primarily large, bulk messages,
making small messages scarce. Almost all messages are hence only sent
via the /detached/ mode (depicted in green).
\CH{I'm not sure why we chose the breakpoint the way it is for detached; this is not obvious?}

#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth,page=1]{./figures/stampede_calibration_send.png}
  \caption{Visualization of the analysis results as obtained on the Stampede system.}
  \label{fig:stampede_calibration}
  \labspace
\end{figure}
#+END_EXPORT

*** Network topology

#+BEGIN_EXPORT latex
%% 
%% This draws a fat tree. If you want to change its appearance, see the \size macro.
%%
\tikzstyle{switch}=[draw, circle, minimum width=1cm, minimum height = 1cm]
\tikzstyle{compute}=[draw, rectangle, minimum width=0.5cm, minimum height = 0.5cm, node distance=0.5cm]
\tikzstyle{base}=[ellipse, minimum width=2cm, minimum height = 0.5cm, node distance = 0.5cm]
\tikzstyle{bigswitch}=[base, draw]
\begin{figure}[t]
  \centering
	\begin{tikzpicture}[scale=0.4,transform shape]
	    \pgfmathtruncatemacro{\size}{3}    % Number of ports in a switch, THE PARAMETER TO CHANGE
	    \pgfmathtruncatemacro{\sizesquare}{\size*\size}
	    \pgfmathtruncatemacro{\boundSwitch}{\size-1}
	    \pgfmathtruncatemacro{\boundCompute}{\size*\size-1}
	    % Compute nodes
	    \foreach \i in {0,...,\boundSwitch}{
		\pgfmathtruncatemacro{\incr}{\sizesquare*\i}
		\foreach \x in {0,...,\boundCompute} {
              -- CH: I added the +\boundSwitch here to move the nodes to the right
		    \pgfmathtruncatemacro{\z}{\x+\incr+\boundSwitch}
		    \pgfmathsetmacro{\pos}{\z/2}
		    \node[compute] (c_\z) at (\pos, 0) {} ;
		}
	    }
	    % Switches L1 and L2
	    \foreach \i in {0,...,\boundSwitch}{
		\pgfmathtruncatemacro{\incr}{\size*\i}
		\foreach \x in {0,...,\boundSwitch} {
		    \pgfmathtruncatemacro{\z}{\x+\incr}
              -- CH: I added the +\boundSwitch/1 here (was: /4) to move the nodes to the right
              -- (this moves them below the L3 layer)
		    \pgfmathsetmacro{\pos}{\incr*\size/2+\x*\size/2+(\boundSwitch/1)}
		    \node[switch] (l1_\z) at (\pos, 4) {} ;
		    \node[switch] (l2_\z) at (\pos, 8) {} ;
		}
	    }
	    % Edges of the islets
	    \foreach \i in {0,...,\boundSwitch}{
		\pgfmathtruncatemacro{\incr}{\size*\i}
		\foreach \switch in {0,...,\boundSwitch} {
		    \pgfmathtruncatemacro{\sw}{\switch+\incr}
		    \foreach \y in {0,...,\boundSwitch} {
                  -- CH: I added the +\boundSwitch here. This corrects the edges for the leafs. 
			\pgfmathtruncatemacro{\comp}{\switch*\size+\y+\incr*\size+\boundSwitch}
			\draw (l1_\sw.south) -- (c_\comp.north);
		    }
		    \foreach \root in {0,...,\boundSwitch} {
			\pgfmathtruncatemacro{\ro}{\root+\incr}
			\draw (l1_\sw.north) -- (l2_\ro.south);
		    }
		}
	    }
	    \node (l1) at (-1, 4) {\Huge $L_1$} ;
	    \node (l2) at (-1, 8) {\Huge $L_2$} ;
	    \node (l3) at (-1, 12) {\Huge $L_3$} ;

	    \pgfmathtruncatemacro{\boundDoubleSwitch}{\size*2-1}
	    % Switches L3
	    \foreach \x in {0,...,\boundDoubleSwitch} {
		\pgfmathsetmacro{\pos}{\x*\size+(\boundSwitch/2)}
		\node[switch] (l3_\x) at (\pos, 12) {} ;
	    }
	    % Upper edges
	    \foreach \root in {0,...,\boundDoubleSwitch} {
		\foreach \switch in {0,...,\boundCompute} {
		    \pgfmathtruncatemacro{\switchmod}{mod(\switch,\size)}
		    \pgfmathtruncatemacro{\rootmod}{mod(\root,\size)}
		    \ifthenelse{\equal{\switchmod}{\rootmod}}{
			\draw (l2_\switch.north) -- (l3_\root.south);
		    }{}
		}
	    }
	\end{tikzpicture}
      \caption{\label{fig:fat_tree}A fat tree}
    \end{figure}
#+END_EXPORT

# Interestingly, the Stampede website says it's "56 GB/s" but I
# checked and can only find 56 Gbit/s. I hence go with that, as it
# seems more reasonable as well.

Stampede leverages Mellanox FDR InfiniBand technology with
\SI{56}{\giga\bit\per\second}, setup in a
fat-tree topology on two levels (called cores and
leafs) with 8\nbsp{}SX6536 core-switches, each with 648\nbsp{}ports and
\SI{73}{\tera\bit\per\second} capacity, and 320\nbsp{}36-port
leaf-switches with a capacity of \SI{4}{\tera\bit\per\second}. 
20\nbsp{}ports of each leaf-switch are connected to compute nodes, whereas the remaining 16\nbsp{}ports are connected to
the core-switches. In this topology, any message reaches the
destination within no more than 5\nbsp{}hops.

Figure\ref{fig:fat_tree} depicts a fat-tree with *4 (?) levels* (*I'm tired now.*)

*** Summarizing modeling uncertainties
- MKL version ???
- iMPI version ???
- HPL compilation ? Possible modifications s.a. using openMP to have thread taking care of MPI communications and progressions.
** Simulating HPL
*** Performance Prediction
- Gflop Plots, impossibility to reach the desired performance
- Additionnal points in favorable conditions do not improve much
  except when multiplying bw by a factor 10, which is totally
  unrealistic.
*** Investigation
We explain our investigation and possible reasons for the previous
mismatch.
- Gantt chart + analysis + explanation that there is something wrong with communications. Three parts in the communications:
  - synchro
  - bcast
  - update
  Such an execution + tracing is very fast! :)
- Only an overlap of computation and communication would allow to
  obtain such performances. This is really strange as the current
  version has almost no overlap, which can be explained by the
  reported DEPTH=0. But setting DEPTH to 1 is a very strange choice.
- The bcast algorithm seems very intense and ok although it was what
  we initially focused on the most, trying to change it as much as
  possible with little effect on the overall performance. We decided
  to tell our TACC colleagues there was something wrong and they
  explained us they had run the Intel binary. It appears that
  intel_xphpl seems to have another bcast algorithm whose name is
  "HPL_bcast_bpush" and which does non-blocking sends, unlike all
  other algorithms. This is a possible explanation for the improved performance although the output reports blonM was used.
- The update communication is quite long and organized in trees involving really long communications. This is expected with such algorithm but this choice is very surprising for large setups.

We obviously do not aim at reverse engineering the intel_xhpl code but it appears 1) clearly that many optimizations have been done on the communication side and 2) very likely that the reported parameters are not the ones in fullrun, probably because several decisions have been hardcoded. 

We already had conducted successful small-scale validation studies. We
conclude that our large-scale (in)validation is unfortunately not
possible as we do not have access to the original source code used by Intel but we claim that the modifications we did were minor and should easily be applied to their version. Such a simulator would then be a valuable tuning tool.
* Capacity Planning
\AL{Taken from CCgrid or alike ?}
* Conclusions
#+LaTeX: \label{sec:cl}

Prediction of makespan of applications running on large-scale clusters
is an intricate problem. In this article, we explained the problems
that we encountered and how we adjusted parts of HPL to make
emulation feasible. Although we had to change or remove some of the source code of the
program, changesets remained small and were applied to less than $1\%$ of
the code base. These modifications allowed us to run HPL on top of a
simulation framework, SimGrid / SMPI, using just a commodity laptop
instead of a cluster with several thousand nodes.

We also pointed out that not only the application or the runtime may
render an out-of-the-box approach at large-scale infeasible but that
the kernel configuration may be the cause as well. More specifically,
we showed that performance can become unsupportable due to page table
sizes, when support for huge pages is not activated.

Although being capable of predicting an application's performance on a
platform is by itself interesting, we believe that this will become
invaluable in the future to aid compute centers with the decision of
whether a new machine will work best for a given application or if an
upgrade of the current machine should be considered. This goal will be
subject to a more thorough investigation in the very near future.

As we saw in Section\ref{sec:hplchanges}, two BLAS functions (=dgemm=
and =dtrsm=) were the dominating factor with regards to the runtime although other BLAS
functions were called as well. For this study, we neglected the other
functions but with a fully automatic calibration procedure for any
BLAS function results could effortlessly become more precise as the
application could just be linked against a BLAS-replacement
library. 
#+LaTeX: \CH{Problem here: HPL uses \texttt{HPL\_dtrsm()} wrappers.}

* Acknowledgements

Experiments presented in this paper were carried out using the Grid'5000 testbed, supported by a scientific interest group hosted by Inria and including CNRS, RENATER and several Universities as well as other organizations (see https://www.grid5000.fr).

** References                                                       :ignore:

# See next section to understand how refs.bib file is created.

#+LATEX: \bibliographystyle{IEEEtran}
#+LATEX: \bibliography{refs}


* Bib file is here                                                 :noexport:

Tangle this file with C-c C-v t

#+begin_src bib :tangle refs.bib
@IEEEtranBSTCTL{IEEEexample:BSTcontrol,
   CTLuse_article_number = "yes",
   CTLuse_paper = "yes",
   CTLuse_url = "yes",
   CTLuse_forced_etal = "yes",
   CTLmax_names_forced_etal = "6",
   CTLnames_show_etal = "3",
   CTLuse_alt_spacing = "yes",
   CTLalt_stretch_factor = "4",
   CTLdash_repeated_names = "yes",
   CTLname_format_string = "{f. ~}{vv ~}{ll}{, jj}",
   CTLname_latex_cmd = "",
   CTLname_url_prefix = "[Online]. Available:"
}

@mastersthesis{cornebize:hal-01544827,
  TITLE = {{Capacity Planning of Supercomputers: Simulating MPI Applications at Scale}},
  AUTHOR = {Cornebize, Tom},
  URL = {https://hal.inria.fr/hal-01544827},
  SCHOOL = {{Grenoble INP ; Universit{\'e} Grenoble - Alpes}},
  YEAR = {2017},
  MONTH = Jun,
  KEYWORDS = {Simulation ;  MPI runtime and applications ;  Performance prediction and extrapolation ;  High Performance LINPACK},
  PDF = {https://hal.inria.fr/hal-01544827/file/report.pdf},
  HAL_ID = {hal-01544827},
  HAL_VERSION = {v1},
}

@incollection{grid5000,
   title = {Adding Virtualization Capabilities to the {Grid'5000} Testbed},
   author = {Balouek, Daniel and Carpen-Amarie, Alexandra and Charrier, Ghislain and Desprez, Fr{\'e}d{\'e}ric and Jeannot, Emmanuel and Jeanvoine, Emmanuel and L{\`e}bre, Adrien and Margery, David and Niclausse, Nicolas and Nussbaum, Lucas and Richard, Olivier and P{\'e}rez, Christian and Quesnel, Flavien and Rohr, Cyril and Sarzyniec, Luc},
   booktitle = {Cloud Computing and Services Science},
   publisher = {Springer International Publishing},
   OPTpages = {3-20},
   volume = {367},
   editor = {Ivanov, IvanI. and Sinderen, Marten and Leymann, Frank and Shan, Tony },
   series = {Communications in Computer and Information Science },
   isbn = {978-3-319-04518-4 },
   doi = {10.1007/978-3-319-04519-1\_1 },
   year = {2013},
}

%%% Online simulation of MPI applications
@article{xsim,
  author        = "Christian Engelmann",
  title         = {{Scaling To A Million Cores And Beyond: {Using} Light-Weight
                   Simulation to Understand The Challenges Ahead On The Road To
                   Exascale}},
  journal       = "FGCS",
  volume        = 30,
  pages         = "59--65",
  month         = jan,
  year          = 2014,
  publisher     = "Elsevier"}

@Article{sstmacro,
  author = {Curtis L. Janssen and Helgi Adalsteinsson and Scott Cranford and Joseph P. Kenny and Ali Pinar and David A. Evensky and Jackson Mayo},
  journal = {International Journal of Parallel and Distributed Systems},
  title = {A Simulator for Large-scale Parallel Architectures},
  volume = {1},
  number = {2},
  pages = {57--73},
  year = {2010},
  note = "\url{http://dx.doi.org/10.4018/jdst.2010040104}",
  doi = {10.4018/jdst.2010040104}
}

@article{SST,
  author    = {Rodrigues, Arun and Hemmert, Karl and Barrett, Brian
                  and Kersey, Chad and Oldfield, Ron and Weston, Marlo
                  and Riesen, Rolf and Cook, Jeanine and Rosenfeld,
                  Paul and CooperBalls, Elliot and Jacob, Bruce },
  title     = {{The Structural Simulation Toolkit}},
  journal   = {{SIGMETRICS} Performance Evaluation Review},
  volume    = 38,
  number    = 4,
  pages     = {37--42},
  year      = 2011
}

@article{dickens_tpds96,
  title={{Parallelized Direct Execution Simulation of Message-Passing
                  Parallel Programs}},
  author={Dickens, Phillip and Heidelberger, Philip and Nicol, David},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume=7,
  number=10,
  year=1996,
  pages={1090--1105}
}

@ARTICLE{bagrodia_ijhpca01,
  author={Bagrodia, Rajive and Deelman, Ewa and Phan, Thomas},
  title={{Parallel Simulation of Large-Scale Parallel Applications}},
  journal={International Journal of High Performance Computing and
                  Applications},
  volume=15,
  number=1,
  year=2001,
  pages={3--12}
}

%%% Offline simulation of MPI applications
@INPROCEEDINGS{loggopsim_10,
  title={{LogGOPSim - Simulating Large-Scale Applications in 
          the LogGOPS Model}},
  author={Hoefler, Torsten and Siebert, Christian and Lumsdaine, Andrew},
  month=Jun,
  year={2010},
  pages = {597--604},
  booktitle={Proc. of the LSAP Workshop},
}

@inproceedings{hoefler-goal,
  author={T. Hoefler and C. Siebert and A. Lumsdaine},
  title={{Group Operation Assembly Language - A Flexible Way to Express Collective Communication}},
  year={2009},
  booktitle={Proc. of the 38th ICPP}
}

@inproceedings{bigsim_04,
  author={Zheng, Gengbin and Kakulapati, Gunavardhan and Kale,
                  Laxmikant},
  title={{BigSim: A Parallel Simulator for Performance Prediction of
                  Extremely Large Parallel Machines}},
  year=2004,
  booktitle={Proc. of the 18th IPDPS},
}

@inproceedings{dimemas,
	title = {{Dimemas: Predicting MPI Applications Behaviour in Grid Environments}},
	year = {2003},
	month = jun,
	booktitle = {Proc. of the Workshop on Grid Applications and
                  Programming Tools},
	author = {Rosa M. Badia and Jes{\'u}s Labarta and Judit Gim{\'e}nez and Francesc Escal{\'e}}
}

@article {CODES,
 title = {Enabling Parallel Simulation of Large-Scale HPC Network Systems},
 journal = {IEEE Transactions on Parallel and Distributed Systems},
 year = {2016},
 author = {Mubarak, M. and C. D. Carothers and Robert B. Ross and Philip H. Carns}
}

@article{ROSS_SC12,
author = {Misbah Mubarak and Christopher D. Carothers and Robert Ross and Philip Carns},
title = {{Modeling a Million-Node Dragonfly Network Using Massively Parallel Discrete-Event Simulation}},
journal ={SC Companion},
year = {2012},
pages = {366-376},
}

%%% Self citations on previous work
@Article{simgrid,
  title = {{Versatile, Scalable, and Accurate Simulation of Distributed Applications and Platforms}},
  author = {Casanova, Henri and Giersch, Arnaud and Legrand, Arnaud and Quinson, Martin and Suter, Fr{\'e}d{\'e}ric},
  publisher = {Elsevier},
  pages = {2899-2917},
  journal = {Journal of Parallel and Distributed Computing},
  volume = {74},
  number = {10},
  year = {2014}
}

@InProceedings{simetierre,
  author = {Bobelin, Laurent and Legrand, Arnaud and 
                  M{\'a}rquez, David Alejandro Gonz{\'a}lez and Navarro,
                  Pierre and Quinson, Martin and Suter,
                  Fr{\'e}d{\'e}ric and Thiery, Christophe},
  title = 	 {{Scalable Multi-Purpose Network Representation for
                  Large Scale Distributed System Simulation}},
  booktitle = {Proc. of the 12th IEEE/ACM International
                  Symposium on Cluster, Cloud and Grid Computing},
  year = 	 2012,
  pages = {220--227},
  address = 	 {Ottawa, Canada}
}

@InProceedings {simgrid_simix2_12,
  author = {Martin Quinson and Cristian Rosa and Christophe Thi{\'e}ry},
  title = {Parallel Simulation of Peer-to-Peer Systems},
  booktitle = {{P}roc. of the 12th {IEEE/ACM} {I}ntl. {S}ymposium on {C}luster, Cloud and Grid {C}omputing},
  year = {2012},
  address = {Ottawa, Canada}   
} 

@InProceedings {DCLV_LSAP_10,
  title = {{Fast and Scalable Simulation of Volunteer Computing Systems
                  Using SimGrid}},
  booktitle = {Proc. of the Workshop on Large-Scale System and Application
                  Performance},
  year = {2010},
  month = Jun,
  address = {Chicago, IL},
  author = {Donassolo, Bruno and Casanova, Henri and Legrand, Arnaud
                  and Velho, Pedro},
  category = {core}
} 

@InProceedings{SMPI,
  author	= {Clauss, Pierre-Nicolas and Stillwell, Mark and Genaud,
		  St\'ephane and Suter, Fr\'ed\'eric and Casanova, Henri and
		  Quinson, Martin},
  title	= {{Single Node On-Line Simulation of MPI Applications with
		  SMPI}},
  booktitle= {Proc. of the 25th IEEE Intl. Parallel and
		  Distributed Processing Symposium},
  year	= 2011,
  address	= {Anchorage, AK}
}


@Article{Velho_TOMACS13,
  author = {Velho, Pedro and Schnorr, Lucas and Casanova, Henri and Legrand, Arnaud},
  title = 	 {{On the Validity of Flow-level TCP Network Models for Grid and Cloud Simulations}},
  journal = 	 {ACM Transactions on Modeling and Computer Simulation},
  year = 	 {2013},
 PUBLISHER = {ACM}, 
  VOLUME = 23, 
  NUMBER = 4,
  pages = 23, 
  MONTH = Oct
}

@article{degomme:hal-01415484,
  TITLE = {Simulating MPI applications: the SMPI approach},
  AUTHOR = {Degomme, Augustin and Legrand, Arnaud and Markomanolis, Georges and Quinson, Martin and Stillwell, Mark S and Suter, Frédéric},
  JOURNAL = {{IEEE Transactions on Parallel and Distributed Systems}},
  PUBLISHER = {{Institute of Electrical and Electronics Engineers}},
  volume =       "28",
  number =       "8",
  pages =        "2387--2400",
  PAGES = {14},
  YEAR = {2017},
  MONTH = Feb,
  DOI = {10.1109/TPDS.2017.2669305},
  KEYWORDS = {Simulation ; MPI runtime and applications ; Performance prediction and extrapolation},
  PDF = {https://hal.inria.fr/hal-01415484/file/smpi_article.pdf},
  HAL_ID = {hal-01415484},
  HAL_VERSION = {v2},
  category =     "core",
}

@InProceedings{heinrich:hal-01523608,
  title =        "{Predicting the Energy Consumption of MPI Applications
                 at Scale Using a Single Node}",
  author =       "Franz C. Heinrich and Tom Cornebize and Augustin
                 Degomme and Arnaud Legrand and Alexandra Carpen-Amarie
                 and Sascha Hunold and Anne-Cécile Orgerie and Martin
                 Quinson",
  URL =          "https://hal.inria.fr/hal-01523608",
  booktitle =    "Proc. of the 19th IEEE Cluster Conference",
  year =         "2017",
  keywords =     "simulation ; HPC ; energy ; platform modeling",
  pdf =          "https://hal.inria.fr/hal-01523608/file/predicting-energy-consumption-at-scale.pdf",
  hal_id =       "hal-01523608",
  category =     "core",
}

#+end_src

* Emacs Setup 							   :noexport:
# Local Variables:
# eval:    (require 'org-install)
# eval:    (org-babel-do-load-languages 'org-babel-load-languages '( (shell . t) (R . t) (perl . t) (ditaa . t) ))
# eval:    (setq org-confirm-babel-evaluate nil)
# eval:    (unless (boundp 'org-latex-classes) (setq org-latex-classes nil))
# eval:    (add-to-list 'org-latex-classes '("IEEEtran"
# "\\documentclass[conference, 10pt]{IEEEtran}\n \[NO-DEFAULT-PACKAGES]\n \[EXTRA]\n  \\usepackage{graphicx}\n  \\usepackage{hyperref}"  ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")                       ("\\subsubsection{%s}" . "\\subsubsection*{%s}")                       ("\\paragraph{%s}" . "\\paragraph*{%s}")                       ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# eval:    (add-to-list 'org-latex-classes '("llncs" "\\documentclass{llncs2e/llncs}\n \[NO-DEFAULT-PACKAGES]\n \[EXTRA]\n"  ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")                       ("\\subsubsection{%s}" . "\\subsubsection*{%s}")                       ("\\paragraph{%s}" . "\\paragraph*{%s}")                       ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# eval:    (add-to-list 'org-latex-classes '("acm-proc-article-sp" "\\documentclass{acm_proc_article-sp}\n \[NO-DEFAULT-PACKAGES]\n \[EXTRA]\n"  ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")                       ("\\subsubsection{%s}" . "\\subsubsection*{%s}")                       ("\\paragraph{%s}" . "\\paragraph*{%s}")                       ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# eval:    (add-to-list 'org-latex-classes '("sig-alternate" "\\documentclass{sig-alternate}\n \[NO-DEFAULT-PACKAGES]\n \[EXTRA]\n"  ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")                       ("\\subsubsection{%s}" . "\\subsubsection*{%s}")                       ("\\paragraph{%s}" . "\\paragraph*{%s}")                       ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# eval:    (setq org-alphabetical-lists t)
# eval:    (setq org-src-fontify-natively t)
# eval:   (setq ispell-local-dictionary "american")
# eval:   (eval (flyspell-mode t))
# eval:   (setq org-todo-keyword-faces '(("FLAWED" . (:foreground "RED" :weight bold))))
# eval:   (custom-set-variables '(org-babel-shell-names (quote ("sh" "bash" "csh" "ash" "dash" "ksh" "mksh" "posh" "zsh"))))
# eval:   (add-to-list 'load-path ".")
# eval:   (require 'ox-extra)
# eval:   (setq org-latex-tables-centered nil)
# eval:   (ox-extras-activate '(ignore-headlines))
# End:
