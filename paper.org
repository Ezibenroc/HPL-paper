# -*- coding: utf-8 -*-
# -*- org-confirm-babel-evaluate: nil -*-
# -*- mode: org -*-
#+TITLE:
#+LANGUAGE:  en
#+OPTIONS: H:5 author:nil email:nil creator:nil timestamp:nil skip:nil toc:nil ^:nil
#+TAGS: ARNAUD(a) CHRISTIAN(c) ANNE-CECILE(A)
#+TAGS: noexport(n) DEPRECATED(d) ignore(i)
#+TAGS: EXPERIMENT(e) LU(l) EP(e)
#+STARTUP: overview indent inlineimages logdrawer hidestars
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w@) | DONE(d!) CANCELLED(c@) DEFERRED(@) FLAWED(f@)
#+LATEX_CLASS: IEEEtran
#+LATEX_CLASS_OPTIONS: [nofonttune]
#+PROPERTY: header-args :eval never-export
#+LATEX_HEADER: \usepackage{DejaVuSansMono}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: %\usepackage{fixltx2e}
#+LATEX_HEADER: \usepackage{ifthen,figlatex}
#+LATEX_HEADER: \usepackage{longtable}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{wrapfig}
#+LATEX_HEADER: \usepackage{subfigure}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{color,soul}
#+LATEX_HEADER: \usepackage[export]{adjustbox}
#+LATEX_HEADER: \usepackage{xspace}
#+LATEX_HEADER: \usepackage{amsmath,amssymb}
#+LATEX_HEADER: \usepackage[american]{babel}
#+LATEX_HEADER: \usepackage{relsize}
#+LATEX_HEADER: \AtBeginDocument{
#+LATEX_HEADER:   \definecolor{pdfurlcolor}{rgb}{0,0,0.6}
#+LATEX_HEADER:   \definecolor{pdfcitecolor}{rgb}{0,0.6,0}
#+LATEX_HEADER:   \definecolor{pdflinkcolor}{rgb}{0.6,0,0}
#+LATEX_HEADER:   \definecolor{light}{gray}{.85}
#+LATEX_HEADER:   \definecolor{vlight}{gray}{.95}
#+LATEX_HEADER: }
#+LATEX_HEADER: %\usepackage[paper=letterpaper,margin=1.61in]{geometry}
#+LATEX_HEADER: \usepackage{url} \urlstyle{sf}
#+LATEX_HEADER: \usepackage[normalem]{ulem}
#+LATEX_HEADER: \usepackage{todonotes}
#+LATEX_HEADER: \usepackage{fancyvrb}
#+LATEX_HEADER: \usepackage[colorlinks=true,citecolor=pdfcitecolor,urlcolor=pdfurlcolor,linkcolor=pdflinkcolor,pdfborder={0 0 0}]{hyperref}
#+LATEX_HEADER: \usepackage{color,colortbl}
#+LATEX_HEADER: \definecolor{gray98}{rgb}{0.98,0.98,0.98}
#+LATEX_HEADER: \definecolor{gray20}{rgb}{0.20,0.20,0.20}
#+LATEX_HEADER: \definecolor{gray25}{rgb}{0.25,0.25,0.25}
#+LATEX_HEADER: \definecolor{gray16}{rgb}{0.161,0.161,0.161}
#+LATEX_HEADER: \definecolor{gray60}{rgb}{0.6,0.6,0.6}
#+LATEX_HEADER: \definecolor{gray30}{rgb}{0.3,0.3,0.3}
#+LATEX_HEADER: \definecolor{bgray}{RGB}{248, 248, 248}
#+LATEX_HEADER: \definecolor{amgreen}{RGB}{77, 175, 74}
#+LATEX_HEADER: \definecolor{amblu}{RGB}{55, 126, 184}
#+LATEX_HEADER: \definecolor{amred}{RGB}{228,26,28}
#+LATEX_HEADER: \definecolor{amdove}{RGB}{102,102,122}
#+LATEX_HEADER: \usepackage{xcolor}
#+LATEX_HEADER: \usepackage[procnames]{listings}
#+LATEX_HEADER: \lstset{ %
#+LATEX_HEADER:  backgroundcolor=\color{gray98},    % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
#+LATEX_HEADER:  basicstyle=\tt\scriptsize,        % the size of the fonts that are used for the code
#+LATEX_HEADER:  breakatwhitespace=false,          % sets if automatic breaks should only happen at whitespace
#+LATEX_HEADER:  breaklines=true,                  % sets automatic line breaking
#+LATEX_HEADER:  showlines=true,                   % sets automatic line breaking
#+LATEX_HEADER:  captionpos=b,                     % sets the caption-position to bottom
#+LATEX_HEADER:  commentstyle=\color{gray30},      % comment style
#+LATEX_HEADER:  extendedchars=true,               % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
#+LATEX_HEADER:  frame=single,                     % adds a frame around the code
#+LATEX_HEADER:  keepspaces=true,                  % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
#+LATEX_HEADER:  keywordstyle=\color{amblu},       % keyword style
#+LATEX_HEADER:  procnamestyle=\color{amred},      % procedures style
#+LATEX_HEADER:  language=[95]fortran,             % the language of the code
#+LATEX_HEADER:  numbers=left,                     % where to put the line-numbers; possible values are (none, left, right)
#+LATEX_HEADER:  numbersep=5pt,                    % how far the line-numbers are from the code
#+LATEX_HEADER:  numberstyle=\tiny\color{gray20},  % the style that is used for the line-numbers
#+LATEX_HEADER:  rulecolor=\color{gray20},         % if not set, the frame-color may be changed on line-breaks within not-black text (\eg comments (green here))
#+LATEX_HEADER:  showspaces=false,                 % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
#+LATEX_HEADER:  showstringspaces=false,           % underline spaces within strings only
#+LATEX_HEADER:  showtabs=false,                   % show tabs within strings adding particular underscores
#+LATEX_HEADER:  stepnumber=2,                     % the step between two line-numbers. If it's 1, each line will be numbered
#+LATEX_HEADER:  stringstyle=\color{amdove},       % string literal style
#+LATEX_HEADER:  tabsize=2,                        % sets default tabsize to 2 spaces
#+LATEX_HEADER:  % title=\lstname,                    % show the filename of files included with \lstinputlisting; also try caption instead of title
#+LATEX_HEADER:  procnamekeys={call}
#+LATEX_HEADER: }
#+LATEX_HEADER: \definecolor{colorfuncall}{rgb}{0.6,0,0}
#+LATEX_HEADER: \newcommand{\prettysmall}{\fontsize{6}{8}\selectfont}
#+LATEX_HEADER: \let\oldtexttt=\texttt
#+LATEX_HEADER: \renewcommand\texttt[1]{\oldtexttt{\smaller[1]{#1}}}
# #+LATEX_HEADER: \usepackage[round-precision=3,round-mode=figures,scientific-notation=true]{siunitx}
#+LATEX_HEADER: \usepackage[binary-units]{siunitx}
#+LATEX_HEADER: \DeclareSIUnit\flop{Flop}
#+LATEX_HEADER: \DeclareSIUnit\flops{\flop\per\second}
#+LATEX_HEADER:\usepackage{tikz}
#+LATEX_HEADER:\usetikzlibrary{arrows,shapes,positioning,shadows,trees,calc}
#+LATEX_HEADER:\usepackage{pgfplots}
#+LATEX_HEADER:\pgfplotsset{compat=1.13}

#+LATEX_HEADER: \usepackage{enumitem}
#+LATEX_HEADER: \setlist[itemize,1]{leftmargin=\dimexpr 26pt-.2in}
#+LATEX_HEADER: \usepackage[mode=buildnew]{standalone}
#+LATEX_HEADER: \usepackage[ruled,vlined,english]{algorithm2e}
#+LATEX_HEADER: \DontPrintSemicolon

#+LaTeX: \newcommand\myemph[1]{\color{colorfuncall}\textbf{#1}}%

#+LaTeX: \newcommand\labspace[1][-0.25cm]{\vspace{#1}}
#+LaTeX: \renewcommand\O{\ensuremath{\mathcal{O}}\xspace}%

#+BEGIN_EXPORT latex
\makeatletter
\newcommand{\removelatexerror}{\let\@latex@error\@gobble}
\makeatother
#+END_EXPORT

* LaTeX Preamble                                                     :ignore:
#+BEGIN_EXPORT latex
\let\oldcite=\cite
\renewcommand\cite[2][]{~\ifthenelse{\equal{#1}{}}{\oldcite{#2}}{\oldcite[#1]{#2}}\xspace}
\let\oldref=\ref
\def\ref#1{~\oldref{#1}\xspace}
\def\eqref#1{~(\oldref{#1})\xspace}
\def\ie{i.e.,\xspace}
\def\eg{e.g.,\xspace}
\def\etal{~\textit{et al.\xspace}}
%\newcommand{\AL}[2][inline]{\todo[caption={},color=green!50,#1]{\small\sf\textbf{AL:} #2}}
%\newcommand{\TC}[2][inline]{\todo[caption={},color=blue!50,#1]{\small\sf\textbf{TOM:} #2}}
%\newcommand{\CH}[2][inline]{\todo[color=red!30,#1]{\small\sf \textbf{CH:} #2}}
\newcommand{\AL}[2][inline]{}
\newcommand{\TC}[2][inline]{}
\newcommand{\CH}[2][inline]{}

%% Omit the copyright space.
%\makeatletter
%\def\@copyrightspace{}
%\makeatother

%\def\IEEEauthorblockN#1{\gdef\IEEEauthorrefmark##1{\ensuremath{{}^{\textsf{##1}}}}#1}
%\newlength{\blockA}
%\setlength{\blockA}{.35\linewidth}
%\def\IEEEauthorblockA#1{
%  \scalebox{.9}{\begin{minipage}{\blockA}\normalsize\sf
%    \def\IEEEauthorrefmark##1{##1: }
%    #1
%  \end{minipage}}
%}
% \def\IEEEauthorrefmark#1{#1: }

\title{Emulating High Performance Linpack on a Commodity Server at the Scale of a Supercomputer}
%\title{Simulating the Energy Consumption of MPI~Applications}
% Predicting the Performance and the Power Consumption of MPI Applications With SimGrid
  %\titlerunning{Power-aware simulation for large-scale systems with SimGrid}
  %

  \author{
  \IEEEauthorblockN{Tom Cornebize, Franz C. Heinrich, Arnaud Legrand}\\
  \IEEEauthorblockA{Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, 38000 Grenoble, France\\
    tom.cornebize@univ-grenoble-alpes.fr, franz-christian.heinrich@inria.fr, arnaud.legrand@imag.fr}
  }


  \maketitle              % typeset the title of the contribution
#+END_EXPORT
* Abstract                                                           :ignore:
#+LaTeX: \begin{abstract}
The Linpack benchmark, in particular the High-Performance Linpack
(HPL) implementation, has emerged as the de-facto standard benchmark
to rank supercomputers in the TOP500. With a power consumption of
several MW per hour on a TOP500 machine, test-running HPL on the whole
machine for hours is extremely expensive. With core-counts beyond the
100,000 cores threshold being common and sometimes even ranging into
the millions, an optimization of HPL parameters (problem size, grid
arrangement, granularity, collective operation algorithms, etc.)
specifically suited to the network topology and performance is
essential. Such optimization can be particularly time consuming and
can hardly be done through simple mathematical performance models. In
this article, we explain how we both extended the SimGrid's SMPI
simulator and slightly modified HPL to allow a fast emulation of HPL
on a single commodity computer at the scale of a supercomputer. More
precisely, we take as a motivating use case the large-scale run
performed on the Stampede cluster at TACC in 2013, when it got ranked
6th in the TOP500. While this qualification run required the
dedication of 6,006 computing nodes of the supercomputer and more than
120\nbsp{}TB of RAM for more than 2\nbsp{}hours, we manage to simulate a similar
configuration on a commodity computer with 16\nbsp{}GB of RAM in about
47\nbsp{}hours. Allied to a careful modeling of Stampede, this simulation
allows us to evaluate the performance that would have been obtained
using the freely available version of HPL. Such performance reveals much
lower than what was reported and which was obtained using a
closed-source version specifically designed by the Intel
engineers. Our simulation allows us to hint where the main algorithmic
improvements must have been done in HPL. 
#+LaTeX: \end{abstract}


#+BEGIN_EXPORT latex
% this is need to trim the number of authors and et al. for more than 3 authors
\bstctlcite{IEEEexample:BSTcontrol}
#+END_EXPORT
* Introduction

The world's largest and fastest machines are ranked twice a year in the so-called
TOP500 list. Among the benchmarks that are often used to evaluate
those machines, the Linpack benchmark, in particular the High-Performance Linpack (HPL)
implementation, has emerged as the de-facto standard benchmark, although
other benchmarks such as HPCG and HPGMG have recently been proposed to
become the new standard. Today, machines with 100,000\nbsp{}cores 
and more are common and several machines beyond the 1,000,000\nbsp{}cores mark
are already in production. This high density of computation units requires diligent optimization of application
parameters, such as problem size, process organization or choice of algorithm, as these
have an impact on load distribution and network utilization.
Furthermore, to yield best benchmark results,
runtimes (such as OpenMPI) and supporting libraries such as LAPACK
(particularly the by HPL heavily used =DGEMM= routine) need to be fine-tuned and adapted to the
underlying platform. 

Alas, it takes typically several hours to run HPL on the list's number one system.
This duration, combined with the power consumption that often reaches several MW per
hour for TOP500 machines, makes it financially infeasible to test-run HPL on the whole 
machine just to tweak parameters. 
Yet, performance results of an already deployed, current-generation machine typically also
play a role in the funding process for future machines. Results near
the optimal performance for the current machine are hence considered critical for
HPC centers and vendors. These entities would benefit largely from being able to
tune parameters without actually running the benchmark for hours.
# This estimation can be done either via (mathematical) performance models (e.g., by
# estimating performance of specific functions) or by a simulation based approach.
# While performance models neglect the
# oftentimes serious impact of the network (\eg due to congestion, shared bandwidth,
# ...), this is not in general true for the simulation approach.

# \CH{Furthermore, simulations can be used to validate/check that the execution went well (operated near the peak performance) but can also help to find the right parameters for the application, runtime and network.}

In this article, we explain how to predict the performance of HPL
through simulation with the SimGrid/SMPI simulator. We particularly detail how we obtained
faithful models for several functions (\eg =DGEMM= and =DTRSM=) and how we managed
to reduce the memory consumption from more than a hundred terabytes to several
gigabytes, allowing us to emulate HPL on a commonly available server node.
We demonstrate the effectiveness of our solution by
simulating the run conducted on the Stampede cluster at
TACC in 2013 for the TOP500 . 
#+LATEX: \AL{Rephrase this maybe? We after all didn't succeed.}

This article is organized as follows:
Section\ref{sec:con} presents the main characteristics of the HPL
application and provides detail on the run that was conducted at TACC
in 2013.  Section\ref{sec:relwork} discusses existing related work and
explains why emulation (or /online simulation/) is the only relevant
approach when studying an application as complex as HPL. In
Section\ref{sec:smpi}, we briefly present the simulator we used for
this work, SimGrid/SMPI, followed by an
extensive discussion in Section\ref{sec:em} about the
optimizations on all levels (\ie simulator, application, system) that
were necessary to make a large-scale run feasible. The scalability of
our approach is evaluated in Section\ref{sec:scalabilityevol}. The
modeling of the Stampede platform and the comparison of our simulation
with the 2013 execution is detailed in
Section\ref{sec:science}. Lastly, Section\ref{sec:cl} concludes this
article by summarizing our contributions.

* Context
#+LaTeX: \label{sec:con}

# The HPLinpack benchmark consists of a set of rules: A set of linear
# equations, $Ax = b$, needs to be solved and it requires furthermore that the input matrix can be of
# arbitrary dimension =n= and that O(n³) + O(n²) operations be used
# (hence, Strassen's matrix multiplication is prohibited).

** High-Performance Linpack
\label{sec:hpl}
#+BEGIN_EXPORT latex
\begin{figure}
  \newcommand{\mykwfn}[1]{{\bf\textsf{#1}}}%
  \SetAlFnt{\sf}%
  \SetKwSty{mykwfn}%
  \SetKw{KwStep}{step}%
  \centering
  \begin{minipage}[m]{0.4\linewidth}
    % \vspace{0.3cm} % ugly, could not align the drawing with the algorithm with minipages or tabular...
    \begin{tikzpicture}[scale=0.23]
      \draw (0, 0) -- (0, 12) -- (12, 12) -- (12, 0) -- cycle;
      \foreach \i in {2}{
        \draw [fill=lightgray] (\i, 0) -- (\i, 12-\i) -- (12, 12-\i) -- (12, 0) -- cycle;
        \draw [fill=gray] (\i, 12-\i) -- (\i, 12-\i-1) -- (\i+1, 12-\i-1) -- (\i+1, 12-\i) -- cycle;
        \draw[very thick, -latex] (\i,12-\i) -- (\i+2,12-\i-2);
        \draw[<->] (\i, 12-\i+0.5) -- (\i+1, 12-\i+0.5) node [pos=0.5, yshift=+0.15cm] {\scalebox{.8}{\texttt{NB}}};
      }
      \foreach \i in {3}{
        \draw [fill=white] (\i, 0) -- (\i, 12-\i) -- (12, 12-\i) -- (12, 0) -- cycle;
        \draw (\i,12-\i) -- (\i,0);
        \draw[very thick, -latex] (\i,12-\i) -- (\i+2,12-\i-2);
      }
      \draw[dashed] (0, 12) -- (12, 0);
      \node(L) at (2, 2) {\ensuremath{\boldsymbol{L}}};
      \node(U) at (10, 10) {\ensuremath{\boldsymbol{U}}};
      \node(A) at (8, 4) {\ensuremath{\boldsymbol{A}}};
      \draw[<->] (0, -0.5) -- (12, -0.5) node [pos=0.5, yshift=-0.3cm] {$N$};

    \end{tikzpicture}
  \end{minipage}%
  \begin{minipage}[m]{0.6\linewidth}
    \removelatexerror
    \begin{algorithm}[H]
      allocate and initialize $A$\;
      \For{$k=N$ \KwTo $0$ \KwStep \texttt{NB}}{
        allocate the panel\;
        factor the panel\;
        broadcast the panel\;
        update the matrix;
      }
    \end{algorithm}
    \vspace{1em}
  \end{minipage}
  
  \caption{Overview of High Performance Linpack}\vspace{-1em}
  \label{fig:hpl_overview}
\end{figure}
#+END_EXPORT

For this work, we use the freely-available reference-implementation of
the High-Performance Linpack benchmark\cite{HPL}, HPL, which is 
used to benchmark systems for the TOP500\cite{top500} list. HPL
requires MPI to be available and implements
a LU decomposition, \ie a factorization of a square matrix $A$ as the
product of a lower triangular matrix $L$ and an upper triangular
matrix $U$. HPL checks the correctness of this factorization by
solving a linear system $A\cdot{}x=b$, but only the factorization step is
benchmarked.  The factorization is based on a right-looking variant of
the LU factorization with row partial pivoting and allows multiple
look-ahead depths. The working principle of the factorization is depicted in
Figure\ref{fig:hpl_overview} and consists of a series of panel
factorizations followed by an update.
HPL uses a two-dimensional block-cyclic data distribution of $A$. Furthermore, HPL implements several custom
collective communication algorithms to efficiently overlap communication
with computation.
#+LATEX: \CH{These two sentences need to be better integrated.}
# (e.g., when pivoting).

The main parameters of HPL are listed subsequently:
- $N$ is the order of the square matrix $A$.
- =NB= is the ''blocking factor``, \ie the granularity at
  which HPL operates when panels are distributed or worked on.
- $P$ and $Q$ denote the number of process rows and the
  number of process columns, respectively;
- =RFACT= determines the panel factorization algorithm. Possible values: Crout, left- or right-looking.
- =SWAP= specifies the swapping algorithm used while pivoting. Two
  algorithms are available: one based on /binary exchange/ (along a virtual tree topology) and the other one based on
  a /spread-and-roll/ (with a higher number of parallel communications). HPL
  also provides a panel-size threshold triggering a switch from one variant to the other.
- =BCAST= sets the algorithm used to broadcast the
  panel *of columns* to the other *process columns*. Legacy versions of
  the MPI standard only supported non-blocking point-to-point communications but did
  not support non-blocking collective communications, which is why HPL
  ships with in total 6 self-implemented variants to efficiently
  overlap the time spent waiting for an incoming panel with updates to
  the trailing matrix: =ring=, =ring-modified=, =2-ring=, =2-ring-modified=,
  =long=, and =long-modified=. The =modified= versions guarantee that
  the process right after the root (\ie the process that will become the root
  in the next iteration) receives data first and does not participate
  further in the broadcast. This process can thereby start working on the
  panel as soon as possible. The =ring= and =2-ring= versions correspond
  to the name-giving two virtual topologies while the =long= version
  is a /spread and roll/ algorithm where messages are chopped into $Q$
  pieces. This generally leads to better bandwidth exploitation. The =ring= and
  =2-ring= variants rely on =MPI_Iprobe=, meaning they
  return control if no message has been fully received yet and hence
  facilitate partial overlapping of communication with computations. In HPL 2.2 and 2.1, this capability
  has been deactivated for =long=. A comment in the source code states that some
  machines apparently get stuck when there are too many ongoing messages.
- =DEPTH= controls how many iterations of the outer loop can overlap with each other.

#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
  \includegraphics[width=.85\linewidth,page=1]{./figures/stampede.pdf}                                                                                                                               
  \vspace{-.5em}\caption{The fat-tree network topology of Stampede.}\vspace{-.5em}
  \label{fig:fat_tree_topology}
  \labspace
\end{figure}
#+END_EXPORT

The sequential complexity of this factorization is 
$\mathrm{flop}(N) = \frac{2}{3}N^3 + 2N^2 + \O(N)$ where $N$ is the
order of the matrix to factorize. The time complexity *is expected* to
be: $$T(N) \approx \frac{\left(\frac{2}{3}N^3 + 2N^2\right)}{P\cdot{}Q\cdot{}w} + \Theta((P+Q)\cdot{}N^2),$$ where
$w$ is the flop rate of *each node* and 
#+LATEX: \AL{Should we note that they're considered homogenous ("each" sounds like it could change)?}
the second term corresponds to the communication overhead which is
influenced by the previously listed parameters (=RFACT=, =SWAP=, =BCAST=,
=DEPTH=, \ldots). 
After each run, HPL reports the overall flop
rate $\mathrm{flop}(N)/T(N)$ (expressed in \si{\giga\flops}) and also
several configurations. See Figure\ref{fig:hpl_output} for a (shortened)
example output.

A large-scale execution of HPL on a real machine in order to submit to the TOP500
can therefore be quite time consuming as all the BLAS kernels, the MPI runtime, and HPL's numerous parameters
need to be tuned carefully in order to reach optimal performance.
** A Typical Run on a Supercomputer
\label{sec:stampede}
In June 2013, the Stampede supercomputer at TACC was ranked 6th in the
TOP500 by achieving \SI{5168.1}{\tera\flops} and was still ranked 20th in
June 2016. In 2017, this machine got upgraded and renamed Stampede2. The Stampede platform
consisted of 6400 Sandy Bridge nodes, each with two 8-core Xeon E5-2680 and one
Intel Xeon Phi KNC MIC coprocessor. The nodes were interconnected
through a \SI{56}{\giga\bit\per\second} FDR InfiniBand 2-level Clos
fat-tree topology built on Mellanox switches. As can be seen in 
Figure\ref{fig:fat_tree_topology}, the 6400 nodes are
divided into groups of 20, with each group being connected to one of the 320 36-port switches (\SI{4}{\tera\bit\per\second}
capacity), which are themselves connected to 8 648-port
''core\nbsp{}switches`` (each with a capacity of \SI{73}{\tera\bit\per\second}). 
The peak performance of the 2 Xeon CPUs per node was approximately \SI{346}{\giga\flops},
while the peak performance of the KNC co-processor was about
\SI{1}{\tera\flops}. The theoretical peak performance of the
platform was therefore \SI{8520}{\tera\flops}. However, in the TOP500, Stampede
was ranked with \SI{5168}{\tera\flops}. According to the log submitted
to the TOP500 (see Figure\ref{fig:hpl_output}) that was provided to us,
this execution took roughly two hours and used $77\times78 = 6,006$
processes. The matrix of order $N = 3,875,000$ occupied approximately
\SI{120}{\tera\byte} of memory, \ie \SI{20}{\giga\byte} per node.
One MPI process per node was used and the exploitation of each node's
computational resources (the 16 CPU-cores and the Xeon Phi) *may have been done through openMP* and/or
Intel's MKL.
#+LATEX: \CH{This is too vague. Not sure OpenMP should be mentioned.}

#+BEGIN_EXPORT latex
\begin{figure}%[!htb]
  \centering
  \scalebox{.73}{\begin{minipage}[b]{.68\textwidth}
  \lstset{frame=bt,language=html,numbers=none,escapechar=£}\lstinputlisting{fullrun_hpl.txt}
  \end{minipage}}
  \caption{HPL output submitted in June 2013 for the ranking of Stampede in the TOP500.}\vspace{-1em}
  \label{fig:hpl_output}
\end{figure}
#+END_EXPORT

*** Hidden information about the Stampede execution              :noexport:
#+BEGIN_SRC C :exports none :tangle fullrun_hpl.txt
================================================================================
HPLinpack 2.1  --  High-Performance Linpack benchmark  --   October 26, 2012
Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTK
Modified by Piotr Luszczek, Innovative Computing Laboratory, UTK
Modified by Julien Langou, University of Colorado Denver
================================================================================


The following parameter values will be used:

£\myemph{N}£        : £\myemph{3875000}£
£\myemph{NB}£       :   £\myemph{1024}£
PMAP     : Column-major process mapping
£\myemph{P}£        :      £\myemph{77}£
£\myemph{Q}£        :      £\myemph{78}£
PFACT    :   Right 
NBMIN    :       4 
NDIV     :       2 
RFACT    :   Crout 
BCAST    :  BlongM 
DEPTH    :       0 
SWAP     : Binary-exchange
L1       : no-transposed form
U        : no-transposed form
EQUIL    : no
ALIGN    :    8 double precision words

--------------------------------------------------------------------------------


[...]


Peak Performance = 5172687.23 GFlops /   861.25 GFlops per node
================================================================================
T/V                N    NB     P     Q               Time                 Gflops
--------------------------------------------------------------------------------
WC05C2R4     3875000  1024    77    78            7505.72            £\myemph{5.16811e+06}£
HPL_pdgesv() start time Sun Jun  2 13:04:59 2013

HPL_pdgesv() end time   Sun Jun  2 15:10:04 2013

--------------------------------------------------------------------------------
||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=        0.0007822 ...... PASSED
#+END_SRC

** Performance Evaluation Challenges
:LOGBOOK:
- State "TODO"       from              [2017-11-15 mer. 16:26]
:END:
#+LaTeX: \label{sec:con:diff}

The performance achieved by Stampede, \SI{5168}{\tera\flops}, needs to
be compared to the peak performance of the 6,006 nodes, \ie
\SI{8084}{\tera\flops}. This discrepancy may be attributed to the node
usage (\eg, the MKL), to the MPI library, to the network topology that
may be unable to deal with the very intensive communication workload, to
load imbalance among nodes because some node happens to be slower for some
reason (defect, system noise, \ldots), to the algorithmic structure of
HPL, etc. In any case, it is difficult to know precisely *what to expect*
without running the application on a machine of this complexity.
#+LATEX: \CH{We talk about what caused the problem here...}

It is clear that due to the level of complexity of both HPL and
the underlying hardware, simple performance models (analytic expressions based
on $N, P, Q$ and estimations of platform characteristics as presented in
Section\ref{sec:hpl}) may be able to provide trends but can by no means
predict the performance for each configuration (\ie consider the
exact effect of HPL's 6 different broadcast algorithms on network
contention). Additionally, these expressions do not allow
engineers to improve the performance through actively identifying performance bottlenecks.
For complex optimizations such as partially non-blocking
collective communication algorithms intertwined with computations,
very faithful modeling of both the application and the platform is
required. Given the scale of this scenario
(3,785\nbsp{}*steps* on 6,006 nodes in two hours), precise
simulations *become* quickly intractable without further work.
* Related Work
#+LaTeX: \label{sec:relwork}

Performance prediction of MPI application through simulation has been
widely studied over the last decades, with today's literature distinguishing mainly
between two approaches: offline and online simulation.

With the most common approach, /offline simulation/, the application
is time-independently traced during an execution on the real
platform. For this execution, only the order of MPI operations and CPU
bursts are of interest so that they can be replayed on top of a simulator
that calculates the timings through its implemented models of the network and the MPI
layer.
#+LATEX: \CH{The previous sentence is too long and complex.}
Once obtained, the trace can be replayed without re-executing the
application on the real platform. Researchers interested in finding out how
their application reacts to changes to the underlying platform can
then modify the platform model as often as they wish and study the
new application's behavior. 
#+LATEX: \CH{This is ugly.}
Most HPC simulators available today, among them BigSim\cite{bigsim_04},
Dimemas\cite{dimemas} and CODES\cite{CODES}, rely on this approach.

The main limitation of this approach comes from the trace
acquisition requirement. Obtaining a trace up front at such scale and complexity
generally renders it impossible to accurately calculate the trace, hence requiring
a full-scale execution. This, however, is exactly what we aim to avoid.
Additionally, the traces contain only information about a single run
and cannot give clues about the consequences (\eg in terms of
communication patterns) of simple modifications of application
parameters. For simple applications (\eg =stencil=), it may be
possible to extrapolate behavior from small-scale
traces\cite{scalaextrap,pmac_lspp13} but whenever the application relies on
non-blocking communication patterns, the execution becomes
non-deterministic. In the case of HPL, this approach is thus unusable
due to the size of the obtained traces (\approx 110 TB for 
Stampede) and the complexity of the application (\eg six broadcast strategies implemented by HPL)
that significantly influence the performance.
\TC{Also, the trace is simply too large? E.g. the trace for the Stampede's simulation would take more than 100GB (compressed).}

The second approach discussed in literature is /online simulation/ and
is required to study HPL.
Here, the application is executed (emulated) in a tightly controlled way on top of the simulator. The
simulator itself is responsible for determining when each process
should be allowed to run. Executing the application allows researchers
to observe its behavior *at the level of MPI*. Only a few
recent simulators support the online approach, such as the
extreme-scale simulator xSim\cite{xsim}, SST Macro\cite{sstmacro} and
SimGrid/SMPI\cite{simgrid}. To the best of our knowledge, only SST Macro and
SimGrid/SMPI are openly available and mature enough to the faithful emulation of
HPL as xSim is at this point not freely available. For the following
discussion of our work, we rely on SimGrid as we 
have an excellent knowledge of its internals although the developments we
propose would a priori also be possible with SST Macro. Emulation with
SimGrid comes with at least two challenges:
- Firstly, as we have seen before, the time-complexity of the algorithm is $\mathcal{O}(N^3)$ and
  $\mathcal{\Theta}(N^2)$ communications are performed, with $N$ being very
  large. The execution on the Stampede cluster took roughly two hours
  on 6,006\nbsp{}compute nodes. Using only a single node, a naive
  emulation of HPL at the scale of the Stampede run would take about
  500\nbsp{}days if perfect scaling is reached. Although the emulation could
  be done in parallel, we want to limit the computing resources used
  for an emulation. 
- Secondly, the tremendous memory consumption and consequent high
  number of RAM accesses for read/write operations need to be dealt with.

# Real execution:
# - Matrix of order 3,875,000
# - Using 6,006 MPI processes
# - About 2 hours
# Requirement for the emulation of Stampede's execution:
# - $\ge 3, 875, 000 2 \times 8$ bytes \approx 120 terabytes of memory
# - $\ge 6, 006 \times 2$ hours \approx 500 days (very optimistic)
  
* SimGrid/SMPI in a nutshell
\label{sec:smpi}

SimGrid\cite{simgrid} is flexible and open-source
simulation framework that was originally designed in 2000 to study scheduling
heuristics tailored for heterogeneous computing grid
environments. Since then, SimGrid has been employed to study cloud and
peer-to-peer systems. SMPI\cite{smpi} is a recently developed
simulator based on SimGrid that supports faithful simulation
of unmodified MPI applications written in C/C++ or FORTRAN.  

Many simulators of distributed computing systems focus foremost on
scalability. Although SimGrid has been successfully used to simulate \eg
peer-to-peer systems comprising two million peers using only a
single standard machine\cite{simgrid_simix2_12}, the main focus of the SimGrid development
team has been to provide validated performance models, particularly
for *network-enabled* operations. This is generally done by comparing simulation
predictions with results obtained through real experiments in order to
confirm network and application models and improve the
quality. In\cite{heinrich:hal-01523608}, we have for example shown how
SMPI can be used to accurately and consistently predict (within a few
percent) both the performance and the energy consumption of some of
the NAS parallel benchmarks as well as of HPL for small-scale clusters (up to
144 cores in\cite{heinrich:hal-01523608} and up to 128 nodes
in\cite{smpi}). In this article, we try to make the comparison on a
much larger scale. This scale, however, comes with the downside of a less well-controlled scenario since the
Stampede run of HPL was done several years ago and we only have
very limited information about the setup (\eg software versions).

** MPI Communication Modeling
The complex network optimizations done in real MPI implementations
pose a challenge when modeling MPI applications.
Transmission protocols (\eg eager or rendez-vous) can be selected
based on the message size, with each protocol incurring its own
synchronization semantics.
These optimizations furthermore cause the network's latency and bandwidth
factors to depend on message size. To capture this behavior, SMPI relies on
a generalization of the LogGPS model\cite{smpi} where several
synchronization and performance modes can be specified. This model
needs to be instantiated once per platform through a carefully controlled series of messages
(=MPI_Send= and =MPI_Recv=) between two nodes and through a set of
piece-wise linear regressions.

The second challenge concerns modeling network topologies
and contention. SMPI relies on SimGrid's flow-level communication models: each ongoing
communication is represented by a /flow/ and thus as a single entity rather than
a number of individual packets. With *steady-state* being assumed, contention
between active communications is modeled as a bandwidth sharing
problem that accounts for non-trivial phenomena (\eg RTT-unfairness of
TCP, cross-traffic interference or network
heterogeneity\cite{Velho_TOMACS13}). However, a communication
that starts or ends has to trigger a possibly expensive re-computation
of the bandwidth sharing in order to keep results accurate, making
this approach too slow and complex to scale to large platforms. Transient
phenomena and network protocol *instability* are not modeled but network
topology and heterogeneity are accounted for. A consequence of this flow-based model is that the *simulation cost* of a
message passing through the network is independent of its size. This
is advantageous for large-scale applications frequently sending large messages.
#+LATEX: \CH{"A consequence ..." This statement sounds wrong?}

The third and last challenge in terms of performance modeling
deals with collective operations which generally play a key factor for 
application performance. Consequently, performance optimization
of these operations has been studied intensively. Today, MPI implementations
commonly have several alternatives for each collective operation
and select one at runtime, depending on message size and communicator
geometry. SimGrid's SMPI layer implements all the specific collective
communication algorithms from several real MPI implementations (\eg
Open MPI, MPICH, ...) and their selection logic to ensure that any
simulated run of an application is as faithful as possible to real
executions. HPL, however, ships with its own implementation of
collective operations and therefore this is not likely to be the source of
discrepancy between simulation and real executions.
#+LATEX: \CH{These discrepancies were not yet discussed here. And what is the challenge?}

** Application Behavior Modeling
SMPI relies on the /online/
simulation approach where the application code is executed and part of
the instruction stream is intercepted and passed on to a simulator. 
SMPI maps every MPI process of the application onto a
lightweight simulation thread, which is run in mutual exclusion from
the others. Every time a thread enters an MPI call, 
it yields to the simulation kernel and the time it spent
computing (in isolation from every other thread) since the previous
MPI call can be injected into the simulator as a virtual delay. 

Since all simulated MPI ranks are mapped to threads of a single
process, they are effectively folded into the same address space
and therefore global variables in the MPI application are shared
between threads unless they are /privatized/ to ensure that the
simulated MPI ranks are still isolated from each others. Several
technical approaches are possible to handle this issue\cite{smpi}. The solution
SMPI uses by default consists of remapping the data
segment containing global variables of the code to a rank specific
copy of that segment each time we context switch from a process to another. This
remapping is done using the ~mmap~ system call, which leverages the
virtual memory mechanism of the operating system.
The main drawback of this approach is that it is somewhat 
expensive in terms of both simulation time and memory requirements
since the whole parallel application is actually executed on a single
host machine.
#+LATEX: \CH{This sentence doesn't make sense to me?}
SMPI provides two simple annotation mechanisms allowing its users
to exploit the regularity of HPC applications and to drastically
reduce both memory footprint and simulation duration of an emulation:
- Kernel sampling :: for many applications, the *execution structure* is
     independent *on* the computation results. Most
     computation-intensive kernels (\eg BLAS kernels in the case of
     HPL) can then actually be skipped for the purpose of the
     simulation. SMPI supports several macros (\eg
     =SMPI_SAMPLE_LOCAL= and =SMPI_SAMPLE_GLOBAL=) to annotate regular
     computation kernels. The regularity allows SMPI to execute these
     regions a few times in a simulation run to obtain estimations of
     their cost. Once enough samples samples have been obtained, they
     are skipped and the cost is derived from the previously gathered samples.
- Memory  folding :: SMPI provides the =SMPI_SHARED_MALLOC= (=SMPI_SHARED_FREE)= macro to
     replace calls to =malloc= (=free=). Through these macros, programmers can
     indicate to SMPI that some data structures can safely be
     shared between processes in simulation and that the data 
     contained within is not of critical importance (\eg an input
     matrix) for the execution and may even be overwritten in
     simulation. This is possible since in simulation, only the
     behavior of the application but not the correctness of computation results are of
     concern. When used, SimGrid allocates a single block of physical memory (of default size \SI{1}{\mega\byte}) for the whole
     execution which is shared by all MPI processes.
     =SMPI_SHARED_MALLOC= then only reserves a range of virtual addresses of the desired
     size and cyclically maps them onto the previously obtained
     physical address, as illustrated by Figure\ref{fig:global_shared_malloc}. 
     This mechanism allows applications to obtain a nearly constant memory
     footprint, regardless of the size of the actual allocations.

    # At the first call to =SMPI_SHARED_MALLOC=, a temporary file is created. The file descriptor is a global variable,
    # accessible by all the MPI processes, since they are implemented by POSIX threads.

    # At every call to =SMPI_SHARED_MALLOC=, a first call to =mmap= is done with the required size and the flag =MAP_ANONYMOUS=
    # (thus without any file descriptor). The effect of this call is to reserve the whole interval of virtual
    # addresses. Then, for each sub-interval, a new call to =mmap= is done with the temporary file. The address of the
    # sub-interval itself is passed with the flag =MAP_FIXED=, which forces the mapping to keep the same virtual address.
    # As a result, each of these sub-intervals of virtual addresses are mapped onto a same interval of physical
    # addresses. We therefore have a block of virtual addresses of arbitrary size backed by a constant amount of physical
    # memory. Since there are almost no computations left, this is harmless with respect to the simulation. Note that such
    # allocations cannot be fully removed as many parts of the code
    # still access it from time to time.

    #+BEGIN_EXPORT latex
    \tikzset{draw half paths/.style 2 args={%
      % From https://tex.stackexchange.com/a/292108/71579
      decoration={show path construction,
        lineto code={
          \draw [#1] (\tikzinputsegmentfirst) --
             ($(\tikzinputsegmentfirst)!0.5!(\tikzinputsegmentlast)$);
          \draw [#2] ($(\tikzinputsegmentfirst)!0.5!(\tikzinputsegmentlast)$)
            -- (\tikzinputsegmentlast);
        }
      }, decorate
    }}
    \begin{figure}%[htbp]
      \centering
      \begin{tikzpicture}[scale=0.7]
        \pgfmathtruncatemacro{\size}{4}
        \pgfmathtruncatemacro{\width}{2}
        \pgfmathtruncatemacro{\sizem}{\size-1}
        \pgfmathtruncatemacro{\smallbasex}{4}
        \pgfmathtruncatemacro{\smallbasey}{\size/2}
        \pgfmathtruncatemacro{\smallstopx}{\smallbasex+\width}
        \pgfmathtruncatemacro{\smallstopy}{\smallbasey+1}
        \foreach \i in {0,\sizem}{
	    \pgfmathtruncatemacro{\j}{\i+1}
	    \draw (0, \i) -- (0, \j);
	    \draw (\width, \i) -- (\width, \j);
	    \draw[dotted] (0, \i) -- (\width, \i);
	    \draw[dotted] (0, \j) -- (\width, \j);
	}
	\draw[dashed] (0, 1) -- (0, \sizem);
	\draw[dashed] (\width, 1) -- (\width, \sizem);
	\draw (0, 0)     -- (\width, 0);
	\draw (0, \size) -- (\width, \size);
        \draw (\smallbasex,\smallbasey) -- (\smallstopx,\smallbasey) -- (\smallstopx,\smallstopy) -- (\smallbasex,\smallstopy) -- cycle;
        \foreach \i in {0,\sizem}{
	    \pgfmathtruncatemacro{\j}{\i+1}
	    \draw[dotted] (\width, \i) -- (\smallbasex, \smallbasey);
	    \draw[dotted] (\width, \j) -- (\smallbasex, \smallstopy);
	    \pgfmathsetmacro{\xleft}{\width}
	    \pgfmathsetmacro{\xright}{\smallbasex}%{\width/2.0+\smallbasex/2.0}
	    \pgfmathsetmacro{\yleft}{\i + 0.5}
	    \pgfmathsetmacro{\yright}{\smallbasey + 0.5}
	    \path [draw half paths={solid, -latex}{draw=none}]  (\xleft, \yleft) -- (\xright, \yright);
	}
	\draw[decorate,line width=1pt,decoration={brace,raise=0.2cm}] (0, 0) -- (0, \size) node [pos=0.5, xshift=-1cm] {virtual};
	\draw[decorate,line width=1pt,decoration={brace,mirror,raise=0.2cm}] (\smallstopx, \smallbasey) -- (\smallstopx, \smallstopy) node [pos=0.5, xshift=1.2cm] {physical};
      \end{tikzpicture}
      \caption{\label{fig:global_shared_malloc}SMPI shared malloc mechanism: large area of virtual memory are cyclically mapped onto the same physical page.}\vspace{-1em}
    \end{figure}
    #+END_EXPORT
* Improving SMPI Emulation Mechanisms and Preparing HPL
#+LaTeX: \label{sec:em}

In this section, we present our changes to SimGrid and HPL that were
required for a scalable and faithful simulation of HPL. We provide,
space permitted, a brief evaluation for our modifications but refer the interested
reader to\cite{cornebize:hal-01544827} and to our laboratory 
#+LaTeX: notebook\footnote{See \texttt{journal.org} at \url{https://github.com/Ezibenroc/simulating_mpi_applications_at_scale/}},
where each modification has been investigated in more detail.
The experiments in this section were performed on nodes of the
Nova cluster from the Grid'5000 testbed\cite{grid5000}. On these
nodes, a Debian Stretch image (kernel 4.9) runs on top of 
\SI{32}{\giga\byte} RAM of memory and two 8-core Intel Xeon E5-2620 v4
CPUs processors running at \SI{2.1}{\GHz}. Only a single core was used for each
experiment.

** Kernel modeling
#+BEGIN_EXPORT latex
\begin{figure}%[!htb]
%  \null\vspace{-1cm}
  \centering
  \subfigure[Non-intrusive macro replacement.\label{fig:macro_simple}]{
    \begin{minipage}[b]{\linewidth}
      \lstset{frame=bt,language=C,numbers=none,escapechar=|}\lstinputlisting{HPL_dtrsm_macro_simple.c}
    \end{minipage}}
  \subfigure[Gain in term of simulation time.\label{fig:kernel_sampling}]{
    \begin{minipage}[b]{\linewidth} 
      \includegraphics[width=\linewidth,page=2]{figures/validation_kernel_modeling.pdf}
    \end{minipage}}
  \caption{Replacing the calls to computationally expensive functions by a model allows to significantly reduce simulation time.}\vspace{-1em}
\end{figure}
#+END_EXPORT

       As explained in Section\ref{sec:con:diff}, faithful prediction
       of HPL necessitates emulation, \ie to execute the code. For its
       computations, HPL relies heavily on BLAS kernels such as
       =dgemm= (for matrix-matrix multiplication) or =dtrsm= (for solving
       an equation of the form $Ax=b$). An analysis of an HPL
       simulation with 64 processes and a very small matrix of order
       $30,000$ yields that already roughly \SI{96}{\percent} of
       the time is spent in these two very regular kernels. The values computed
       by these functions are of no importance in the control flow of HPL
       and can thus be ignored when simulating.
       However, larger matrices will cause these kernels to consume
       an even larger percentage of the computation time, meaning that
       most of this time can be saved by redirecting calls to
       =dgemm= and =dtrsm= to functions that evaluate a
       performance model for the respective kernel. Figure\ref{fig:macro_simple} depicts this process.
       The macro mechanism allows us to keep HPL code modifications to an absolute
       minimum. The =(9.882e-12)= value represents the inverse of the
       flop rate for this computation kernel and was obtained
       through calibration. The time it takes to execute the real
       kernel with these parameters is then calculated and
       consequently passed on to =smpi_execute_benched=.
       This function effectively advances the clock of the executing
       process by entering a sleep state for the duration it was given
       as a parameter. The gain in simulation time we
       observed for a small scenario is depicted in
       Figure\ref{fig:kernel_sampling}. On the one hand, this modification
       speeds up the simulation by orders of magnitude, as we expected, when matrix
       order increases. On the other hand, this also leads to a more
       optimistic performance estimation. We believe this may not only
       be caused by an inaccuracy in our model but also by a reduction in context-switches, \eg by the operating system.
       #+LATEX: \CH{Re-work this. "Absence of performance variability when kernel models are used."}

*** Hidden section with estimation of the quality/speed of the simulation :noexport:
Inspire from the entry of Tom's Journal "2017-11-15 Wednesday :
Regenerating the validation plot for smpi_execute".

#+begin_src R :results output :session *R* :exports both
library(ggplot2)
library(gridExtra)
library(grid)
old <- read.csv("/home/alegrand/Work/SimGrid/tom/m2_internship_journal/validation/result_size_L0.csv")
new <- read.csv("/home/alegrand/Work/SimGrid/tom/m2_internship_journal/validation/result_size_L1.csv")
old$kernel_sampling = FALSE
new$kernel_sampling =  TRUE
results = rbind(old, new)
generic_do_plot <- function(plot, fixed_shape=TRUE) {
#   For xrange, see https://stackoverflow.com/questions/7705345/how-can-i-extract-plot-axes-ranges-for-a-ggplot2-object
#   old version for xrange (broken)
#   xrange = ggplot_build(plot)$panel$ranges[[1]]$x.range
#   new version for xrange (may break in the next ggplot update...)
    xrange = ggplot_build(plot)$layout$panel_ranges[[1]]$x.range
    xwidth = xrange[2] - xrange[1]
    if(fixed_shape) {
        point = stat_summary(fun.y = mean, geom="point", shape=21)
    }
    else {
        point = stat_summary(fun.y = mean, geom="point")
    }
    return(plot +
        stat_summary(fun.data = mean_se, geom = "errorbar", width=xwidth/20)+
        stat_summary(fun.y = mean, geom="line")+
        point+
        theme_bw()+ scale_color_brewer(palette="Set1") + 
        expand_limits(x=0, y=0))
}

# From https://stackoverflow.com/a/38420690/4110059
grid_arrange_shared_legend <- function(..., nrow = 1, ncol = length(list(...)), position = c("bottom", "top", "right")) {

  plots <- list(...)
  position <- match.arg(position)
  g <- ggplotGrob(plots[[1]] + theme(legend.position = position))$grobs
  legend <- g[[which(sapply(g, function(x) x$name) == "guide-box")]]
  lheight <- sum(legend$height)
  lwidth <- sum(legend$width)
  gl <- lapply(plots, function(x) x + theme(legend.position = "none"))
  gl <- c(gl, nrow = nrow, ncol = ncol)

  combined <- switch(position,
                     "bottom" = arrangeGrob(do.call(arrangeGrob, gl),
                                            legend,
                                            ncol = 1,
                                            heights = unit.c(unit(1, "npc") - lheight, lheight)),
                     "top" = arrangeGrob(legend, do.call(arrangeGrob,gl),
                                            ncol = 1,
                                            heights = unit.c(lheight, unit(1, "npc") - lheight)),
                     "right" = arrangeGrob(do.call(arrangeGrob, gl),
                                           legend,
                                           ncol = 2,
                                           widths = unit.c(unit(1, "npc") - lwidth, lwidth)))
  grid.newpage()
  grid.draw(combined)

}
#+end_src

#+RESULTS:

#+begin_src R :file figures/validation_kernel_modeling.pdf :results value graphics :results output :session *R* :exports both :width 6.2 :height 2.5
plot1 = generic_do_plot(ggplot(results, aes(x=size, y=Gflops, color=kernel_sampling, linetype=kernel_sampling))) +
    labs(colour="Kernel modeling") +
    labs(linetype="Kernel modeling") +
    xlab('Matrix rank') +
    ylab('Performance [Gflop/s]') +
    ggtitle("Performance estimation\n(P=Q=8, i.e., 64 MPI process)")
plot2 = generic_do_plot(ggplot(results, aes(x=size, y=simulation_time, color=kernel_sampling, linetype=kernel_sampling))) +
    labs(colour="Kernel modeling") +
    labs(linetype="Kernel modeling") +
    xlab('Matrix rank') +
    ylab('Time [seconds]') +
    ggtitle("Simulation time\n(P=Q=8, i.e., 64 MPI process)")

grid_arrange_shared_legend(plot2, plot1, ncol=2, position="top")
#+end_src

#+RESULTS:
[[file:figures/validation_kernel_modeling.pdf]]



*** Hidden section with macro code                               :noexport:
#+BEGIN_SRC C :exports none :tangle HPL_dtrsm_macro_real.c
#define |\color{colorfuncall}HPL\_dtrsm|(layout, Side, Uplo, TransA, Diag, M, N, alpha, A, lda, B, ldb) ({ \
    double expected_time;                                                           \
    double coefficient, intercept;                                                  \
    if((M) > 512 && (N) > 512) {                                                    \
        coefficient = (double)SMPI_DTRSM_PHI_COEFFICIENT;                           \
        intercept = (double)SMPI_DTRSM_PHI_INTERCEPT;                               \
    } else {                                                                        \
        coefficient = (double)SMPI_DTRSM_CPU_COEFFICIENT;                           \
        intercept = (double)SMPI_DTRSM_CPU_INTERCEPT;                               \
    }                                                                               \
    if((Side) == HplLeft) {                                                         \
        expected_time = coefficient*((double)(M))*((double)(M))*((double)(N));      \
    } else {                                                                        \
        expected_time = coefficient*((double)(M))*((double)(N))*((double)(N));      \
    }                                                                               \
    expected_time += intercept                                                      \
    if(expected_time > 0)                                                           \
        |\color{colorfuncall}smpi\_execute\_benched|(expected_time);                                        \
})
#+END_SRC

#+BEGIN_SRC C :exports none :tangle HPL_dtrsm_macro_simple_old.c
#define |\color{colorfuncall}HPL\_dtrsm|(layout, Side, Uplo, TransA, Diag, M, N, alpha, A, lda, B, ldb) ({      \
    double expected_time = (9.882e-12)*((double)M)*((double)M)*((double)N) + 4.329e-02;   \
    if(expected_time > 0)                                                                 \
        |\color{colorfuncall}smpi\_execute\_benched|(expected_time);                      \
})
#+END_SRC

#+BEGIN_SRC C :exports none :tangle HPL_dtrsm_macro_simple.c
#define |\color{colorfuncall}HPL\_dtrsm|(layout, Side, Uplo, TransA, Diag,      \ 
        M, N, alpha, A, lda, B, ldb) ({                  \
    double expected_time = (9.882e-12)*((double)M)*      \
                   ((double)M)*((double)N) + 4.329e-02;  \
    if(expected_time > 0)                                \
        |\color{colorfuncall}smpi\_execute\_benched|(expected_time);             \
})
#+END_SRC

#+BEGIN_EXPORT latex
\CH{Found this in Tom's logbook. Check if this is the final version. Also, we can apparently just call \texttt{make SMPI\_OPTS=-DSMPI\_OPTIMIZATION} (what about \texttt{arch=SMPI}?). See his logbook}
#+END_EXPORT
** Adjusting the behavior of HPL
#+LaTeX: \label{sec:hplchanges}

#+BEGIN_EXPORT latex
\tikzstyle{switch}=[draw, circle, minimum width=1cm, minimum height = 1cm]
\tikzstyle{compute}=[draw, rectangle, minimum width=0.5cm, minimum height = 0.5cm, node distance=0.5cm]
\tikzstyle{base}=[ellipse, minimum width=2cm, minimum height = 0.5cm, node distance = 0.5cm]
\tikzstyle{bigswitch}=[base, draw]
\begin{figure}%[htbp]
  \centering
  {\begin{minipage}{1.0\linewidth}
  \subfigure[Structure of the panel in HPL.\label{fig:panel_structure}]{\small
    \begin{minipage}[b]{\linewidth}\centering
      \begin{tikzpicture}[scale=0.8]
        \draw [fill=gray] (3, 2) -- (6, 2) -- (6, 3) -- (3, 3) -- cycle;
        \draw (0, 2) -- (9, 2) -- (9, 3) -- (0, 3) -- cycle;
        \draw[dashed] (3, 2) -- (3, 3);
        \draw[dashed] (6, 2) -- (6, 3);
        \node(1) at (1.5, 2.5) {matrix parts};
        \node(2) at (4.5, 2.5) {indices};
        \node(3) at (7.5, 2.5) {matrix parts};
        \draw[decorate,line width=1pt,decoration={brace,raise=0.2cm}] (0, 3) -- (3, 3) node [pos=0.5, yshift=0.5cm] {can be shared};
        \draw[decorate,line width=1pt,decoration={brace,raise=0.2cm}] (6, 3) -- (9, 3) node [pos=0.5, yshift=0.5cm] {can be shared};
        \draw[decorate,line width=1pt,decoration={brace,raise=0.2cm, mirror}] (3, 2) -- (6, 2) node [pos=0.5, yshift=-0.5cm] {must not be shared};
      \end{tikzpicture}
    \end{minipage}}
  \subfigure[Reusing panel allocation from an iteration to another.\label{fig:panel_reuse}]{\small
    \begin{minipage}[b]{\linewidth}\centering
      \begin{tikzpicture}[yscale=.6]
        \draw [fill=gray] (2, 1) -- (4, 1) -- (4, 1.5) -- (2, 1.5) --cycle;
        \draw (0, 1) -- (6, 1) -- (6, 1.5) -- (0, 1.5) -- cycle;
        \draw[dashed] (2, 1) -- (2, 1.5);
        \draw[dashed] (4, 1) -- (4, 1.5);

        \draw [fill=gray] (2, 0) -- (3, 0) -- (3, .5) -- (2, .5) --cycle;
        \draw (1, 0) -- (4, 0) -- (4, .5) -- (1, .5) -- cycle;
        \draw[dashed] (2, 0) -- (2, .5);
        \draw[dashed] (3, 0) -- (3, .5);

        \draw[-latex] (2, 1) -- (2, .5);
        \draw[decorate,line width=1pt,decoration={brace,raise=0.2cm}] (0, 1.5) -- (6, 1.5) node [pos=0.5, yshift=0.5cm] {initial buffer};
        \draw[decorate,line width=1pt,decoration={brace,raise=0.2cm, mirror}] (1, 0) -- (4, 0) node [pos=0.5, yshift=-0.5cm] {current buffer};
      \end{tikzpicture}
    \end{minipage}
  }    
  \end{minipage}}
  \caption{Panel structure and allocation strategy when simulating.\label{fig:panel}}\vspace{-1em}
\end{figure}
#+END_EXPORT

HPL is generally configured to use huge pseudo-randomly generated
matrices that require to be setup every time HPL is executed. HPL
excludes the time spent for setting up the matrices and for the 
validation of the computed result in the reported \si{\giga\flops} performance. 
Said verification is also rendered meaningless as we skipped all the
computations and replaced them by a kernel model. Since both 
phases do not have an impact on the performance of the platform, we can safely
skip them.

The lion's share of the computation time was consumed by calls to
=dgemm= and =dtrsm=. We identified in total seven other functions through
profiling as computationally expensive enough to justify a specific
handling: The BLAS functions =dgemv=, =dswap=, =daxpy=,
=dscal=, =dtrsv=, =dger=, and =idamax=. All of these functions are called during the
LU factorization and hence impact the performance measured by HPL; however, because of
the removal of the =dgemm= and =dtrsm= computations they all operate (\eg copy) on
bogus data and hence also produce bogus data. We also determined that their
impact on the performance prediction was minimal and that 
modeling them precisely would not pay off. We hence modeled them
as being instantaneous.

Note that working on fake data has a few consequences as HPL
implements an LU factorization with partial pivoting and a special
treatment of the =idamax= function that returns the index of the first
element equaling the maximum absolute value. The cost of this function was
ignored as well but its return value was arbitrarily set to make the
simulation fully deterministic. In all our evaluations, this
modification was harmless in term of performance prediction while it
allows to additionally speed-up the simulation by a factor of $\approx3$ to $4$
on small scenarios ($N=30,000$) and significantly more on larger setups.
** Memory folding
#+LATEX: \TC{We also removed five HPL functions, e.g. HPL\_dlacpy. Should we mention this?}
As explained in Section\ref{sec:smpi}, when emulating an application
with SMPI, all MPI processes are run within the same simulation process on a single
node. The memory consumption of the simulation can therefore quickly reach
several \si{\tera\byte} of RAM. 

Yet, as we no longer operate on real data, storing the whole
input matrix $A$ is needless. Since only a minimal portion of the code was
modified, however, some functions may still read or write some parts of the matrix.
It is thus not possible to simply remove the memory allocations of
large data structures. Instead, SMPI's =SHARED_MALLOC= mechanism can be used
to share unimportant data structures between all MPI
processes, minimizing the memory footprint.

The largest two allocated data structures in HPL are the input matrix =A=
(with a size of typically several \si{\giga\byte} per process) and the =panel= which contains much
information about the sub-matrix currently being factorized. This sub-matrix 
typically occupies a few hundred \si{\mega\byte} per process.

Although using the default =SHARED_MALLOC= mechanism works flawlessly
with =A=, a more careful strategy needs to be used for the
=panel=. Indeed, =panel= is an intricate data structure with both \texttt{int}s
(accounting for matrix indices, error codes, MPI tags, and pivoting information)
and \texttt{double}s (corresponding to a copy of sub-matrices of =A=). To
optimize data transfers, HPL flattens this structure into a single
allocation of \texttt{double}s (see
Figure\ref{fig:panel_structure}). Using a fully shared memory
allocation for the =panel= therefore leads to index corruption that results in
classical invalid memory accesses as well as communication
deadlocks, as processes may not send to or receive from the correct
process. Since \texttt{int}s and \texttt{double}s are stored in
non-contiguous parts of this flat allocation, it is therefore
essential to have a mechanism that preserves the process-specific
content. We have thus introduced a new
=SMPI_PARTIAL_SHARED_MALLOC= that works as follows: 
~mem = SMPI_PARTIAL_SHARED_MALLOC(500, {27,42 , 100,200}, 2)~.
In this example, 500 bytes are allocated in =mem= with the elements
=mem[27]=, ..., =mem[41]= and =mem[100]=, ..., =mem[199]= being shared between
processes (they are therefore generally completely corrupted) while all other
elements remain private. We applied this mechanism to HPL by modifying
only a few lines and so, panels are now shared
between processes with the part in the middle storing the indices being private to
each MPI process. 
#+LATEX: \TC{Can you please verify the preceding paragraph?}

Designating memory explicitly as private, shared or partially shared
does not only help with memory management but also with
performance. As SMPI is internally aware of the memory's
visibility, it can avoid calling =memcopy= when large messages
containing shared segments are sent from one MPI rank to another.
For fully private data *segments* or partially shared segments, SMPI
identifies and only copies those parts that are designated as private
(as they are process-dependent) into the corresponding private buffers
on the receiver side.

HPL simulation times are considerably improved by this, as
the main datastructure that is being communicated between ranks, the
=panel=, is partially shared with only the smaller part being private.
The error introduced by these new allocations, compared to
the version from Section *REFERENCE*, is negligible (below 1%) while we gain
a drastic improvement of the memory consumption. For instance, for a
matrix of order $40,000$ and $64$ MPI processes, the memory consumption
decreases from about \SI{13.5}{\giga\byte} to less than
\SI{40}{\mega\byte}.
** Panel reuse
The original HPL code \texttt{malloc}s/\texttt{free}s panels in each
iteration, with the size of the panel strictly decreasing from
iteration to iteration. As we explained above, the partial sharing of panels requires
some extra effort and introduces an overhead that makes these repeated
allocations / frees become a bottleneck. Since
the very first allocation can fit all subsequent panels, we modified
HPL to only allocate the very first panel and reuse it from one
iteration to another (see Figure\ref{fig:panel_reuse}).

We consider this optimization harmless with respect to simulation
accuracy as the maximum error that we observed, compared to the previous
version, was always less than \SI{1}{\percent}. The gain in terms of simulation
time is significant, albeit less impressive than for previous
optimizations: For a very small matrix of order $40,000$ and $64$ MPI processes,
the simulation time decreases by four seconds, from \SI{20.5}{\sec} to
\SI{16.5}{\sec}, thanks to a reduction of time consumed by the system,
from \SI{5.9}{\sec} to \SI{1.7}{\sec}. The number of page faults decreased from $2$ million to
$0.2$ million, confirming the dramatic effect this
series of allocation/deallocation would have at scale.
** TODO MPI process representation (mmap vs. dlopen)
SimGrid folds parallel applications into a single process and hence,
local static and global variables become an issue as it must be guaranteed that
each rank has its own set of global variables. SMPI supports two
mechanisms to achieve this: The usage of either =mmap= or =dlopen=.
\TC{The implementation of mmap and dlopen is not a contribution of this paper, maybe we should put this in the state of the art and only discuss here the impact of the choice?}
*** mmap
When =mmap= is used, SMPI copies the =data= segment on startup for each
rank into the heap. When control is transferred from one rank to
another, the =data= segment is =mmap='ed to the location of this rank's
copy on the heap. All ranks have hence the same addresses in the
virtual address space at their disposition although they point to
different physical addresses based on the rank. This also means
inevitably that caches must be flushed to ensure that no data of one
rank leaks into the other rank. This overhead makes the usage of =mmap=
a rather expensive operation.

# \TOM{Can you tell me how often these operations were executed, as you've already done in your journal on 2017-04-11 ("Looking at the syscalls")?}
*** dlopen
With =dlopen=, copies of the global variables are still made but they
are stored inside the =data= segment as opposed to the heap. When
switching from one rank to another, the starting virtual address for
the storage is readjusted rather than the addresses point
to. This means that each rank has its own unique pool of addresses for
global variables. The main advantage of this approach is that caches do not need to
be flushed as in the case for the =mmap= approach, because data
consistency can always be guaranteed.

*** Impact of choice of mmap/dlopen
The choice of mmap or dlopen influences the simulation time indirectly
through its direct impact on system/user time and page faults.  As an
example, for a matrix of order 80,000 and 32 MPI processes, the number
of minor page faults drops from \num{4412047} (with =mmap=) to
\num{6880} (with =dlopen=). This results in a drop in system time from 
\SI{10.64}{\sec} (out of \SI{51.47}{\sec} in total) to
\SI{2.12}{\sec}. Obviously, the larger the matrix and the number of
process, the larger the number of context switch during the
simulation, and the higher the gain.

# See Tom's journal (Performance evaluation of the privatization
# mechanism: =dlopen= vs =mmap= ) ; there are some graphs that we might be
# able to use, such as in
# https://github.com/Ezibenroc/m2_internship_journal/blob/master/simgrid_privatization/

** Huge pages    
For larger matrix orders (\ie with $N$ larger than a few hundreds of
thousands), the performance of the simulation quickly
deteriorates as the memory consumption rises rapidly and the CPU
utilization drops unless . Running the simulation while monitoring the system
shows that the program is regularly stalled while the kernel loads the
CPU at \SI{100}{\percent}, which explains the low CPU utilization for the program
itself.
\TC{This is false, see the entry from 2017-07-22 in my journal. If we use a tempfs file system, the CPU load remains high, we only have a large memory consumption.}

As aforementioned, we fold the memory in order to reduce the /physical/
memory usage. The /virtual/ memory, on the other hand, is still
allocated for every process *since the allocation calls are still executed*.
Without a reduction of allocated virtual addresses, the page table
rapidly becomes too large to be efficiently maintained. More
precisely, the size of the page table containing pages of size \SI{4}{\kibi\byte} can be computed as:

    #+LATEX: \[ PT_{size}(N) = \frac{N^2 \cdot \texttt{sizeof(double)}}{4,096} \cdot \texttt{sizeof(pointer)} \]

This means that only to store the addresses for a matrix of order $N=4,000,000$, the page table
grows to $PT_{size}(4,000,000) = \num{2.5e11}$ bytes, \ie
\SI{250}{\gibi\byte}. Thankfully, the x86-64 architecture supports several page
sizes, known as ''huge pages`` *in* Linux. Typically, these pages are
around \SI{2}{\mebi\byte} (instead of \SI{4}{\kibi\byte}), although other sizes
(\SIrange{2}{256}{\mebi\byte}) are possible as well.

Unfortunately, changing the page size requires administrator (root) privileges as the
Linux kernel support for /hugepages/ needs to be activated and a
=hugetlbfs= file system must be mounted. After at least one huge
page was allocated, the path of the allocated file system can then be
passed on to SimGrid that will then pass the flag =MAP_HUGETLB=
to =mmap= in =SMPI_SHARED_MALLOC= and replace the file given to =mmap= by
a file opened in the =hugetlbfs= file system.

Setting the page size to \SI{2}{\mebi\byte} has two advantages:
Firstly, the page table size decreases drastically. For example, for a
matrix of order $N=4,000,000$, it shrinks from \SI{250}{\gibi\byte} to 
\SI{0.488}{\gibi\byte}. Secondly, simulation time decreases drastically since using hugepages
reduces the amount of page faults and time spent on page table
management by the kernel. For a matrix of order $300,000$ and $64$ MPI
processes, we saw CPU utilization rise from \SI{66}{\percent} to
\SI{99}{\percent} and a reduction of simulation time from about
\SI{580}{\sec} to roughly \SI{175}{\sec}.
# Values from Figure 4.9 in Tom's report
* Scalability Evaluation
#+LaTeX: \label{sec:scalabilityevol}

#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth,page=2]{./figures/scalability_plot_size.pdf}
%  \includegraphics[width=\linewidth,page=2]{./figures/scalability_plot_nbproc.pdf}
  \caption{Time complexity and memory consumption are linear in the number of processes but remain mildly quadratic with matrix rank.}\vspace{-1em}
  \label{fig:hpl_scalability}
  \labspace
\end{figure}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{figure*}%[!htb]
  \centering
  \begin{minipage}[b]{.27\textwidth}
    \includegraphics[width=\linewidth,page=2]{./figures/stampede_knc_model.pdf}
    \vspace{-2em}
    \caption{Automatic offloading on the KNC depends on matrix dimensions.}
    \vspace{-1em}
    \label{fig:hpl_mkl}
  \end{minipage}~~~\begin{minipage}[b]{.7\textwidth}\centering
    \scalebox{.88}{\begin{tabular}{l|r|r|r|r}
    & \multicolumn{2}{c|}{CPU (\texttt{CPU})} & \multicolumn{2}{c}{KNC (\texttt{PHI}) }\\
    & Coefficient $[\si{\sec\per\flop}]$& Intercept $[\sec]$ & Coefficient $[\si{\sec\per\flop}]$& Intercept $[\sec]$ \\
    \hline
    \texttt{DGEMM} & \num{1.029e-11} & \num{2.737e-02} & \num{1.981e-12} & \num{6.316e-01} \\
    \texttt{DTRSM} & \num{9.882e-12} & \num{4.329e-02} & \num{1.954e-12} & \num{5.222e-01}
    \end{tabular}}\medskip\\
    \lstset{frame=bt,language=C,numbers=none,escapechar=|}\lstinputlisting{HPL_dtrsm_macro_real.c}
    \caption{Modeling automatic offloading on KNC in MKL BLAS kernels.}
    \vspace{-1em}
    \label{fig:macro_real}
  \end{minipage}
\end{figure*}
#+END_EXPORT
#+BEGIN_EXPORT latex
\TC{The KNC plots are wrong. They have an intercept of more than 2 whereas in reality it is about 0.5. I think they have been made with the old results, when no warm-up was done. See the files:
| Function | Mode | File                                    |
|----------+------+-----------------------------------------|
| dgemm    | KNC  | stampede/calibration/6/large\_dgemm.csv  |
| dgemm    | CPU  | stampede/calibration/7/dgemm\_module.csv |
| dtrsm    | KNC  | stampede/calibration/6/large\_dtrsm.csv  |
| dtrsm    | CPU  | stampede/calibration/7/dtrsm\_module.csv |
}
#+END_EXPORT

# SMPI_DGEMM_COEFFICIENT=1.029e-11 SMPI_DGEMM_INTERCEPT=2.737e-02 SMPI_DGEMM_PHI_COEFFICIENT=1.981e-12 SMPI_DGEMM_PHI_INTERCEPT=6.316e-01 \
# SMPI_DTRSM_COEFFICIENT=9.882e-12 SMPI_DTRSM_INTERCEPT=4.329e-02 SMPI_DTRSM_PHI_COEFFICIENT=1.954e-12 SMPI_DTRSM_PHI_INTERCEPT=5.222e-01"

In Section\ref{sec:em} we explained the problems we encountered when trying
large-scale simulation on a single node and how we solved them. 
Most of this consisted of identifying and eliminating bottlenecks one after the
other while simultaneously making sure that the accuracy of our performance prediction was
not impacted. Clearly, the main goal was to reduce the
*complexity* of simulating HPL from $\O(N^3) + \O(N^2\cdot{}P\cdot{}Q)$ to a more
reasonable *complexity*. The $\O(N^3)$ was removed through evading most
of the computations. Ideally, since there are $N/NB$ steps, 
the complexity would be decreased to something independent of
$N$. SimGrid's fluid models, used to simulate communications, do not
depend on $N$. Therefore, the time to simulate a step of HPL should mostly depend on $P$ and
$Q$. Yet, some memory operations on the panel that are related to pivoting
are intertwined in HPL with collective communications, meaning that it
is impossible to completely get rid of the $\O(N)$ complexity without
modifying HPL more profoundly.

Although our goal is to model and simulate HPL on the Stampede
platform, we decided to try a first evaluation on a
similar, albeit non-existing, platform comprising 4,096 8-core nodes
interconnected through a $\langle2;16,32;1,16;1,1;8\rangle$ fat-tree topology
built on ideal network links with a bandwidth of
\SI{50}{\giga\byte\per\sec} and a latency of \SI{5}{\micro\sec}. We ran
simulations with $512$; $1,024$; $2,048$ or $4,096$ MPI processes and
with matrices of orders \num{5e5}, \num{1e6}, \num{2e6} or \num{4e6}.
The impact of the matrix order on total makespan and memory is illustrated in Figure\ref{fig:hpl_scalability}. 
With all previously described
optimizations enabled, the largest simulation took close to $47$ hours and consumed
\SI{16}{\gibi\byte} of memory whereas the smallest one took $20$ minutes and \SI{282}{\mibi\byte} of memory.
One can also see that, when the matrix order ($N$) is increased, memory consumption and
simulation time both *grow* slightly quadratic as the amount of matrix
elements is $N^{2}$ and the number of steps *(iterations?)* of the algorithm also *grows* linearly.

Moreover, all the simulations spend less than \SI{10}{\percent} of their execution time in kernel
mode, which means the number of system calls is reasonably low.
** Hidden section                                                 :noexport:
Got data and code from the "2017-06-05 Monday: Plots for scalability
test" section of Tom's journal:

#+begin_src R :results output :session *R* :exports both
library(ggplot2)
library(ggrepel)
library(reshape2)
library(gridExtra)
results = rbind(
    read.csv('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/scalability/result_500000_512.csv'),
    read.csv('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/scalability/result_500000_1024.csv'),
    read.csv('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/scalability/result_500000_2048.csv'),
    read.csv('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/scalability/result_500000_4096.csv'),
    read.csv('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/scalability/result_1000000_512.csv'),
    read.csv('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/scalability/result_1000000_1024.csv'),
    read.csv('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/scalability/result_1000000_2048.csv'),
    read.csv('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/scalability/result_1000000_4096.csv'),
    read.csv('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/scalability/result_2000000_512.csv'),
    read.csv('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/scalability/result_2000000_1024.csv'),
    read.csv('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/scalability/result_2000000_2048.csv'),
    read.csv('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/scalability/result_2000000_4096.csv'),
    read.csv('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/scalability/result_4000000_512.csv'),
    read.csv('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/scalability/result_4000000_1024.csv'),
    read.csv('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/scalability/result_4000000_2048.csv'),
    read.csv('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/scalability/result_4000000_4096.csv')
)
results$simulation_time = results$simulation_time/3600
results$memory_size = results$memory_size * 1e-9
number_verb <- function(n) {
    return(format(n,big.mark=",",scientific=FALSE))
}
results$size_verb = factor(unlist(lapply(results$size, number_verb)), levels = c('500,000','1,000,000','2,000,000','4,000,000'))
results$nb_proc_verb = factor(unlist(lapply(results$nb_proc, number_verb)), levels = c('512', '1,024', '2,048', '4,096'))
results
#+end_src

#+RESULTS:
#+begin_example
             topology nb_roots nb_proc    size  full_time        time Gflops
1  2;16,32;1,16;1,1;8       16     512  500000    91246.1    91246.02  913.3
2  2;16,32;1,16;1,1;8       16    1024  500000    46990.1    46990.02 1773.0
3  2;16,32;1,16;1,1;8       16    2048  500000    24795.5    24795.50 3361.0
4  2;16,32;1,16;1,1;8       16    4096  500000    13561.0    13561.01 6145.0
5    2;16,32;1,16;1,1       16     512 1000000   716521.0   716521.00  930.4
6    2;16,32;1,16;1,1       16    1024 1000000   363201.0   363201.04 1836.0
7    2;16,32;1,16;1,1       16    2048 1000000   186496.0   186495.70 3575.0
8  2;16,32;1,16;1,1;8       16    4096 1000000    97836.6    97836.54 6814.0
9    2;16,32;1,16;1,1       16     512 2000000  5685080.0  5685077.72  938.1
10   2;16,32;1,16;1,1       16    1024 2000000  2861010.0  2861012.55 1864.0
11   2;16,32;1,16;1,1       16    2048 2000000  1448900.0  1448899.09 3681.0
12 2;16,32;1,16;1,1;8       16    4096 2000000   742691.0   742690.59 7181.0
13 2;16,32;1,16;1,1;8       16     512 4000000 45305100.0 45305083.56  941.8
14 2;16,32;1,16;1,1;8       16    1024 4000000 22723800.0 22723820.45 1878.0
15 2;16,32;1,16;1,1;8       16    2048 4000000 11432900.0 11432938.62 3732.0
16 2;16,32;1,16;1,1;8       16    4096 4000000  5787160.0  5787164.09 7373.0
   simulation_time application_time user_time system_time major_page_fault
1        0.3311083          204.992   1098.25       93.12                0
2        0.6895222          441.897   2296.51      184.70                0
3        1.4144361          872.425   4741.26      349.79                0
4        3.1448889         1947.320  10640.63      679.53                0
5        0.7319722          500.970   2367.19      259.91                0
6        1.6771917         1036.960   5515.36      515.05                0
7        3.4421944         2092.950  11389.36      995.39                0
8        7.2368056         4362.660  24082.38     1966.10                0
9        1.9263500         1169.660   6193.80      683.73                0
10       4.2217500         2551.100  13714.01     1430.93                0
11       8.9621111         5236.560  29357.92     2844.89                0
12      18.0156389        10643.600  59444.40     5402.24                0
13       4.8156944         3030.400  15090.31     1945.23                0
14      10.6613611         6435.870  34249.71     3827.36                0
15      23.2042222        13080.500  75523.95     7684.52                0
16      47.1275000        26745.400 154314.76    15085.08                0
   minor_page_fault cpu_utilization        uss         rss page_table_size
1            960072            0.99  155148288  2055086080        10604000
2           1054062            0.99  369696768  4383203328        21240000
3           1282294            0.99 1012477952  9367576576        42912000
4           1852119            0.99 3103875072 15318568960        87740000
5           1916208            0.99  153665536  2317279232        10600000
6           2002989            0.99  369676288  4837175296        21252000
7           2154982            0.99 1010696192  7774138368        42908000
8           2768705            0.99 3103895552 16934834176        87748000
9           3801905            0.99  150765568  2758770688        10604000
10          3872820            0.99  365555712  5273034752        21220000
11          4038099            0.99 1009606656  7415914496        42884000
12          4704339            0.99 3102445568 19464646656        87748000
13          7663911            0.98  151576576  2056916992        10604000
14          7725625            0.99  369872896  4120702976        21212000
15          7917525            0.99 1012191232  9221050368        42880000
16          8550745            0.99 3113381888 20408209408        87808000
   memory_size size_verb nb_proc_verb
1    0.2825585   500,000          512
2    0.4299489   500,000        1,024
3    0.9628262   500,000        2,048
4    2.8140421   500,000        4,096
5    0.8944435 1,000,000          512
6    1.0553098 1,000,000        1,024
7    1.5811707 1,000,000        2,048
8    3.4254070 1,000,000        4,096
9    3.3384202 2,000,000          512
10   3.4971116 2,000,000        1,024
11   4.0274084 2,000,000        2,048
12   5.9101348 2,000,000        4,096
13  13.0790605 4,000,000          512
14  13.2755579 4,000,000        1,024
15  13.8251837 4,000,000        2,048
16  15.7636690 4,000,000        4,096
#+end_example

#+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  library(gridExtra)
  library(grid)

  generic_do_plot <- function(plot, fixed_shape=TRUE) {
  #   For xrange, see https://stackoverflow.com/questions/7705345/how-can-i-extract-plot-axes-ranges-for-a-ggplot2-object
  #   old version for xrange (broken)
  #   xrange = ggplot_build(plot)$panel$ranges[[1]]$x.range
  #   new version for xrange (may break in the next ggplot update...)
      xrange = ggplot_build(plot)$layout$panel_ranges[[1]]$x.range
      xwidth = xrange[2] - xrange[1]
      if(fixed_shape) {
          point = stat_summary(fun.y = mean, geom="point", shape=21)
      }
      else {
          point = stat_summary(fun.y = mean, geom="point")
      }
      return(plot +
          stat_summary(fun.data = mean_se, geom = "errorbar", width=xwidth/20)+
          stat_summary(fun.y = mean, geom="line")+
          point+
          theme_bw()+
          expand_limits(x=0, y=0))
  }
  do_plot <- function(df, x, y, color, color_title, fixed_val, other_fixed_val=-1) {
      if(y == "simulation_time") {
          y_title = "Simulation time (seconds)"
          title = "Simulation time"
      }
      else if(y == "memory_size") {
          y_title = "Memory consumption (bytes)"
          title = "Memory consumption"
      }
      else {
          stopifnot(y == "Gflops")
          y_title = "Performance estimation (Gflops)"
          title = "Performance estimation"
      }
      if(x == "size") {
          fixed_arg = "nb_proc"
          x_title = "Matrix size"
          title = paste(title, "for different matrix sizes\nUsing", fixed_val, "MPI processes")
      }
      else {
          stopifnot(x == "nb_proc")
          fixed_arg = "size"
          x_title = "Number of processes"
          title = paste(title, "for different number of processes\nUsing a matrix size of", format(fixed_val,big.mark=",",scientific=FALSE))
      }
      sub_df = df[df[fixed_arg] == fixed_val,]
      p = generic_do_plot(ggplot(sub_df, aes_string(x=x, y=y, linetype=color, color=color, group=color))) +
          ggtitle(title)+
          xlab(x_title)+
          ylab(y_title)+
          labs(colour=color_title)+
          labs(linetype=color_title)
      if(other_fixed_val != -1) {
          rect <- data.frame(xmin=-Inf, xmax=Inf, ymin=-Inf, ymax=Inf)
          my_xmin = other_fixed_val * 0.9
          my_xmax = other_fixed_val * 1.1
          my_ymax = max(sub_df[sub_df[x] == other_fixed_val,][y])
          y_delta = my_ymax * 0.1
          my_ymax = my_ymax + y_delta
          my_ymin = min(sub_df[sub_df[x] == other_fixed_val,][y]) - y_delta
          p = p + geom_rect(data=rect, aes(xmin=my_xmin, xmax=my_xmax, ymin=my_ymin, ymax=my_ymax),color="grey20", alpha=0.1, inherit.aes=FALSE)
      }
      return(p)
  }

  # From https://stackoverflow.com/a/38420690/4110059
  grid_arrange_shared_legend <- function(..., nrow = 1, ncol = length(list(...)), position = c("bottom", "right")) {

    plots <- list(...)
    position <- match.arg(position)
    g <- ggplotGrob(plots[[1]] + theme(legend.position = position))$grobs
    legend <- g[[which(sapply(g, function(x) x$name) == "guide-box")]]
    lheight <- sum(legend$height)
    lwidth <- sum(legend$width)
    gl <- lapply(plots, function(x) x + theme(legend.position = "none"))
    gl <- c(gl, nrow = nrow, ncol = ncol)

    combined <- switch(position,
                       "bottom" = arrangeGrob(do.call(arrangeGrob, gl),
                                              legend,
                                              ncol = 1,
                                              heights = unit.c(unit(1, "npc") - lheight, lheight)),
                       "right" = arrangeGrob(do.call(arrangeGrob, gl),
                                             legend,
                                             ncol = 2,
                                             widths = unit.c(unit(1, "npc") - lwidth, lwidth)))
    grid.newpage()
    grid.draw(combined)

  }

  do_multiple_plot <- function(df, x1, x2, y, color, color_title, fixed_x1, fixed_x2) {
      my_ymax = max(df[y])
      return(
          grid_arrange_shared_legend(
              do_plot(df, x1, y, color, color_title, fixed_x1, fixed_x2) + expand_limits(x=0, y=my_ymax),
              do_plot(df, x2, y, color, color_title, fixed_x2, fixed_x1) + expand_limits(x=0, y=my_ymax),
              nrow=1, ncol=2
          ))
  }

  do_four_plot <- function(df, x1, x2, y1, y2, color, color_title, fixed_x1, fixed_x2) {
      my_y1max = max(df[y1])
      my_y2max = max(df[y2])
      return(
          grid_arrange_shared_legend(
              do_plot(df, x1, y1, color, color_title, fixed_x1, fixed_x2) + expand_limits(x=0, y=my_y1max),
              do_plot(df, x2, y1, color, color_title, fixed_x2, fixed_x1) + expand_limits(x=0, y=my_y1max),
              do_plot(df, x1, y2, color, color_title, fixed_x1, fixed_x2) + expand_limits(x=0, y=my_y2max),
              do_plot(df, x2, y2, color, color_title, fixed_x2, fixed_x1) + expand_limits(x=0, y=my_y2max),
              nrow=2, ncol=2
          ))
  }
#+end_src

#+RESULTS:

#+begin_src R :file figures/scalability_2.pdf :results value graphics :results output :session *R* :exports both :width 4 :height 2.5
nbproc_time = generic_do_plot(ggplot(results, aes(x=nb_proc, y=simulation_time, color=size_verb))) +
    xlab("Number of processes") +
    ylab("Simulation time (hours)") +
    labs(colour="Matrix size")+
    ggtitle("Simulation time for different number of processes")+
    theme(legend.position = "none")+
    geom_text_repel(
        data = subset(results, nb_proc == max(nb_proc)),
        aes(label = size_verb),
        nudge_x = 45,
        segment.color = NA,
        show.legend = FALSE
      )
nbproc_time
#+end_src

#+RESULTS:
[[file:figures/scalability_2.pdf]]

#+begin_src R :file figures/scalability_4.pdf :results value graphics :results output :session *R* :exports both :width 4 :height 2.5
nbproc_mem = generic_do_plot(ggplot(results, aes(x=nb_proc, y=memory_size, color=size_verb))) +
    xlab("Number of processes") +
    ylab("Memory consumption (gigabytes)") +
    labs(colour="Matrix size")+
    ggtitle("Memory consumption for different number of processes")+
    theme(legend.position = "none")+
    geom_text_repel(
        data = subset(results, nb_proc == max(nb_proc)),
        aes(label = size_verb),
        nudge_x = 45,
        segment.color = NA,
        show.legend = FALSE
    )
nbproc_mem
#+end_src

#+RESULTS:
[[file:figures/scalability_4.pdf]]


#+begin_src R :file figures/scalability_1.pdf :results value graphics :results output :session *R* :exports both :width 4 :height 2.5
size_time = generic_do_plot(ggplot(results, aes(x=size, y=simulation_time, color=nb_proc_verb))) +
    xlab("Matrix rank") +
    ylab("Simulation time (hours)") +
    labs(colour="Number of processes")+ scale_color_brewer(palette="Set1")+
#    ggtitle("Simulation time for different matrix sizes")+
    theme(legend.position = "none")+
    geom_text_repel(
        data = subset(results, size == max(size)),
        aes(label = nb_proc_verb),
        nudge_x = 45,
        segment.color = NA,
        show.legend = FALSE
      )
size_time
#+end_src

#+RESULTS:
[[file:figures/scalability_1.pdf]]

#+begin_src R :file figures/scalability_3.pdf :results value graphics :results output :session *R* :exports both :width 4 :height 2.5
size_mem = generic_do_plot(ggplot(results, aes(x=size, y=memory_size, color=nb_proc_verb))) +
    xlab("Matrix rank") +
    ylab("Memory consumption (gigabytes)") +
    labs(colour="Number of processes")+
#    ggtitle("Memory consumption for different matrix sizes")+
    theme(legend.position = "none")+scale_color_brewer(palette="Set1")+
    geom_text_repel(
        data = subset(results, size == max(size)),
        aes(label = nb_proc_verb),
        nudge_x = 45,
        segment.color = NA,
        show.legend = FALSE
      )
size_mem
#+end_src

#+RESULTS:
[[file:figures/scalability_3.pdf]]

#+begin_src R :file figures/scalability_plot_size.pdf :results value graphics :results output :session *R* :exports both :width 7 :height 3.5
grid_arrange_shared_legend(size_time, size_mem, nrow=1, ncol=2)
#+end_src

#+RESULTS:
[[file:figures/scalability_plot_size.pdf]]

#+begin_src R :file figures/scalability_plot_nbproc.pdf :results value graphics :results output :session *R* :exports both :width 8 :height 3.5
grid_arrange_shared_legend(nbproc_time, nbproc_mem, nrow=1, ncol=2)
#+end_src

#+RESULTS:
[[file:figures/scalability_plot_nbproc.pdf]]


#+begin_src R :results output :session *R* :exports both
fit_sim = lm(data=results, simulation_time ~ nb_proc*(size+I(size^2)))
summary(fit_sim)
#+end_src

#+RESULTS:
#+begin_example

Call:
lm(formula = simulation_time ~ nb_proc * (size + I(size^2)), 
    data = results)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.192256 -0.050079 -0.004809  0.045721  0.231054 

Coefficients:
                    Estimate Std. Error t value Pr(>|t|)    
(Intercept)       -1.522e-01  1.866e-01  -0.815   0.4339    
nb_proc           -1.162e-04  7.907e-05  -1.469   0.1725    
size               6.919e-08  2.214e-07   0.313   0.7610    
I(size^2)         -8.691e-14  4.689e-14  -1.853   0.0935 .  
nb_proc:size       1.608e-09  9.379e-11  17.142 9.64e-09 ***
nb_proc:I(size^2)  3.450e-16  1.987e-17  17.366 8.49e-09 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1343 on 10 degrees of freedom
Multiple R-squared:  0.9999,	Adjusted R-squared:  0.9999 
F-statistic: 2.46e+04 on 5 and 10 DF,  p-value: < 2.2e-16
#+end_example

#+begin_src R :results output :session *R* :exports both
grid.lines = 26
x.pred <- seq(min(results$nb_proc), max(results$nb_proc), length.out = grid.lines)
y.pred <- seq(min(results$size), max(results$size), length.out = grid.lines)
xy <- expand.grid( nb_proc = x.pred, size = y.pred)
z.pred <- matrix(predict(fit_sim, newdata = xy), 
                 nrow = grid.lines, ncol = grid.lines)
# fitted points for droplines to surface
fitpoints <- predict(fit_sim)
#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R* 
library("plot3D")
scatter3D(
   results$nb_proc, results$size, results$simulation_time, ticktype = "detailed", phi = 20, theta = -50, bty ="g",
    pch = 20, cex = 2, type="l", r=10,
    surf = list(x = x.pred, y = y.pred, z = z.pred,  
    facets = NA, fit = fitpoints),colvar=NULL)
#+end_src

#+RESULTS:
[[file:/tmp/babel-23284Iao/figure23284S2p.png]]

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R* 
library("plot3D")
scatter3D(results$nb_proc, results$size, results$simulation_time, ticktype = "detailed", phi = 0, theta = -50, bty ="g",
           surf = list(x = unique(results$nb_proc), y = unique(results$size), z =  matrix(results$simulation_time, nrow=length(unique(results$nb_proc))),  facets = NA))
#+end_src

#+RESULTS:
[[file:/tmp/babel-23284Iao/figure23284QCE.png]]



#+begin_src R :results output :session *R* :exports both
fit_sim = lm(data=results,  memory_size ~ (nb_proc + I(nb_proc^2)) + I(size^2))
summary(fit_sim)
#+end_src

#+RESULTS:
#+begin_example

Call:
lm(formula = memory_size ~ (nb_proc + I(nb_proc^2)) + I(size^2), 
    data = results)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.046408 -0.005840  0.001738  0.011710  0.058452 

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)  -3.785e-02  2.247e-02  -1.685   0.1179    
nb_proc       1.264e-04  2.519e-05   5.019   0.0003 ***
I(nb_proc^2)  1.288e-07  5.211e-09  24.712 1.17e-11 ***
I(size^2)     8.166e-13  1.063e-15 767.967  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.02691 on 12 degrees of freedom
Multiple R-squared:      1,	Adjusted R-squared:      1 
F-statistic: 2.043e+05 on 3 and 12 DF,  p-value: < 2.2e-16
#+end_example

#+begin_src R :results output :session *R* :exports both
grid.lines = 26
x.pred <- seq(min(results$nb_proc), max(results$nb_proc), length.out = grid.lines)
y.pred <- seq(min(results$size), max(results$size), length.out = grid.lines)
xy <- expand.grid( nb_proc = x.pred, size = y.pred)
z.pred <- matrix(predict(fit_sim, newdata = xy), 
                 nrow = grid.lines, ncol = grid.lines)
# fitted points for droplines to surface
fitpoints <- predict(fit_sim)
#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R* 
library("plot3D")
scatter3D(
   results$nb_proc, results$size, results$memory_size, ticktype = "detailed", phi = -10, theta = -50, bty ="g",
    pch = 18, cex = 2, 
    surf = list(x = x.pred, y = y.pred, z = z.pred,  
    facets = NA, fit = fitpoints),colvar=NULL)
#+end_src

#+RESULTS:
[[file:/tmp/babel-23284Iao/figure23284e_o.png]]

* Modeling Stampede and Simulating HPL
#+LaTeX: \label{sec:science}

** Modeling Stampede
*** Computations
Each node of the Stampede cluster comprises two 8-core Intel Xeon
E5-2680 8C \SI{2.7}{\GHz} CPU and one 61-core Intel Xeon Phi SE10P
(KNC) \SI{1.1}{\GHz} accelerator that is roughly three times more
powerful than the two CPUs. The Xeon Phis can be used in two ways:
either as a classical accelerator, \ie to offload expensive
computations from the CPU onto the accelerator, or by compiling
binaries specifically for the Xeon Phi and *running MPI directly between* Xeon
Phis. While the accelerator's \SI{8}{\gibi\byte} of RAM are rather
small, the main advantage of the second approach is that data does not
need to be transferred back and forth between the node's CPU and the
accelerator via the x16 PCIe bus.

The HPL output submitted to the TOP500 (Figure\ref{fig:hpl_output})
does not provide an explanation about how the KNC was used. We
are certain, however, that only a single MPI process per node was run and that the
KNC was hence rather used as an accelerator. This is easily possible 
with the Intel's Math Kernel Library that comes with
support for automatic offloading for selected BLAS functions. 
Alas, we do not know which version was used in 2013 and therefore decided to
use the default on Stampede in the beginning of 2017, \ie
version\nbsp{}11.1.1. The MKL documentation states
that depending on the matrix geometry, the computation will be done
either on all the cores of the CPU or *exclusively?* on the KNC.  In the case of
=DGEMM=, the computation of $A=\alpha\cdot{}A+\beta\cdot{}B\times{}C$ with $A, B, C$ being of
dimensions $M\times{}K$, $K\times{}N$ and $M\times{}N$, respectively, is offloaded onto the KNC whenever $M$
and $N$ are both larger than $1280$ while $K$ is *simultaneously* larger
than $256$. Similarly, offloading for =DTRSM= is used when both $M$ and $N$
are larger than $512$. To illustrate this, Figure\ref{fig:hpl_mkl}
depicts the time we measured for MKL's =DGEMM= and =DTRSM= implementations
on matrices with dimensions 
randomly taken from $[1,8\,000]$. Clearly, offloading to the KNC results in a
better throughput (lower slope) but incurs a higher latency
(higher intercept). The *number of flops* is always of the order
of $M\cdot{}N\cdot{}K$ ($M\cdot{}N^2$ for =DTRSM=) but the model that describes the time it
takes to run =DGEMM= (=DTRSM=) is very different for small and large
matrices. The table in Figure\ref{fig:macro_real} indicates the
parameters of the linear regression for the four scenarios (=DGEMM=
vs. =DTRSM= and =CPU= vs. =Phi=). The effective performance is close to the
peak performance: \eg for =DGEMM= on the =Phi=, the flop rate is
$2/\num{1.981e-12} = \SI{1.009}{\tera\flops}$. Since the granularity
used in HPL (see Figure\ref{fig:hpl_output}) is 1024, all calls (except
for maybe the very last iteration) should be offloaded to the KNC. 
In any case, this behavior can easily be accounted for by replacing the
macro in Figure\ref{fig:macro_simple} by the one in Figure\ref{fig:macro_real}.

# The accelerators are essential to the performance of the cluster,
# delivering \SI{7}{\peta\flops} of sustainable performance whereas
# the CPUs are only capable of delivering \SI{2}{\peta\flops}. On
# matrices of the size used for this work, however, CPUs are barely
# used.

# See CH's journal from [2017-10-03 Tue] to see how the version was determined
**** R figures                                                  :noexport:
#+begin_src R :results output :session *R* :exports both
  library(gridExtra)
  library(ggplot2)
  dgemm <- read.csv('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/stampede/offloading_dgemm.csv')
  dgemm$m = as.double(dgemm$m)
  dtrsm <- read.csv('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/stampede/offloading_dtrsm.csv')
  dtrsm$m = as.double(dtrsm$m)

  dgemm_new = dgemm[dgemm$automatic_offload == 'True',]
  dtrsm_new = dtrsm[dtrsm$automatic_offload == 'True',]
  dgemm_new$offload = (dgemm_new$m > 1280 & dgemm_new$n > 1280 & dgemm_new$k > 256)
  dtrsm_new$offload = (dtrsm_new$m > 512 & dtrsm_new$n > 512)

  dgemm_new$flops = dgemm_new$m * dgemm_new$n * dgemm_new$k;
  dtrsm_new$flops = dtrsm_new$m * dtrsm_new$n^2;

  dgemm_new$type = "dgemm";
  dtrsm_new$type = "dtrsm";

  df = dtrsm_new
  df$k = NA;
  df$lead_C = NA;
  df = rbind(df,dgemm_new)
  head(df)
  tail(df)
#+end_src

#+RESULTS:
#+begin_example
       time    m    n lead_A lead_B automatic_offloading offload        flops
1  0.029975 7251  261   7251   7251                 True   FALSE    493945371
4  2.227428  578 4619   4619   4619                 True    TRUE  12331723058
5  0.042097 4424  420   4424   4424                 True   FALSE    780393600
8  0.018786 3115  305   3115   3115                 True   FALSE    289772875
10 2.931274  650 5466   5466   5466                 True    TRUE  19420151400
12 3.240624 5606 6490   6490   6490                 True    TRUE 236125280600
    type  k lead_C
1  dtrsm NA     NA
4  dtrsm NA     NA
5  dtrsm NA     NA
8  dtrsm NA     NA
10 dtrsm NA     NA
12 dtrsm NA     NA
         time    m    n lead_A lead_B automatic_offloading offload        flops
89   0.083594  244 5757   5757   5757                 True   FALSE    847038924
91   4.932572 5527 6493   6493   6493                 True    TRUE 189518248891
931  2.943795 1425 6127   6127   6127                 True    TRUE  33954761775
96   0.262358   62 6621   6621   6621                 True   FALSE   2151851484
981  2.749753 4991 2256   4991   4991                 True    TRUE  17002140960
1001 2.383139 1421 1348   1646   1646                 True    TRUE   3152926168
      type    k lead_C
89   dgemm  603   5757
91   dgemm 5281   6493
931  dgemm 3889   6127
96   dgemm 5242   6621
981  dgemm 1510   4991
1001 dgemm 1646   1646
#+end_example

#+begin_src R :results output :session *R* :exports both
get_legend<-function(myggplot){
  tmp <- ggplot_gtable(ggplot_build(myggplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}
#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file figures/stampede_knc_model.pdf :exports both :width 3.5 :height 5.3 :session *R* 
  labels = rbind(
      data.frame(x = 2E11, y=3.5, label="Offloading on the KNC", offload = T),
      data.frame(x = 1.1E11, y=1, label="Computation on the CPU", offload = F));
  
  p1 = ggplot(dgemm_new, aes_string(x='m*n*k', y='time', color='offload')) +
      geom_point() + geom_smooth(method="lm",fullrange=T) + theme(legend.position="top") + 
      geom_text(data = labels, aes(x=x, y=y, color=offload, label=label)) + ylim(0,1.1*max(dgemm_new$time)) +
      ggtitle('Duration of DGEMM') + theme_bw() +
      xlab("M.N.K [Flop]") + ylab("Duration [s]") + scale_color_brewer(palette="Set1")

  p1_legend = get_legend(p1);
  p1 = p1 + theme(legend.position="none")
  labels = rbind(
      data.frame(x = 3E11, y=2.8, label="Offloading on the KNC", offload = T),
      data.frame(x = 1.6E11, y=.6, label="Computation on the CPU", offload = F));
  p2 = ggplot(dtrsm_new, aes_string(x='m*n*n', y='time', color='offload')) +
      geom_point()+ geom_smooth(method="lm",fullrange=T) +  ylim(0,1.1*max(dtrsm_new$time)) +
      geom_text(data = labels, aes(x=x, y=y, color=offload, label=label)) +
      ggtitle('Duration of DTRSM') + theme_bw() +
      xlab("M.N² [Flop]") + ylab("Duration [s]") + scale_color_brewer(palette="Set1")
  p2 = p2 + theme(legend.position="none")

   lay <- rbind(c(1), c(2));
   grid.arrange(p1,p2, layout_matrix = lay,widths=c(1), heights=c(2,2));
#+end_src

#+RESULTS:
[[file:figures/stampede_knc_model.pdf]]

#+begin_src sh :results output :exports both
pdfcrop figures/stampede_knc_model.pdf figures/stampede_knc_model.pdf
#+end_src

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R* 
  ggplot(df, aes(x=flops, y=time, color=offload)) +
      geom_point() + geom_smooth(method="lm") + 
      facet_wrap(~type)
#+end_src

#+RESULTS:
[[file:/tmp/babel-1674kfe/figure1674sWN.png]]

*** Communications
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth,page=1]{./figures/stampede_calibration_send.png}
  \caption{Modeling communication time on stampede. Each color corresponds to a
      manually adjusted breakpoint corresponding to a synchronization mode 
      (eager, rendez-vous,...). }\vspace{-1em}
  \label{fig:stampede_calibration}
  \labspace
\end{figure}
#+END_EXPORT

We unfortunately do not know for sure which version of Intel MPI was used in
2013, so we decided to use the default (recommended) one on Stampede
in May 2017, \ie version 3.1.4. As explained in 
Section\ref{sec:smpi}, SMPI's communication model is a hybrid model
between the LogP family and a fluid model. For each message, the send mode
\eg fully asynchronous, detached or eager, is determined solely by the
message size. It is hence possible to model the resulting performance
of *communication operations* through a piece-wise, linear model, as depicted in
Figure\ref{fig:stampede_calibration}. For a thorough discussion of
the calibration techniques, see\cite{smpi}. Although *the behavior of*
=MPI_Send= is quite stable and piece-wise regular, *the behavior of*
=MPI_Recv= is surprising: for small messages with a size of less than \SI{17420}{\byte}
(represented by purple, blue and red dots), one can observe two modes,
namely "slow" and "fast" communications. "Slow"
communications take twice longer and are much more common than the 
"fast" ones. We observed this behavior *several times* although both MPI
processes that were used in the calibration were connected through 
the same local switch. This "perturbation" was present throughout the execution of our
calibration. 
#+LATEX: \CH{We want to say: We don't know why this is here.}
Considering that small messages are scarce in HPL, we eventually decided to
ignore this phenomenon and opted to use the more favorable scenario (fast
communications) for small messages. We believe that the impact of
our choice on the simulation accuracy is minimal as primarily large,
bulk messages are sent that make use of the /rendez-vous/ mode (depicted in dark green).

# Interestingly, the Stampede website says it's "56 GB/s" but I
# checked and can only find 56 Gbit/s. I hence go with that, as it
# seems more reasonable as well.

As explained in Section\ref{sec:stampede}, Stampede leverages Mellanox
FDR InfiniBand technology with \SI{56}{\giga\bit\per\second}, setup in
a fat-tree topology (see Figure\ref{fig:fat_tree_topology}). We
assumed the routing was done through D-mod-K\cite{dmodk} as it is
commonly used on this topology and configured SMPI accordingly.
**** Stampede network calibration figures                       :noexport:
This figure is generated in [[file:~/Work/SimGrid/platform-calibration/data/stampede_17_06_01-17:14/calibration/analysis.org][the platform calibration repository]]. Data
should be read from there. Final adjustments (in the "Combined plot
section") were done here:

#+begin_src R :results output :session *R* :exports both
library(gridExtra)
get_legend<-function(myggplot){
  tmp <- ggplot_gtable(ggplot_build(myggplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}

p1 = eth$p_send + theme(legend.position="top", legend.background = element_rect(fill = "white", colour = NA)) + guides(colour = guide_legend(override.aes = list(alpha = 1)))
#     + annotate("text",x=1E2,y=2.3E-6, label="40GB IB model",color="black")


p2 = eth$p_recv + theme(legend.position="top", legend.background = element_rect(fill = "white", colour = NA)) 
#     + annotate("text",x=1E2,y=2.3E-6, label="40GB IB model",color="black")

p1_legend = get_legend(p1);
p1 = p1 + theme(legend.position="none")
p2 = p2 + theme(legend.position="none")

# lay <- rbind(c(1,1),
#              c(2,3));
# p = grid.arrange(p1_legend,p1,p2, layout_matrix = lay,widths=c(2,2), heights=c(1.3,4));
# ggsave(filename="/tmp/taurus_send_recv.pdf",plot=p,width = 6, height = 4)

lay <- rbind(c(1,2));
p = grid.arrange(p1,p2, layout_matrix = lay,widths=c(2,2), heights=c(4));
ggsave(filename="/tmp/stampede_send_recv_eth.pdf",plot=p,width = 6, height = 3)
#+end_src

#+RESULTS:
: Warning message:
: Transformation introduced infinite values in continuous x-axis
: Warning messages:
: 1: Transformation introduced infinite values in continuous x-axis 
: 2: Transformation introduced infinite values in continuous x-axis

#+begin_src R :results output :session *R* :exports both
ggsave(filename="/tmp/stampede_send_recv_eth.png",plot=p,width = 6, height = 3)
#+end_src

#+RESULTS:

#+begin_src sh :results output :exports both
cp /tmp/stampede_send_recv_eth.png ./figures/stampede_calibration_send.png
#+end_src

#+RESULTS:
*** Summarizing modeling uncertainties
For the compiler, Intel MPI and the MKL, we were unable to determine
which version was used in 2013, but decided to go for rather optimistic
choices. The model for the MKL and for Intel MPI are close to the peak
performance. For the compiler, it is imaginable that it managed to optimize
HPL thoroughly outside of the MKL. Yet, most of these parts
are executed but not accounted for in our simulations: this
allows us to obtain fully deterministic simulations without harming the
outcome of the simulation as these parts only represent a tiny fraction of
the total execution time of HPL. A few HPL compilation flags (\eg
=HPL_NO_MPI_DATATYPE= and =HPL_COPY_L= that control whether MPI datatypes
should be used and how, respectively) could not be deduced from
HPL's original output on Stampede but we believe their impact to be
minimal. Finally, the HPL output reports the use of HPL v2.1 but the
main difference between v2.1 and v2.2 is the option to
continuously report factorization progress. We hence decided to apply
our modifications to the later version of HPL.

Due to these choices, we could expect the prediction of
our simulations to be optimistic but close to results obtained by a real life execution.
# - iMPI version ???
# - HPL compilation ? Possible modifications s.a. using openMP to have thread taking care of MPI communications and progressions.
** Simulating HPL
*** Performance Prediction
The solid line in Figure\ref{fig:stampede_prediction} represents the HPL
performance prediction as obtained with SMPI with the Stampede model
described in the previous section. Although we expected SMPI to be
optimistic, the prediction was surprisingly much lower than what was
obtained in 2013. After verifying *in our simulations*
that no part of HPL was left unmodeled, we suspected our network model
to be the problem and decided to
investigate whether *undue* network congestion could explain the
performance. Alas, even a congestion-free network model 
(dashed line Figure\ref{fig:stampede_prediction}) only
results in minor improvements. In our experiments with the KNC,
either the CPU or the KNC seemed to be used at one time and a specifically
optimized version of the MKL may have been used in 2013. 
Removing the offloading latency and modeling each node as a
single \SI{1.2}{\tera\flops} node does not sufficiently explain the
divide between our results and reality.

A significant improvement was obtained by removing the network latency 
and increasing the bandwidth by a factor 10, which is completely
unrealistic and can hence not be the solution.

#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth,page=1]{./figures/stampede_simgrid.pdf}
  \caption{Performance prediction of HPL on Stampede using SimGrid.}\vspace{-1em}
  \label{fig:stampede_prediction}
  \labspace
\end{figure}
#+END_EXPORT

**** HPL prediction                                             :noexport:

Starting from Tom's journal, entry "2017-09-27 Wednesday : Complete experiments with the crosstraffic desabled"
#+begin_src R :results output :session *R* :exports both
library(ggplot2)
# files <- dir('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/stampede/simulation/8', pattern = '\\.csv', full.names = TRUE)
# tables <- lapply(files, read.csv)
# results = do.call(rbind, tables)

classical = rbind(read.csv('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/stampede/simulation/8/result_1000000.csv'),
    read.csv('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/stampede/simulation/8/result_2000000.csv'),
    read.csv('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/stampede/simulation/8/result_3875000.csv'))
classical$mode = 'classical'

highbw = rbind(read.csv('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/stampede/simulation/8/result_highbw_1000000.csv'),
    read.csv('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/stampede/simulation/8/result_highbw_2000000.csv'),
    read.csv('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/stampede/simulation/8/result_highbw_3875000.csv'))
highbw$mode = 'highbw'

fatpipe = rbind(read.csv('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/stampede/simulation/8/result_fatpipe_1000000.csv'),
    read.csv('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/stampede/simulation/8/result_fatpipe_2000000.csv'),
    read.csv('/home/alegrand/Work/SimGrid/tom/m2_internship_journal/stampede/simulation/8/result_fatpipe_3875000.csv'))
fatpipe$mode = 'fatpipe'

results = rbind(classical, highbw, fatpipe)
#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file figures/stampede_simgrid.pdf :exports both :width 5 :height 3 :session *R* 
res_lab = results[results$mode != 'highbw' & results$size > 3E6,];
res_lab$x=res_lab$size;
res_lab$y=res_lab$Gflops;
res_lab$xl=res_lab$x*.9;
res_lab$yl=res_lab$y*.9;


res_lab$xl=res_lab$x*.8;
res_lab$yl=res_lab$y*.9;

res_lab[res_lab$mode=='classical',]$xl=3.4e6
res_lab[res_lab$mode=='classical',]$yl=2.5e6
res_lab[res_lab$mode=='fatpipe',]$xl=2.e6
res_lab[res_lab$mode=='fatpipe',]$yl=4.1e6

res_lab$label = NA;
res_lab[res_lab$mode=='fatpipe',]$label = "Simulation\n (No Contention)";
res_lab[res_lab$mode=='classical',]$label = "Simulation\n (Fat Tree)";

ggplot(results[results$mode != 'highbw',], aes(x=size, y=Gflops, color=mode, linetype=mode)) + 
    # Inner labels
    geom_segment(data=res_lab, 
                 aes(x=xl, xend=x, y=yl, yend=y), linetype="solid",
                 color="black") +
    geom_label(data=res_lab, 
               aes(label = factor(label), x=xl, y=yl, fill=mode),
               colour =  "white", fontface = "bold") +
    # SMPI lines
    geom_point() + geom_line() +  
    # Top500 perf
    geom_hline(yintercept=5.16811e+06) + 
    annotate("text",x=3875000, 1E2,y=5.4e+06, hjust="right",
        label="Top 500 performance",color="black") + 
    annotate("text",x=3875000, 1E2,y=4.96e+06, hjust="right",
        label="(5.168 TeraFlop/s)",color="black") + 
    annotate("point",x=3875000, 1E2,y=5.16811e+06,color="black") + 
    # Cosmetics
    guides(fill=FALSE, color=FALSE, linetype=FALSE) + ylab("GFlop/s") + xlab("Matrix rank") +
    theme_bw() +  scale_color_brewer(palette="Set1") +  scale_fill_brewer(palette="Set1")+ 
    theme(legend.position="top") +
    ggtitle('Performance of HPL')
#+end_src

#+RESULTS:
[[file:figures/stampede_simgrid.pdf]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R* 
ggplot(results, aes(x=size, y=simulation_time, color=mode, linetype=mode)) + geom_point() + geom_line() + ggtitle('Simulation time')
#+end_src

#+RESULTS:
[[file:/tmp/babel-23284Iao/figure232842Rn.png]]

*** Performance Gap Investigation
# The simulation time to get the full-scale trace was:
# - 420 seconds for two iterations (250 seconds spent in HPL),
# - 609 seconds for five iterations (265 seconds spent in HPL).
In this section, we explain our investigation and give possible reasons for
the previous mismatch. With SMPI it is simple to trace
the first iterations of HPL in order to get an idea of what could be
improved (the trace for the first five iterations can be obtained in
about 609 seconds on a laptop and is with compression about
\SI{175}{\mega\byte}). Figure\ref{fig:hpl_gantt} illustrates the first iterations
very synchronous and iterative nature: One can identify first a factorization of the panel, then a broadcast to all the
nodes, and finally an update of trailing matrix.
Visibly more than one fifth of each iteration is spent exclusively communicating (although the first
iterations are the ones with the lowest communication to computation ratio),
which prevents HPL from reaching the expected performance. 

Overlapping this communication with computation would improve
performance significantly. The fact that there is
almost no overlap can be explained by the look-ahead ~DEPTH=0~
parameter, which is supposedly the right configuration (see
Figure\ref{fig:hpl_output}). This choice is quite surprising as even
the tuning section of the HPL documentation indicates that a depth of
1 generally yields the best results, even though a large problem size may
be needed to see some performance gain. After discussing this
surprising behavior with the Stampede-team, we were informed that the
run in 2013 was execute with an HPL binary that was provided by Intel
and that HPL's code was probably specifically modified for Stampede. We
believe that some configuration values have been hardcoded to enforce an overlap of
iterations with others. Indeed, the shortened part (marked as "=[...]=") in
Figure\ref{fig:hpl_output} provides information about the progression of
HPL throughout iterations and statistics *for the process owning the
panel* about the time spent in the most important parts. 
According to these statistics, the total time
spent in the =Update= section is \SI{9390}{\sec} whereas the total
execution time is \SI{7505}{\sec}, which is impossible unless iterations have overlapped.

The broadcast and swapping algorithms use *very heavy*
communication patterns. This is not at all surprising as for a matrix of
this order, several hundreds of megabytes need to be broadcast. 
We tried the other of the 6 broadcast algorithms HPL comes with but
did not realize significantly better overall performance. 
An analysis *of the symbols* contained in the Intel binary
indicates that it ships with another broadcast algorithm whose name is
=HPL_bcast_bpush= and which calls non-blocking sends, unlike the other
broadcasting algorithms. This seems to be a plausible explanation for the
performance obtained in 2013 although the output reports that the =BlongM=
algorithm was used. Likewise, the swapping algorithm that was used
(~SWAP=Binary-exchange~) involves communications that are rather long and
organized in trees, which is surprising as the ~spread-roll~ algorithm
is recommended for large problem sizes.

#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth,page=1]{./figures/fullscale_unzoomed.png}
  \caption{Gantt chart of the first two iterations of HPL. Communication states are in 
      red while computations are in light blue. Each communication between two process
      is represented with a white arrow, which results in very cluttered white areas.}\vspace{-1em}
  \label{fig:hpl_gantt}
  \labspace
\end{figure}
#+END_EXPORT

We do not aim to reverse engineer the Intel HPL code. We can, however,
already draw two conclusions from our simple analysis: 1) it is apparent that many optimizations have been done on
the communication side and 2) it is very likely that the reported
parameters are not the ones used in the real execution, probably because 
these values were hardcoded and the configuration file was not updated
accordingly.
*** Gantt charts                                                 :noexport:
#+begin_src sh :results output :exports both
cp /home/alegrand/Work/SimGrid/tom/m2_internship_journal/stampede/communications/fullscale_unzoomed.png figures/
#+end_src

#+RESULTS:

* Conclusions
#+LaTeX: \label{sec:cl}

Studying HPC applications at scale can be very time- and
resource-consuming. Simulation is often an effective approach in this
context and SMPI has previously been successfully validated in several small-scale
studies with standard HPC applications\cite{smpi,heinrich:hal-01523608}.  In this
article, we proposed and evaluated extensions to the SimGrid/SMPI
framework that allowed us to emulate HPL at the scale of a
supercomputer. Our application of choice, HPL, is particularly challenging in terms of simulation
as it implements its own set of non-blocking collective operations
that rely on =MPI_Iprobe= in order to facilitate overlapping with computations.

More specifically, we tried to reproduce the execution of HPL on the
Stampede supercomputer conducted in $2013$ for the TOP500, which
involved a \SI{120}{\tera\byte} matrix and took two hours on 6,006\nbsp{}nodes.  
Our emulation of a similar configuration ran on a single machine for
about $47$ hours and required less than \SI{16}{\giga\byte} of RAM. This emulation
leveraged several non-trivial operating-system level optimizations
(memory mapping, dynamic library loading, huge pages) that have been
integrated into the last version of SimGrid/SMPI.

The downside of scaling this high is a less well-controlled scenario.
The reference run of HPL on Stampede was done several years ago and we only
have very limited information about the setup (\eg software versions
and configuration), while a reservation and re-execution on the whole
machine was impossible for us. We nevertheless modeled Stampede carefully, which
allowed us to predict the performance that would
have been obtained using an unmodified, freely available version of HPL.
Unfortunately, despite all our efforts, the predicted performance
was much lower than what was reported in 2013. We determined that this
discrepancy comes from the fact that a modified, closed-source version of HPL
supplied by Intel was used in 2013.
We believe that some of the HPL configuration parameters were
hardcoded and therefore misreported in the output. An analysis of the optimized
HPL binary confirmed that algorithmic differences were likely to be the
reason for the performance differences.

We conclude that a large-scale (in)validation is unfortunately not
possible due to the modified source code being unavailable to us.
We claim that the modifications we made are
minor and should be applicable to that optimized version. In fact, while HPL
comprises 16K lines of ANSI C over 149 files, our modifications only
changed 14 files and involved 286 line insertions and 18 deletions.

Although being capable of predicting an application's performance on a
platform is by itself interesting, we believe that this capability
will become invaluable in the future to aid compute centers with the
decision of whether a new machine will work best for a given
application or if an upgrade of the current machine should be
considered. 
# This goal will be subject to a more thorough investigation
# in the very near future.

# As we saw in Section\ref{sec:hplchanges}, two BLAS functions (=dgemm=
# and =dtrsm=) were the dominating factor with regards to the runtime although other BLAS
# functions were called as well. For this study, we neglected the other
# functions but with a fully automatic calibration procedure for any
# BLAS function results could effortlessly become more precise as the
# application could just be linked against a BLAS-replacement
# library. 
# #+LaTeX: \CH{Problem here: HPL uses \texttt{HPL\_dtrsm()} wrappers.}

# #+LaTeX: \CH{I like the idea of pointing out again that our simulation takes much longer (48 hours instead of 2?) but that we use 1/6000 of the ressources}

* Acknowledgements

Experiments presented in this paper were carried out using the Grid'5000 testbed, supported by a scientific interest group hosted by Inria and including CNRS, RENATER and several Universities as well as other organizations (see https://www.grid5000.fr).

** References                                                       :ignore:

# See next section to understand how refs.bib file is created.

#+LATEX: \bibliographystyle{IEEEtran}
#+LATEX: \bibliography{refs}

* Bib file is here                                                 :noexport:

Tangle this file with C-c C-v t

#+begin_src bib :tangle refs.bib
@IEEEtranBSTCTL{IEEEexample:BSTcontrol,
   CTLuse_article_number = "yes",
   CTLuse_paper = "yes",
   CTLuse_url = "yes",
   CTLuse_forced_etal = "yes",
   CTLmax_names_forced_etal = "6",
   CTLnames_show_etal = "3",
   CTLuse_alt_spacing = "yes",
   CTLalt_stretch_factor = "4",
   CTLdash_repeated_names = "yes",
   CTLname_format_string = "{f. ~}{vv ~}{ll}{, jj}",
   CTLname_latex_cmd = "",
   CTLname_url_prefix = "[Online]. Available:"
}

@mastersthesis{cornebize:hal-01544827,
  TITLE = {{Capacity Planning of Supercomputers: Simulating MPI Applications at Scale}},
  AUTHOR = {Cornebize, Tom},
  URL = {https://hal.inria.fr/hal-01544827},
  SCHOOL = {{Grenoble INP ; Universit{\'e} Grenoble - Alpes}},
  YEAR = {2017},
  MONTH = Jun,
  KEYWORDS = {Simulation ;  MPI runtime and applications ;  Performance prediction and extrapolation ;  High Performance LINPACK},
  PDF = {https://hal.inria.fr/hal-01544827/file/report.pdf},
  HAL_ID = {hal-01544827},
  HAL_VERSION = {v1},
}

@incollection{grid5000,
   title = {Adding Virtualization Capabilities to the {Grid'5000} Testbed},
   author = {Balouek, Daniel and Carpen-Amarie, Alexandra and Charrier, Ghislain and Desprez, Fr{\'e}d{\'e}ric and Jeannot, Emmanuel and Jeanvoine, Emmanuel and L{\`e}bre, Adrien and Margery, David and Niclausse, Nicolas and Nussbaum, Lucas and Richard, Olivier and P{\'e}rez, Christian and Quesnel, Flavien and Rohr, Cyril and Sarzyniec, Luc},
   booktitle = {Cloud Computing and Services Science},
   publisher = {Springer International Publishing},
   OPTpages = {3-20},
   volume = {367},
   editor = {Ivanov, IvanI. and Sinderen, Marten and Leymann, Frank and Shan, Tony },
   series = {Communications in Computer and Information Science },
   isbn = {978-3-319-04518-4 },
   doi = {10.1007/978-3-319-04519-1\_1 },
   year = {2013},
}

%%% Online simulation of MPI applications
@article{xsim,
  author        = "Christian Engelmann",
  title         = {{Scaling To A Million Cores And Beyond: {Using} Light-Weight
                   Simulation to Understand The Challenges Ahead On The Road To
                   Exascale}},
  journal       = "FGCS",
  volume        = 30,
  pages         = "59--65",
  month         = jan,
  year          = 2014,
  publisher     = "Elsevier"}

@Article{sstmacro,
  author = {Curtis L. Janssen and Helgi Adalsteinsson and Scott Cranford and Joseph P. Kenny and Ali Pinar and David A. Evensky and Jackson Mayo},
  journal = {International Journal of Parallel and Distributed Systems},
  title = {A Simulator for Large-scale Parallel Architectures},
  volume = {1},
  number = {2},
  pages = {57--73},
  year = {2010},
  note = "\url{http://dx.doi.org/10.4018/jdst.2010040104}",
  doi = {10.4018/jdst.2010040104}
}

@article{SST,
  author    = {Rodrigues, Arun and Hemmert, Karl and Barrett, Brian
                  and Kersey, Chad and Oldfield, Ron and Weston, Marlo
                  and Riesen, Rolf and Cook, Jeanine and Rosenfeld,
                  Paul and CooperBalls, Elliot and Jacob, Bruce },
  title     = {{The Structural Simulation Toolkit}},
  journal   = {{SIGMETRICS} Performance Evaluation Review},
  volume    = 38,
  number    = 4,
  pages     = {37--42},
  year      = 2011
}

@article{dickens_tpds96,
  title={{Parallelized Direct Execution Simulation of Message-Passing
                  Parallel Programs}},
  author={Dickens, Phillip and Heidelberger, Philip and Nicol, David},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume=7,
  number=10,
  year=1996,
  pages={1090--1105}
}

@ARTICLE{bagrodia_ijhpca01,
  author={Bagrodia, Rajive and Deelman, Ewa and Phan, Thomas},
  title={{Parallel Simulation of Large-Scale Parallel Applications}},
  journal={International Journal of High Performance Computing and
                  Applications},
  volume=15,
  number=1,
  year=2001,
  pages={3--12}
}

%%% Offline simulation of MPI applications
@INPROCEEDINGS{loggopsim_10,
  title={{LogGOPSim - Simulating Large-Scale Applications in 
          the LogGOPS Model}},
  author={Hoefler, Torsten and Siebert, Christian and Lumsdaine, Andrew},
  month=Jun,
  year={2010},
  pages = {597--604},
  booktitle={Proc. of the LSAP Workshop},
}

@inproceedings{hoefler-goal,
  author={T. Hoefler and C. Siebert and A. Lumsdaine},
  title={{Group Operation Assembly Language - A Flexible Way to Express Collective Communication}},
  year={2009},
  booktitle={Proc. of the 38th ICPP}
}

@inproceedings{bigsim_04,
  author={Zheng, Gengbin and Kakulapati, Gunavardhan and Kale,
                  Laxmikant},
  title={{BigSim: A Parallel Simulator for Performance Prediction of
                  Extremely Large Parallel Machines}},
  year=2004,
  booktitle={Proc. of the 18th IPDPS},
}

@inproceedings{dimemas,
	title = {{Dimemas: Predicting MPI Applications Behaviour in Grid Environments}},
	year = {2003},
	month = jun,
	booktitle = {Proc. of the Workshop on Grid Applications and
                  Programming Tools},
	author = {Rosa M. Badia and Jes{\'u}s Labarta and Judit Gim{\'e}nez and Francesc Escal{\'e}}
}

@article {CODES,
 title = {Enabling Parallel Simulation of Large-Scale HPC Network Systems},
 journal = {IEEE Transactions on Parallel and Distributed Systems},
 year = {2016},
 author = {Mubarak, M. and C. D. Carothers and Robert B. Ross and Philip H. Carns}
}

@article{ROSS_SC12,
author = {Misbah Mubarak and Christopher D. Carothers and Robert Ross and Philip Carns},
title = {{Modeling a Million-Node Dragonfly Network Using Massively Parallel Discrete-Event Simulation}},
journal ={SC Companion},
year = {2012},
pages = {366-376},
}

%%% Self citations on previous work
@Article{simgrid,
  title = {{Versatile, Scalable, and Accurate Simulation of Distributed Applications and Platforms}},
  author = {Casanova, Henri and Giersch, Arnaud and Legrand, Arnaud and Quinson, Martin and Suter, Fr{\'e}d{\'e}ric},
  publisher = {Elsevier},
  pages = {2899-2917},
  journal = {Journal of Parallel and Distributed Computing},
  volume = {74},
  number = {10},
  year = {2014}
}

@InProceedings{simetierre,
  author = {Bobelin, Laurent and Legrand, Arnaud and 
                  M{\'a}rquez, David Alejandro Gonz{\'a}lez and Navarro,
                  Pierre and Quinson, Martin and Suter,
                  Fr{\'e}d{\'e}ric and Thiery, Christophe},
  title = 	 {{Scalable Multi-Purpose Network Representation for
                  Large Scale Distributed System Simulation}},
  booktitle = {Proc. of the 12th IEEE/ACM International
                  Symposium on Cluster, Cloud and Grid Computing},
  year = 	 2012,
  pages = {220--227},
  address = 	 {Ottawa, Canada}
}

@InProceedings {simgrid_simix2_12,
  author = {Martin Quinson and Cristian Rosa and Christophe Thi{\'e}ry},
  title = {Parallel Simulation of Peer-to-Peer Systems},
  booktitle = {{P}roc. of the 12th {IEEE/ACM} {I}ntl. {S}ymposium on {C}luster, Cloud and Grid {C}omputing},
  year = {2012},
  address = {Ottawa, Canada}   
} 

@InProceedings {DCLV_LSAP_10,
  title = {{Fast and Scalable Simulation of Volunteer Computing Systems
                  Using SimGrid}},
  booktitle = {Proc. of the Workshop on Large-Scale System and Application
                  Performance},
  year = {2010},
  month = Jun,
  address = {Chicago, IL},
  author = {Donassolo, Bruno and Casanova, Henri and Legrand, Arnaud
                  and Velho, Pedro},
  category = {core}
} 

@InProceedings{SMPI_IPDPS,
  author	= {Clauss, Pierre-Nicolas and Stillwell, Mark and Genaud,
		  St\'ephane and Suter, Fr\'ed\'eric and Casanova, Henri and
		  Quinson, Martin},
  title	= {{Single Node On-Line Simulation of MPI Applications with
		  SMPI}},
  booktitle= {Proc. of the 25th IEEE Intl. Parallel and
		  Distributed Processing Symposium},
  year	= 2011,
  address	= {Anchorage, AK}
}


@Article{Velho_TOMACS13,
  author = {Velho, Pedro and Schnorr, Lucas and Casanova, Henri and Legrand, Arnaud},
  title = 	 {{On the Validity of Flow-level TCP Network Models for Grid and Cloud Simulations}},
  journal = 	 {ACM Transactions on Modeling and Computer Simulation},
  year = 	 {2013},
 PUBLISHER = {ACM}, 
  VOLUME = 23, 
  NUMBER = 4,
  pages = 23, 
  MONTH = Oct
}

@article{smpi,
  TITLE = {Simulating MPI applications: the SMPI approach},
  AUTHOR = {Degomme, Augustin and Legrand, Arnaud and Markomanolis, Georges and Quinson, Martin and Stillwell, Mark S and Suter, Frédéric},
  JOURNAL = {{IEEE Transactions on Parallel and Distributed Systems}},
  PUBLISHER = {{Institute of Electrical and Electronics Engineers}},
  volume =       "28",
  number =       "8",
  pages =        "2387--2400",
  PAGES = {14},
  YEAR = {2017},
  MONTH = Feb,
  DOI = {10.1109/TPDS.2017.2669305},
  KEYWORDS = {Simulation ; MPI runtime and applications ; Performance prediction and extrapolation},
  PDF = {https://hal.inria.fr/hal-01415484/file/smpi_article.pdf},
  HAL_ID = {hal-01415484},
  HAL_VERSION = {v2},
  category =     "core",
}

@InProceedings{heinrich:hal-01523608,
  title =        "{Predicting the Energy Consumption of MPI Applications
                 at Scale Using a Single Node}",
  author =       "Franz C. Heinrich and Tom Cornebize and Augustin
                 Degomme and Arnaud Legrand and Alexandra Carpen-Amarie
                 and Sascha Hunold and Anne-Cécile Orgerie and Martin
                 Quinson",
  URL =          "https://hal.inria.fr/hal-01523608",
  booktitle =    "Proc. of the 19th IEEE Cluster Conference",
  year =         "2017",
  keywords =     "simulation ; HPC ; energy ; platform modeling",
  pdf =          "https://hal.inria.fr/hal-01523608/file/predicting-energy-consumption-at-scale.pdf",
  hal_id =       "hal-01523608",
  category =     "core",
}

% Trace extrapolation
@InProceedings{scalaextrap,
  author = 	 {Xing Wu and Frank Mueller},
  title = 	 {{S}cala{E}xtrap: Trace-Based Communication Extrapolation
                  for {SPMD} Programs},
  booktitle = {Proc. of the 16th ACM Symp. on Principles and
               Practice of Parallel Programming},
  year = 	 {2011},
  pages = {113--122},
}

@InProceedings{pmac_lspp13,
  author = 	 {Laura Carrington and Michael Laurenzano and Ananta Tiwari},
  title = 	 {Inferring Large-scale Computation Behavior via Trace Extrapolation},
  booktitle = {Proc. of the Workshop on Large-Scale Parallel Processing},
  year = 	 {2013},
}

@Misc{hpl,
  author = 	 {Antoine Petitet and Clint Whaley and Jack Dongarra and Andy Cleary and Piotr Luszczek},
  title = 	 {HPL - A Portable Implementation of the High-Performance Linpack Benchmark for Distributed-Memory Computers},
  howpublished = {\url{http://www.netlib.org/benchmark/hpl}},
  month = 	 {February},
  year = 	 {2016},
  note = 	 {Version 2.2}
}

@book{top500,
 author = {Meuer, Hans Werner and Strohmaier, Erich and Dongarra, Jack and Simon, Horst D.},
 title = {The TOP500: History, Trends, and Future Directions in High Performance Computing},
 year = {2014},
 isbn = {143981595X, 9781439815953},
 edition = {1st},
 publisher = {Chapman \& Hall/CRC},
} 

@InProceedings{dmodk,
  author = 	 {Eitan Zahavi},
  title = 	 {D-Mod-K Routing Providing Non-Blocking Traffic for Shift Permutations on Real Life Fat Trees},
  booktitle = {Proc. of the ????},
  year = 	 {2010},
}
#+end_src

* Emacs Setup 							   :noexport:
# Local Variables:
# eval:    (require 'org-install)
# eval:    (org-babel-do-load-languages 'org-babel-load-languages '( (shell . t) (R . t) (perl . t) (ditaa . t) ))
# eval:    (setq org-confirm-babel-evaluate nil)
# eval:    (unless (boundp 'org-latex-classes) (setq org-latex-classes nil))
# eval:    (add-to-list 'org-latex-classes '("IEEEtran"
# "\\documentclass[conference, 10pt]{IEEEtran}\n \[NO-DEFAULT-PACKAGES]\n \[EXTRA]\n  \\usepackage{graphicx}\n  \\usepackage{hyperref}"  ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")                       ("\\subsubsection{%s}" . "\\subsubsection*{%s}")                       ("\\paragraph{%s}" . "\\paragraph*{%s}")                       ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# eval:    (add-to-list 'org-latex-classes '("llncs" "\\documentclass{llncs2e/llncs}\n \[NO-DEFAULT-PACKAGES]\n \[EXTRA]\n"  ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")                       ("\\subsubsection{%s}" . "\\subsubsection*{%s}")                       ("\\paragraph{%s}" . "\\paragraph*{%s}")                       ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# eval:    (add-to-list 'org-latex-classes '("acm-proc-article-sp" "\\documentclass{acm_proc_article-sp}\n \[NO-DEFAULT-PACKAGES]\n \[EXTRA]\n"  ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")                       ("\\subsubsection{%s}" . "\\subsubsection*{%s}")                       ("\\paragraph{%s}" . "\\paragraph*{%s}")                       ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# eval:    (add-to-list 'org-latex-classes '("sig-alternate" "\\documentclass{sig-alternate}\n \[NO-DEFAULT-PACKAGES]\n \[EXTRA]\n"  ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")                       ("\\subsubsection{%s}" . "\\subsubsection*{%s}")                       ("\\paragraph{%s}" . "\\paragraph*{%s}")                       ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# eval:    (setq org-alphabetical-lists t)
# eval:    (setq org-src-fontify-natively t)
# eval:   (setq ispell-local-dictionary "american")
# eval:   (eval (flyspell-mode t))
# eval:   (setq org-todo-keyword-faces '(("FLAWED" . (:foreground "RED" :weight bold))))
# eval:   (custom-set-variables '(org-babel-shell-names (quote ("sh" "bash" "csh" "ash" "dash" "ksh" "mksh" "posh" "zsh"))))
# eval:   (add-to-list 'load-path ".")
# eval:   (require 'ox-extra)
# eval:   (setq org-latex-tables-centered nil)
# eval:   (ox-extras-activate '(ignore-headlines))
# End:
